{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# \u2705 TRACKER: Use baseline final formatted summary (output_table)\n",
        "# Assumes 'output_table' exists as your final display DF\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"\ud83d\udccb TRACKER COPY-PASTE (Phase 1 Baseline - output_table)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "exp_id = str(EXPERIMENT_ID) if 'EXPERIMENT_ID' in dir() else 'UNKNOWN'\n",
        "sd = str(EXP_START_DATE) if 'EXP_START_DATE' in dir() else 'UNKNOWN'\n",
        "ed = str(EXP_END_DATE) if 'EXP_END_DATE' in dir() else 'UNKNOWN'\n",
        "rt = float(TOTAL_RUNTIME_SECONDS) if 'TOTAL_RUNTIME_SECONDS' in dir() else None\n",
        "print(f\"Experiment_ID: {exp_id}\")\n",
        "print(f\"Start_Date:    {sd}\")\n",
        "print(f\"End_Date:      {ed}\")\n",
        "print(f\"Runtime_Sec:   {rt if rt is not None else 'N/A'}\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "df = globals().get('output_table', None)\n",
        "if df is None:\n",
        "    print(\"\u26a0\ufe0f output_table not found. Ensure you're running this after the summary is built.\")\n",
        "else:\n",
        "    try:\n",
        "        pdf = df.toPandas()\n",
        "    except Exception:\n",
        "        pdf = df if isinstance(df, pd.DataFrame) else None\n",
        "\n",
        "    if pdf is None or pdf.empty:\n",
        "        print(\"\u26a0\ufe0f output_table is empty.\")\n",
        "    else:\n",
        "        variant_col = 'VARIANT_ID'\n",
        "        orders_col  = 'TXNS_TOTAL'            # Orders / transactions total\n",
        "        revenue_col = 'COMBINED_REVENUE_TOTAL'\n",
        "        redemp_col  = 'REDEMPTIONS_TOTAL'\n",
        "\n",
        "        missing = [c for c in [variant_col, orders_col, revenue_col, redemp_col] if c not in pdf.columns]\n",
        "        if missing:\n",
        "            print(f\"\u26a0\ufe0f Missing columns in output_table: {missing}\")\n",
        "            print(f\"Available columns: {list(pdf.columns)}\")\n",
        "        else:\n",
        "            print(\"Variant,Orders,Revenue,Redemptions\")\n",
        "            for _, row in pdf.iterrows():\n",
        "                print(f\"{row[variant_col]},{row[orders_col]},{row[revenue_col]},{row[redemp_col]}\")\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"\u2705 Copy the 'Variant,Orders,Revenue,Redemptions' lines into your tracker\")\n",
        "print(\"=\"*70)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# \u23f1\ufe0f PHASE 1 BASELINE: Start Timer\n",
        "# NOTE: Using 'import datetime' (not 'from datetime import datetime') to avoid conflicts with SAFE code\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "# Start timing\n",
        "BASELINE_START_TIME = time.time()\n",
        "BASELINE_START_DATETIME = datetime.datetime.now()\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"\u23f1\ufe0f  PHASE 1 BASELINE TIMING STARTED\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"Start Time: {BASELINE_START_DATETIME.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "print(\"=\" * 70)\n",
        "print(\"\\n\u2705 Timer started! Run all cells, then check the final cell for total runtime.\")\n",
        "print(\"=\" * 70)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \ud83d\udd35 SAFE with TOTAL_MARKDOWN Testing\n",
        "\n",
        "---\n",
        "\n",
        "## \u26a0\ufe0f **THIS IS THE TESTING VERSION**\n",
        "\n",
        "This notebook includes the new **TOTAL_MARKDOWN** metric for comparison.\n",
        "\n",
        "### **Key Changes**:\n",
        "| Metric | Source | Description |\n",
        "|--------|--------|-------------|\n",
        "| **DIGITAL_MKDN** | `redemptions_table` | Only digital/online redemption markdowns |\n",
        "| **TOTAL_MARKDOWN** | `combined_txns_table` | ALL markdowns (REVENUE - NET_SALES) |\n",
        "\n",
        "### **Testing Steps**:\n",
        "1. \u2705 Run `markdown_fix.ipynb` first to create test table\n",
        "2. \u2705 This notebook points to: `db_work.EXP_COE_COMBINED_TXNS_GCP_markdown_test`\n",
        "3. \u2705 Compare DIGITAL_MKDN vs TOTAL_MARKDOWN in outputs\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Databricks notebook source\n",
        "# DBTITLE 1,Imports\n",
        "# Imports\n",
        "%run /Workspace/Projects/Experimentation/aaml-experimentation-coe/exp_coe_utils\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Notebook set up\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "##---------------------------------------------------------------------------------------------------------------------------##\n",
        "## System & Notebook Setup\n",
        "\n",
        "## System Settings\n",
        "import pandas as pd\n",
        "pd.set_option('display.max_colwidth', 0)\n",
        "\n",
        "## Notebook Input Widgets\n",
        "\n",
        "# dbutils.widgets.text(\"01. Experiment Identifier\", \"ACIP-20404\")\n",
        "# dbutils.widgets.text(\"02. Start Date\", \"2024-01-10\")\n",
        "# dbutils.widgets.text(\"03. End Date\", \"2024-01-22\")\n",
        "# dbutils.widgets.text(\"04. Significance Level\", defaultValue=\"0.1\")\n",
        "# dbutils.widgets.dropdown(\"05. Experiment Platform\", \"Adobe\", [\"Adobe\", \"Decision Engine\", \"Push\", \"Email\", \"Household IDs\"])\n",
        "# dbutils.widgets.text(\"06. Page Filter\", \"\")\n",
        "# dbutils.widgets.dropdown(\"07. Run Exposure Query\", \"False\", [\"True\", \"False\"])\n",
        "# dbutils.widgets.dropdown(\"08. Metric Queries\", \"Standard\", [\"Standard\", \"DBFS Link\", \"Custom\"])\n",
        "# dbutils.widgets.text(\"09. DBFS Link\", \"\")\n",
        "# dbutils.widgets.combobox(\"10. Segmentation\", \"\", [\"FACTS\", \"MYNEEDS\", \"BNC\", \"FACTS-ExposureTable\", \"MyNeeds-ExposureTable\"])\n",
        "# dbutils.widgets.dropdown(\"11. Exposure Filter\", \"None\", [\"BNC\", \"SNAP\", \"Page\", \"Custom\", \"None\"])\n",
        "# dbutils.widgets.multiselect(\"12. OS Platform\", \"None\", [\"iOS\", \"Android\",\"Web\",\"None\"])\n",
        "# dbutils.widgets.multiselect(\"13. Exclude Banner Filter\", \"\", [\"\", \"pavilions\", \"safeway\",\"andronico\",\"albertsons\",\"jewel-osco\",\"vons\",\"carrsqc\",\"acme\",\"tomthumb\",\"randalls\",\"shaws\",\"balduccis\", \"haggen\" ,\"kingsfoodmarkets\",\"acmemarkets\" ])\n",
        "# dbutils.widgets.dropdown(\"14. Winsorization\", defaultValue=\"99th\", choices=[\"99th\", \"OFF\"])\n",
        "\n",
        "#dbutils.widgets.removeAll()\n",
        "\n",
        "##---------------------------------------------------------------------------------------------------------------------------##\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# # Project Plan: Optimizing the Experimentation Analysis Workflow\n",
        "# ## Executive Summary\n",
        "# **Problem:** Our current experimentation analysis notebook takes over 15 minutes to run. This is primarily due to a series of separate, sequential queries that repeatedly scan and process the same large source tables, leading to significant redundant computation, increased cluster runtime, and higher costs.\n",
        "# **Proposed Solution:** This plan outlines a project to refactor the workflow by implementing a centralized aggregation model. We will replace the ~8 separate, heavy processing jobs with a single, highly-optimized Spark job. This job will create a final, aggregated summary table that will serve as a \"single source of truth\" for all downstream analysis.\n",
        "# **Expected Outcomes:**\n",
        "# * **Cost Reduction:** Drastically lower DBU consumption by eliminating redundant data processing.\n",
        "# * **Increased Velocity:** Reduce the end-to-end runtime from 15+ minutes to just a few minutes, allowing for faster insights.\n",
        "# * **Enhanced Reliability & Consistency:** By using a single source of truth for all metrics, we ensure consistency across all reports (e.g., overall vs. segmented views) and reduce time spent on debugging.\n",
        "# ---\n",
        "# ## 1. Project Objective\n",
        "# The primary objective of this initiative is to re-architect our core A/B testing notebook to be more efficient, scalable, and maintainable. This project will directly address the current performance bottlenecks to reduce operational costs and increase the speed of analysis.\n",
        "# ---\n",
        "# ## 2. Proposed Architecture: Centralized Aggregation Model\n",
        "# The core of this project is to move from our current multi-stage query process to a streamlined, single-pass aggregation architecture.\n",
        "# The proposed workflow will perform the heavy data processing (reading multi-terabyte source tables and performing complex joins) only **once**. The output will be a clean, final summary table containing all required metrics and dimensions. Subsequent analysis and reporting notebooks will then run simple, lightning-fast queries against this much smaller, pre-aggregated table.\n",
        "# ---\n",
        "# ## 3. Scope: Key Metrics & Dimensions\n",
        "# This refactoring will support all existing metrics required for our experiment analysis, aggregated by **Variant** and **Segment**. This includes, but is not limited to:\n",
        "# * **Engagement:** CVR, Visits, Searches, Cart Adds\n",
        "# * **Transactions:** Revenue, Orders, AOV, UPO\n",
        "# * **Financials:** Margin, AGP, Markdown\n",
        "# * **Behavioral:** Clips, Bonus Points, Gas Rewards, Basket Health\n",
        "# ---\n",
        "# ## 4. Phased Implementation Plan\n",
        "# ### Phase 1: Configuration & Setup\n",
        "# *Goal: Improve maintainability and transition away from hardcoded parameters.*\n",
        "# * **1.1. Create Config File:** Establish a `config.yaml` file within the project repository to manage all notebook parameters.\n",
        "# * **1.2. Define Schema:** Structure the YAML file with clear, nested keys for different parameter groups (e.g., `date_range`, `experiment_details`, `table_paths`).\n",
        "# * **1.3. Implement Parser:** Add a cell at the beginning of the notebook to install `PyYAML`, read the config file from its path, and parse it into a Python dictionary.\n",
        "# * **1.4. Refactor Parameter Calls:** Replace all `dbutils.widgets.get()` calls throughout the notebook with dictionary lookups from the loaded config object (e.g., `config['date_range']['start']`).\n",
        "# ### Phase 2: Standardized Exposure Dataset\n",
        "# *Goal: Create a single, reliable, and reusable dataset of all unique exposed users.*\n",
        "# * **2.1. Unify Exposure Logic:** Consolidate the platform-specific SQL queries for fetching raw exposure data into a single cell.\n",
        "# * **2.2. Finalize Deduplication:** Implement the final `QUALIFY ROW_NUMBER()` logic to ensure only the first exposure event for each unique user or household is kept.\n",
        "# * **2.3. Checkpoint to Delta:** Write the cleaned exposure DataFrame to a persistent Delta table (e.g., `db_work.exposure_final_{EXPERIMENT_ID}`). This is a critical checkpoint.\n",
        "# * **2.4. Load from Checkpoint:** Immediately after writing, read from the newly created Delta table to ensure all subsequent steps use this standardized, checkpointed data.\n",
        "# ### Phase 3: Centralized Metric Processing\n",
        "# *Goal: Replace all separate metric queries with a single, unified Spark job.*\n",
        "# * **3.1. Develop the Detailed Data CTE:** Write the first part of the \"super-query,\" which creates a comprehensive transaction-level view (`ALL_TXN_DETAILS`) by joining the main transaction table with all its related detail tables (redemptions, categories, basket health).\n",
        "# * **3.2. Develop the Household Aggregation CTE:** Write the main household-level aggregation (`HOUSEHOLD_SUPER_AGG`). This involves:\n",
        "# * Joining the exposure data (from Phase 2) to the detailed data view (from 3.1) and all other metric tables (clips, gas, bonus points).\n",
        "# * Implementing a `GROUP BY household_id, variant_id, segment`.\n",
        "# * Writing the full `SELECT` clause with conditional aggregations (`SUM(CASE WHEN...)`) to calculate every required metric for every household.\n",
        "# * **3.3. Develop the Final Aggregation:** Write the final `SELECT` statement that aggregates the per-household results from the previous CTE up to the variant and segment level.\n",
        "# * **3.4. Persist the Final Summary Table:** Save the output of the \"super-query\" to a final Delta table (e.g., `my_database.experiment_metrics_final`). This table is the \"single source of truth\" for all reporting.\n",
        "# ### Phase 4: Validation & Health Checks\n",
        "# *Goal: Ensure data integrity checks are efficient and point to the new standardized datasets.*\n",
        "# * **4.1. Refactor SRM Checks:** Update the SRM check cells to read directly from the checkpointed exposure table created in Phase 2.\n",
        "# * **4.2. Verify Data Freshness Checks:** Ensure the existing queries for margin and fiscal period freshness are still functioning as expected. No changes are anticipated as these are already efficient.\n",
        "# ### Phase 5: Streamlined Reporting Layer\n",
        "# *Goal: Simplify all downstream analysis to be fast and consistent.*\n",
        "# * **5.1. Remove Redundant Notebook Cells:** **Delete** all the old, separate metric aggregation cells (`ecomm_txns_agg`, `store_txns_agg`, `clips_agg`, etc.) that have been replaced by the \"super-query\" in Phase 3.\n",
        "# * **5.2. Load Final Data:** Add a single new cell at the start of the reporting section that loads the final summary table from Phase 3.\n",
        "# * **5.3. Refactor Hypothesis Testing:** Point the `hypothesis_test_compare_means` function calls to the appropriate aggregated columns in the new final summary DataFrame.\n",
        "# * **5.4. Refactor Display Tables:** Update the pandas formatting blocks to source their data from the new final summary DataFrame. This may allow for consolidation of some formatting steps.\n",
        "# ---\n",
        "# ## 5. Future Enhancements\n",
        "# Upon successful completion of this core refactoring, we can explore further optimizations:\n",
        "# * **Driver-Node Acceleration:** For any complex final manipulations on the summary table, we can evaluate modern libraries like **DuckDB** or **FireDucks** to further accelerate processing on the driver node.\n",
        "# * **Code Modularity:** The \"super-query\" logic can be extracted into a dedicated `.sql` file and our Python functions into modules to improve long-term code maintainability.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Set up\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Optional Get Adobe Experiment ID from ACIP#\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Obtain the Adobe Experiment ID (Campaign ID)\n",
        "### If an ACIP# is input into the '01. Experiment Identifier' field, then this will look up the Campaign ID by using the CAMPAIGN_DSC field in the ADOBE_TNT table.\n",
        "### If an Adobe Experiment (Campaign ID) is provided, then this will check the ADOBE_TNT table to ensure that it is a valid ID\n",
        "\n",
        "if getArgument(\"05. Experiment Platform\") == 'Adobe':\n",
        "    if getArgument(\"01. Experiment Identifier\").split('-')[0] == 'ACIP':\n",
        "        experiment_id_query = (\n",
        "            f\"\"\"\n",
        "            SELECT DISTINCT\n",
        "            CAST(CAMPAIGN_ID AS INT) AS CAMPAIGN_ID, RECIPE_NM\n",
        "            FROM gcp-abs-udco-bqvw-prod-prj-01.udco_ds_loyl.ADOBE_TNT\n",
        "            WHERE CAMPAIGN_DSC LIKE '%{getArgument(\"01. Experiment Identifier\")}%'\n",
        "            \"\"\"\n",
        "        )\n",
        "    else:\n",
        "        experiment_id_query = (\n",
        "            f\"\"\"\n",
        "            SELECT DISTINCT\n",
        "            CAST(CAMPAIGN_ID AS INT) AS CAMPAIGN_ID, RECIPE_NM\n",
        "            FROM gcp-abs-udco-bqvw-prod-prj-01.udco_ds_loyl.ADOBE_TNT\n",
        "            WHERE CAST(CAMPAIGN_ID AS STRING) LIKE '%{getArgument(\"01. Experiment Identifier\")}%'\n",
        "            \"\"\"\n",
        "        )\n",
        "\n",
        "    experiment_id_sp = bc.read_gcp_table(experiment_id_query)\n",
        "    experiment_id_df = experiment_id_sp.select(\"*\").toPandas()\n",
        "    EXPERIMENT_ID = str(experiment_id_sp.collect()[0]['CAMPAIGN_ID'])\n",
        "\n",
        "else:\n",
        "    EXPERIMENT_ID = str(getArgument(\"01. Experiment Identifier\").replace('-', '_'))\n",
        "\n",
        "print('Experiment ID = {}'.format(EXPERIMENT_ID))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Get Inputs from Notebook\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "EXP_START_DATE = getArgument(\"02. Start Date\")\n",
        "\n",
        "### If a date in the future is provided for end date, then this will correct and run the notebook up until yesterday\n",
        "if datetime.datetime.strptime(getArgument(\"03. End Date\"), '%Y-%m-%d').date() >= datetime.date.today():\n",
        "EXP_END_DATE = str(datetime.date.today() - datetime.timedelta(days=1))\n",
        "else:\n",
        "EXP_END_DATE = getArgument(\"03. End Date\")\n",
        "\n",
        "SIGNIFICANCE = float(getArgument(\"04. Significance Level\"))\n",
        "EXP_PLATFORM = getArgument(\"05. Experiment Platform\")\n",
        "PAGE_FILTER_INPUT = getArgument(\"06. Page Filter\")\n",
        "RUN_EXPOSURE = getArgument(\"07. Run Exposure Query\")\n",
        "METRIC_QUERIES = getArgument(\"08. Metric Queries\")\n",
        "DBFS_LINK = getArgument(\"09. DBFS Link\")\n",
        "SEGMENTATION = getArgument(\"10. Segmentation\")\n",
        "if SEGMENTATION:\n",
        "SEGMENTATION = SEGMENTATION + ','\n",
        "EXPOSURE_FILTER = getArgument(\"11. Exposure Filter\")\n",
        "OS_PLATFORM = getArgument(\"12. OS Platform\")\n",
        "banner_selected = getArgument(\"13. Exclude Banner Filter\")\n",
        "WINSORIZE = getArgument(\"14. Winsorization\")\n",
        "control_variant_nm = 'VARIANT A'\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Experiment Implementation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "one_control = f\"\"\"CASE\n",
        "      WHEN post_tnt LIKE '%{EXPERIMENT_ID}:0:0%' THEN 'VARIANT A'\n",
        "      WHEN post_tnt LIKE '%{EXPERIMENT_ID}:1:0%' THEN 'VARIANT B'\n",
        "      WHEN post_tnt LIKE '%{EXPERIMENT_ID}:2:0%' THEN 'VARIANT C'\n",
        "      WHEN post_tnt LIKE '%{EXPERIMENT_ID}:3:0%' THEN 'VARIANT D'\n",
        "      WHEN post_tnt LIKE '%{EXPERIMENT_ID}:4:0%' THEN 'VARIANT E'\n",
        "      WHEN post_tnt LIKE '%{EXPERIMENT_ID}:5:0%' THEN 'VARIANT F'\n",
        "      WHEN post_tnt LIKE '%{EXPERIMENT_ID}:6:0%' THEN 'VARIANT G'\n",
        "      WHEN post_tnt LIKE '%{EXPERIMENT_ID}:7:0%' THEN 'VARIANT H'\n",
        "      ELSE NULL\n",
        "    END AS VARIANT_ID\"\"\"\n",
        " \n",
        "two_controls = f\"\"\"CASE\n",
        "      WHEN post_tnt LIKE '%{EXPERIMENT_ID}:0:0%' THEN 'VARIANT A'\n",
        "      WHEN post_tnt LIKE '%{EXPERIMENT_ID}:1:0%' THEN 'VARIANT A'\n",
        "      WHEN post_tnt LIKE '%{EXPERIMENT_ID}:2:0%' THEN 'VARIANT B'\n",
        "      WHEN post_tnt LIKE '%{EXPERIMENT_ID}:3:0%' THEN 'VARIANT C'\n",
        "      WHEN post_tnt LIKE '%{EXPERIMENT_ID}:4:0%' THEN 'VARIANT D'\n",
        "      WHEN post_tnt LIKE '%{EXPERIMENT_ID}:5:0%' THEN 'VARIANT E'\n",
        "      WHEN post_tnt LIKE '%{EXPERIMENT_ID}:6:0%' THEN 'VARIANT F'\n",
        "      WHEN post_tnt LIKE '%{EXPERIMENT_ID}:7:0%' THEN 'VARIANT G'\n",
        "      ELSE NULL\n",
        "    END AS VARIANT_ID\"\"\"\n",
        " \n",
        "if EXP_PLATFORM == 'Adobe':\n",
        "  visitor_unit = 'ADOBE_VISITOR_ID'\n",
        "  if any(experiment_id_df['RECIPE_NM'].str.contains('Control 2', case=True, regex=False)):\n",
        "    control_strategy = two_controls\n",
        "    print('2 controls')\n",
        "  else:\n",
        "    control_strategy = one_control\n",
        "    print('1 control')\n",
        "else:\n",
        "  visitor_unit = 'HOUSEHOLD_ID'\n",
        " \n",
        "if EXP_PLATFORM == 'Push':\n",
        "  push_table = 'gcp-abs-udco-bsvw-prod-prj-01.udco_ds_bizops.SQ_PUSH_DATA'\n",
        "elif EXP_PLATFORM == 'Email':\n",
        "  email_table = 'gcp-abs-udco-bsvw-prod-prj-01.udco_ds_bizops.SQ_EMAIL_DATA'\n",
        " \n",
        "print('Experiment ID = {}'.format(EXPERIMENT_ID))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DBFS Link\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Use to direct the metric queries to a pre-run set of tables, identified by the DBFS_LINK\n",
        "\n",
        "if METRIC_QUERIES == 'DBFS Link' and not DBFS_LINK:\n",
        "    displayHTML(\"\"\"<h3><font color=\"red\"> !WARNING! The Metric Query input is set to 'DBFS Link', but did not provide a link. </font></h3>\"\"\")\n",
        "\n",
        "    print('Metric Query Approach: ', METRIC_QUERIES)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Page Filter\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Page filter can accept list of strings separated by \",\" or \", \"\n",
        "### Checkout consists of multiple steps and each can contain 'order confirmation'\n",
        "\n",
        "if PAGE_FILTER_INPUT:\n",
        "    page_split = PAGE_FILTER_INPUT.replace(\", \",\",\").split(\",\")\n",
        "    page_format = [f\"'%{s}%'\".replace(\" \", \"_\") for s in page_split]\n",
        "    page_string = ', '.join(page_format)\n",
        "    \n",
        "    if EXP_PLATFORM == 'Adobe':\n",
        "        page_filter = f\"\"\"AND (lower(post_pagename) LIKE ANY ({page_string})\n",
        "or lower(POST_EVAR151) LIKE ANY ({page_string})\n",
        "or lower(POST_EVAR152) LIKE ANY ({page_string})\n",
        "or lower(POST_EVAR153) LIKE ANY ({page_string}))\"\"\"\n",
        "        print(page_filter)\n",
        "    elif EXP_PLATFORM == 'Decision Engine':\n",
        "        page_filter = f\"\"\"\n",
        "AND (lower(EVENT_TYPE_CD) LIKE '%pageloaded%' AND lower(EVENT_NM) LIKE '{page_string}')\n",
        "\"\"\"\n",
        "        print(page_filter)\n",
        "    elif EXP_PLATFORM == 'Household IDs':\n",
        "        page_filter = f\"\"\"AND PAGES_VIEWED_TXT LIKE ANY ({page_string})\"\"\"\n",
        "        print(page_filter)\n",
        "else:\n",
        "    page_filter = \"\"\n",
        "    print(\"No page filter\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Banner Filter\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if banner_selected != 'None' and banner_selected.strip() != '':\n",
        "    # Splitting the input string by commas and converting to lowercase\n",
        "    selected_banners = banner_selected.split(',')\n",
        "    selected_banners = [x.strip().lower() for x in selected_banners]\n",
        "    # Filtering out values that start with \"business\"\n",
        "    selected_banners = [x for x in selected_banners if not x.startswith(\"business\")]\n",
        "    if EXP_PLATFORM == 'Adobe':\n",
        "        banner_filter = \"AND lower(POST_EVAR4) NOT LIKE 'business%'\"\n",
        "        if selected_banners:\n",
        "            banner_filter += \" AND lower(POST_EVAR4) NOT IN (\" + \", \".join(f\"'{b}'\" for b in selected_banners) + \")\"\n",
        "    elif EXP_PLATFORM == 'Decision Engine':\n",
        "        banner_filter = \"AND lower(BANNER_NM) NOT LIKE 'business%'\"\n",
        "        if selected_banners:\n",
        "            banner_filter += \" AND lower(BANNER_NM) NOT IN (\" + \", \".join(f\"'{b}'\" for b in selected_banners) + \")\"\n",
        "else:\n",
        "    banner_filter = \"\"\n",
        "print(banner_filter)\n",
        "print(banner_selected)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# OS Platform Filter\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if OS_PLATFORM != 'None':\n",
        "    if EXP_PLATFORM == 'Adobe':\n",
        "        os_platform_filter = []\n",
        "        if 'iOS' in OS_PLATFORM:\n",
        "            os_platform_filter.append(\"(LOWER(POST_EVAR90) LIKE '%app%' AND LOWER(POST_EVAR116) LIKE '%ios%')\")\n",
        "        if 'Android' in OS_PLATFORM:\n",
        "            os_platform_filter.append(\"(LOWER(POST_EVAR90) LIKE '%app%' AND LOWER(POST_EVAR116) LIKE '%android%')\")\n",
        "        if 'Web' in OS_PLATFORM:\n",
        "            os_platform_filter.append(\"(LOWER(POST_EVAR90) LIKE '%web%')\")\n",
        "        if os_platform_filter:\n",
        "            os_platform_filter = \"AND (\" + \" OR \".join(os_platform_filter) + \")\"\n",
        "        else:\n",
        "            os_platform_filter = \"\"\n",
        "    elif EXP_PLATFORM == 'Decision Engine':\n",
        "        os_platform_filter = []\n",
        "        if 'iOS' in OS_PLATFORM:\n",
        "            os_platform_filter.append(\"(LOWER(APP_VERSION_CD) LIKE '%appios%')\")\n",
        "        if 'Android' in OS_PLATFORM:\n",
        "            os_platform_filter.append(\"(LOWER(APP_VERSION_CD) LIKE '%appand%')\")\n",
        "        if 'Web' in OS_PLATFORM:\n",
        "            os_platform_filter.append(\"(LOWER(APP_VERSION_CD) LIKE '%web%' OR APP_VERSION_CD IS NULL)\")\n",
        "        if os_platform_filter:\n",
        "            os_platform_filter = \"AND (\" + \" OR \".join(os_platform_filter) + \")\"\n",
        "        else:\n",
        "            os_platform_filter = \"\"\n",
        "    else:\n",
        "        os_platform_filter = \"\"\n",
        "else:\n",
        "    os_platform_filter = \"\"\n",
        "\n",
        "print(os_platform_filter)\n",
        "print(OS_PLATFORM)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3P HouseHold filter\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "household_3p_filter = f''' (safe_cast(HOUSEHOLD_ID as INT) not in (select HOUSEHOLD_ID from gcp-abs-udco-bsvw-prod-prj-01.aamp_ds_datascience.EXP_COE_3P_HHS_LIST) OR HOUSEHOLD_ID is NULL) '''\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Winsorize SQL\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if WINSORIZE == '99th':\n",
        "    ## If 99th percentile is equal to $0, then winsorization is ignored.\n",
        "    metric_rpc_sql = '''CASE WHEN (SELECT TOT_REVENUE_WIN99 FROM WINZ) > 0 AND TOT_REVENUE > (SELECT TOT_REVENUE_WIN99 FROM WINZ) THEN (SELECT TOT_REVENUE_WIN99 FROM WINZ) ELSE TOT_REVENUE END'''\n",
        "else:\n",
        "    metric_rpc_sql = 'TOT_REVENUE'\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exposure Check\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "  exposure_sp_check = spark.sql(\"SELECT MIN(DATE(EXPOSURE_DATETIME)) as FIRST_EXPOSED_DATE, MAX(DATE(EXPOSURE_DATETIME)) as LAST_EXPOSED_DATE FROM experimentation.exposure_sp_{}\".format(EXPERIMENT_ID))\n",
        "  display(exposure_sp_check)\n",
        "  \n",
        "except:\n",
        "  print('No exposure table available.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DBFS Check\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if METRIC_QUERIES == 'Standard':\n",
        "  standard_sp_check = spark.sql(\"SELECT MIN(DATE(TXN_DTE)) as FIRST_DATE, MAX(DATE(TXN_DTE)) as LAST_DATE FROM db_work.EXP_COE_COMBINED_TXNS_GCP\")\n",
        "  standard_check = standard_sp_check.select(\"*\").toPandas()\n",
        " \n",
        "  if (pd.to_datetime(EXP_START_DATE) < pd.to_datetime(standard_check['FIRST_DATE'][0])) or (pd.to_datetime(EXP_END_DATE) > pd.to_datetime(standard_check['LAST_DATE'][0])):\n",
        "      displayHTML(f\"\"\"\n",
        "                  <h3><font color=\"red\"> EXPERIMENT RUN DATES EXCEED STANDARD METRIC DATA PIPELINE! THE STANDARD PIPELINE AGGREGATES THE PREVIOUS 2 MONTHS OF DATA, IF THE EXPERIMENT RUN DATES EXCEED THIS TIME FRAME, THEN USE A CUSTOM TIME FRAME OR EXISTING DBFS LINK. </font></h3>\n",
        "                  <p><font color=\"black\"> First Metric Date:  {standard_check['FIRST_DATE'][0]} </font></p>\n",
        "                  <p><font color=\"black\"> Last Metric Date:  {standard_check['LAST_DATE'][0]} </font></p>\n",
        "                  \"\"\")\n",
        "  else:\n",
        "      displayHTML(f\"\"\"\n",
        "                  <h3><font color=\"green\"> EXPERIMENT RUN DATES ARE WITHIN STANDARD METRIC DATA DATES :) </font></h3>\n",
        "                  <p><font color=\"black\"> First Date:  {standard_check['FIRST_DATE'][0]} </font></p>\n",
        "                  <p><font color=\"black\"> Last Date:  {standard_check['LAST_DATE'][0]} </font></p>\n",
        "                  \"\"\")\n",
        " \n",
        "elif METRIC_QUERIES == 'DBFS Link':\n",
        "  dbfs_sp_check = spark.sql(\"SELECT MIN(DATE(TXN_DTE)) as FIRST_DBFS_DATE, MAX(DATE(TXN_DTE)) as LAST_DBFS_DATE FROM db_work.combined_txns_sp_r1_{}\".format(DBFS_LINK))\n",
        "  dbfs_check = dbfs_sp_check.select(\"*\").toPandas()\n",
        " \n",
        "  if (pd.to_datetime(EXP_START_DATE) < pd.to_datetime(dbfs_check['FIRST_DBFS_DATE'][0])) or (pd.to_datetime(EXP_END_DATE) > pd.to_datetime(dbfs_check['LAST_DBFS_DATE'][0])):\n",
        "    displayHTML(f\"\"\"\n",
        "                  <h3><font color=\"red\"> EXPERIMENT RUN DATES EXCEED DBFS LINK! CHECK DATES AND DBFS LINK AND RE-RUN! </font></h3>\n",
        "                  <p><font color=\"black\"> First DBFS Date:  {dbfs_check['FIRST_DBFS_DATE'][0]} </font></p>\n",
        "                  <p><font color=\"black\"> Last DBFS Date:  {dbfs_check['LAST_DBFS_DATE'][0]} </font></p>\n",
        "                  \"\"\")\n",
        "  else:\n",
        "    displayHTML(f\"\"\"\n",
        "                  <h3><font color=\"green\"> EXPERIMENT RUN DATES ARE WITHIN DBFS DATES :) </font></h3>\n",
        "                  <p><font color=\"black\"> First DBFS Date:  {dbfs_check['FIRST_DBFS_DATE'][0]} </font></p>\n",
        "                  <p><font color=\"black\"> Last DBFS Date:  {dbfs_check['LAST_DBFS_DATE'][0]} </font></p>\n",
        "                  \"\"\")\n",
        " \n",
        "else:\n",
        "  displayHTML(\"\"\"\n",
        "                <h3><font color=\"black\"> CUSTOM METRIC QUERY USED. </font></h3>\n",
        "                <h3><font color=\"orange\"> !WARNING! May take longer to run the notebook with this setting.  For shorter runs, please use the Standard or the DBFS options for the Metric Query parameter. </font></h3>   \n",
        "                <p><font color=\"black\"> Experiment_ID: {} </font></p> \"\"\".format(EXPERIMENT_ID))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Metric Query Table Logic\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### If a DBFS link is provided, then the notebook will point to those tables.\n",
        "### If metric queries are set to \"Custom\" then this notebook will kick off a worfkflow for metrics during the time of the experiment and save them into a DBFS location.\n",
        "### If \"Standard\", then the notebook will access data from the standard metric workflows that execute daily.\n",
        "\n",
        "if METRIC_QUERIES != 'Standard':\n",
        "    if METRIC_QUERIES == 'DBFS Link':\n",
        "        table_suffix = DBFS_LINK\n",
        "    \n",
        "    elif METRIC_QUERIES == 'Custom':\n",
        "        table_suffix = EXPERIMENT_ID\n",
        "    \n",
        "    visit_order_table = f\"\"\"db_work.visit_order_sp_{table_suffix}\"\"\"\n",
        "    cart_coupon_table = f\"\"\"db_work.cart_coupon_sp_{table_suffix}\"\"\"\n",
        "    margin_table = f\"\"\"db_work.margin_sp_{table_suffix}\"\"\"\n",
        "    agp_table = f\"\"\"db_work.agp_sp_{table_suffix}\"\"\"\n",
        "    combined_txn_table = f\"\"\"db_work.combined_txns_sp_r1_{table_suffix}\"\"\"\n",
        "    redemptions_table = f\"\"\"db_work.redemptions_sp_r1_{table_suffix}\"\"\"\n",
        "    clips_table = f\"\"\"db_work.clips_sp_{table_suffix}\"\"\"\n",
        "    gas_table = f\"\"\"db_work.gas_txns_sp_{table_suffix}\"\"\"\n",
        "    bnc_table = f\"\"\"db_work.bnc_sp_{table_suffix}\"\"\"\n",
        "    email_push_table = f\"\"\"db_work.email_push_sp_{table_suffix}\"\"\"\n",
        "    basket_health_table = f\"\"\"db_work.basket_health_sp_{table_suffix}\"\"\"\n",
        "    category_table = f\"\"\"db_work.category_sp_{table_suffix}\"\"\"\n",
        "    account_health_table = f\"\"\"db_work.account_health_sp_{table_suffix}\"\"\"\n",
        "\n",
        "else:\n",
        "    visit_order_table = \"db_work.EXP_COE_VISIT_ORDER_GCP\"\n",
        "    cart_coupon_table = \"db_work.EXP_COE_CART_COUPON_GCP\"\n",
        "    margin_table = \"db_work.EXP_COE_MARGIN_GCP\"\n",
        "    agp_table = \"db_work.EXP_COE_AGP_GCP\"\n",
        "    # combined_txn_table = \"db_work.EXP_COE_COMBINED_TXNS_GCP\"  # ORIGINAL\n",
        "  combined_txn_table = \"db_work.EXP_COE_COMBINED_TXNS_GCP_markdown_test\"  # TEST TABLE\n",
        "    redemptions_table = \"db_work.EXP_COE_REDEMPTIONS_GCP\"\n",
        "    clips_table = \"db_work.EXP_COE_CLIPS_GCP\"\n",
        "    gas_table = \"db_work.EXP_COE_GAS_TXNS_GCP\"\n",
        "    bnc_table = \"db_work.EXP_COE_BNC_GCP\"\n",
        "    email_push_table = \"db_work.EXP_COE_EMAIL_PUSH_OPT_GCP\"\n",
        "    basket_health_table = \"db_work.EXP_COE_BASKET_HEALTH_GCP\"\n",
        "    category_table = \"db_work.EXP_COE_CATEGORY_TXNS_GCP\"\n",
        "    account_health_table = \"db_work.EXP_COE_ACC_HEALTH_GCP\"\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# STOP Notebook if DBFS does not match Run Dates\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if METRIC_QUERIES == 'Standard':\n",
        "    if (pd.to_datetime(EXP_START_DATE) < pd.to_datetime(standard_check['FIRST_DATE'][0])) or (pd.to_datetime(EXP_END_DATE) > pd.to_datetime(standard_check['LAST_DATE'][0])):\n",
        "        dbutils.notebook.exit(\"Experiment Run dates exceed Standard Metric Data dates. Further tasks will be skipped\")\n",
        "    elif METRIC_QUERIES == 'DBFS Link':\n",
        "        if (pd.to_datetime(EXP_START_DATE) < pd.to_datetime(dbfs_check['FIRST_DBFS_DATE'][0])) or (pd.to_datetime(EXP_END_DATE) > pd.to_datetime(dbfs_check['LAST_DBFS_DATE'][0])):\n",
        "            dbutils.notebook.exit(\"Experiment Run dates exceed DBFS Link. Further tasks will be skipped\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CUSTOM Metric Flow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if METRIC_QUERIES == 'Custom':\n",
        "    dbutils.notebook.run(\n",
        "    \"/Workspace/Projects/Experimentation/aaml-experimentation-coe-dev/Workflows/EXP COE - Metric Workflow\",\n",
        "    0,\n",
        "    {\n",
        "    \"01. Start Date\": EXP_START_DATE,\n",
        "    \"02. End Date\": EXP_END_DATE,\n",
        "    \"03. Experiment ID\": EXPERIMENT_ID,\n",
        "    \"04. Metric Selection\": \"SAFE\"\n",
        "    }\n",
        "    )\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Exposures\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exposure Query\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if RUN_EXPOSURE == 'True':\n",
        "  if EXP_PLATFORM == 'Adobe':\n",
        "    exposure_query = (\n",
        "        f\"\"\"\n",
        "        with UNFILTERED_EXPOSURE as(\n",
        "        SELECT\n",
        "            CONCAT(post_visid_high, post_visid_low) AS ID,\n",
        "            post_evar49 AS ADOBE_VISITOR_ID,\n",
        "            SAFE_CAST(post_evar46 AS INT) AS CLUBCARD_ID,\n",
        "            FIRST_VALUE (SAFE_CAST(post_evar47 AS INT) IGNORE NULLS) OVER (PARTITION BY CONCAT(post_visid_high, post_visid_low) ORDER BY DATE_TIME ASC ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS HOUSEHOLD_ID,\n",
        "            {control_strategy},\n",
        "            DATE(DATE_TIME) AS EXPOSURE_DATETIME\n",
        "          FROM gcp-abs-udco-bqvw-prod-prj-01.udco_ds_cust.CLICK_HIT_DATA\n",
        "          WHERE 1=1\n",
        "            AND DATE(DW_CREATETS) > DATE_ADD(DATE('{EXP_START_DATE}'), INTERVAL -1 DAY)\n",
        "            AND (DATE(DATE_TIME) >= '{EXP_START_DATE}' AND DATE(DATE_TIME) <= '{EXP_END_DATE}')\n",
        "            AND POST_TNT LIKE '%{EXPERIMENT_ID}%'\n",
        "            AND CAST(EXCLUDE_HIT AS INT) = 0\n",
        "            AND CAST(hit_source AS INT) NOT IN (5,7,8,9)\n",
        "            {page_filter}\n",
        "            {os_platform_filter}\n",
        "            {banner_filter}\n",
        "          QUALIFY ROW_NUMBER() OVER(PARTITION BY ADOBE_VISITOR_ID ORDER BY DATE_TIME ASC) = 1\n",
        "          )\n",
        " \n",
        "          select *\n",
        "          from UNFILTERED_EXPOSURE\n",
        "          WHERE {household_3p_filter}\n",
        "          \"\"\"\n",
        "        )\n",
        "    \n",
        "  elif EXP_PLATFORM == 'Decision Engine':\n",
        "    exposure_query = (\n",
        "        f\"\"\"\n",
        "        SELECT\n",
        "            HOUSEHOLD_ID\n",
        "          , CASE\n",
        "              WHEN DECISION_ENGINE_VARIANT_CD = 'A' THEN 'VARIANT A'\n",
        "              WHEN DECISION_ENGINE_VARIANT_CD = 'B' THEN 'VARIANT B'\n",
        "              WHEN DECISION_ENGINE_VARIANT_CD = 'C' THEN 'VARIANT C'\n",
        "              WHEN DECISION_ENGINE_VARIANT_CD = 'D' THEN 'VARIANT D'\n",
        "              WHEN DECISION_ENGINE_VARIANT_CD = 'E' THEN 'VARIANT E'\n",
        "              WHEN DECISION_ENGINE_VARIANT_CD = 'F' THEN 'VARIANT F'\n",
        "              WHEN DECISION_ENGINE_VARIANT_CD = 'G' THEN 'VARIANT G'\n",
        "            ELSE NULL\n",
        "          END AS VARIANT_ID\n",
        "          , DATE(TIMESTAMP(EVENT_TS,'America/Boise')) AS EXPOSURE_DATETIME\n",
        "        FROM gcp-abs-udco-bqvw-prod-prj-01.udco_ds_cust.CUSTOMER_SESSION_EVENT_MASTER\n",
        "        WHERE DECISION_ENGINE_EXPERIMENT_ID LIKE '%{EXPERIMENT_ID}%'\n",
        "          AND DATE(TIMESTAMP(EVENT_TS,'America/Boise')) BETWEEN  '{EXP_START_DATE}' and '{EXP_END_DATE}'\n",
        "          AND CAST(DECISION_ENGINE_VARIANT_CD AS STRING) IN ('A','B','C','D','E','F','G')\n",
        "          {page_filter}\n",
        "          {os_platform_filter}\n",
        "          {banner_filter}\n",
        "          AND {household_3p_filter}\n",
        "        QUALIFY ROW_NUMBER() OVER(PARTITION BY DECISION_ENGINE_EXPERIMENT_ID,HOUSEHOLD_ID,VARIANT_ID ORDER BY EVENT_TS)=1\n",
        "        \"\"\"\n",
        "      )\n",
        "    \n",
        "  elif ((EXP_PLATFORM == 'Push') | (EXP_PLATFORM == 'Email')):\n",
        "    exposure_query = (\n",
        "      \"\"\"\n",
        "        #### TODO: NEED TO UPDATE THIS\n",
        "      \"\"\"\n",
        "      )\n",
        "  \n",
        "  elif EXP_PLATFORM == 'Household IDs':\n",
        " \n",
        "    # FACTS Column check\n",
        "    column_check_query = f'''\n",
        "      SELECT column_name\n",
        "      FROM gcp-abs-udco-bsvw-prod-prj-01.aamp_ds_datascience.INFORMATION_SCHEMA.COLUMNS\n",
        "      WHERE table_name = '{EXPERIMENT_ID}'\n",
        "      '''\n",
        "    column_name_list = bc.read_gcp_table(column_check_query).select(\"column_name\").rdd.flatMap(lambda x: x).collect()\n",
        "    if 'ANNUAL_FACTS' in column_name_list:\n",
        "      FACTS_COLUMN_PULL = 'ANNUAL_FACTS as FACTS'\n",
        "    elif 'TWELVE_WEEKS_FACTS' in column_name_list:\n",
        "      FACTS_COLUMN_PULL = 'TWELVE_WEEKS_FACTS as FACTS'\n",
        "    else:\n",
        "      FACTS_COLUMN_PULL = 'FACTS'\n",
        "    \n",
        "    # Exposure Query\n",
        "    exposure_query = (\n",
        "      f\"\"\"\n",
        "      SELECT DISTINCT\n",
        "        SAFE_CAST(HOUSEHOLD_ID AS INT) as HOUSEHOLD_ID\n",
        "        , CASE\n",
        "            WHEN VARIANT = 'A' THEN 'VARIANT A'\n",
        "            WHEN VARIANT = 'B' THEN 'VARIANT B'\n",
        "            WHEN VARIANT = 'C' THEN 'VARIANT C'\n",
        "            WHEN VARIANT = 'D' THEN 'VARIANT D'\n",
        "            WHEN VARIANT = 'E' THEN 'VARIANT E'\n",
        "            WHEN VARIANT = 'F' THEN 'VARIANT F'\n",
        "            WHEN VARIANT = 'G' THEN 'VARIANT G'\n",
        "            WHEN VARIANT = 'H' THEN 'VARIANT H'\n",
        "            WHEN VARIANT = 'I' THEN 'VARIANT I'\n",
        "            ELSE NULL\n",
        "          END AS VARIANT_ID,\n",
        "          {FACTS_COLUMN_PULL},\n",
        "          MYNEEDS,\n",
        "          {SEGMENTATION}\n",
        "          CAST('{EXP_START_DATE}' AS DATE) AS EXPOSURE_DATETIME\n",
        "      FROM gcp-abs-udco-bsvw-prod-prj-01.aamp_ds_datascience.{EXPERIMENT_ID}\n",
        "      where {household_3p_filter}\n",
        "      \"\"\"\n",
        "      )\n",
        " \n",
        "  print(exposure_query)\n",
        "  # exposure_sp = bc.read_gcp_table(exposure_query)\n",
        "  # exposure_sp.createOrReplaceTempView('exposure_sp')\n",
        " \n",
        "  exposure_sp_raw = bc.read_gcp_table(exposure_query)\n",
        "  exposure_sp_raw.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(f'''experimentation.exposure_sp_raw_{EXPERIMENT_ID}''')  \n",
        "  \n",
        "  exposure_sp = spark.table(f'''experimentation.exposure_sp_raw_{EXPERIMENT_ID}''')\n",
        "  exposure_sp.createOrReplaceTempView('exposure_sp')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SRM By Date\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql import Window\n",
        "\n",
        "if EXP_PLATFORM != 'Household IDs':\n",
        "    distinct_counts = (exposure_sp.groupBy('VARIANT_ID', 'EXPOSURE_DATETIME').agg(F.countDistinct(visitor_unit).alias('distinct_visitors')))\n",
        "    windowval = (Window.partitionBy('VARIANT_ID').orderBy('EXPOSURE_DATETIME').rangeBetween(Window.unboundedPreceding, 0))\n",
        "\n",
        "    exposure_rolling = (distinct_counts\n",
        "    .withColumn('visitors_rolling', F.sum('distinct_visitors').over(windowval)))\n",
        "\n",
        "    exposure_control = (exposure_rolling.filter(F.col('VARIANT_ID') == 'VARIANT A')\n",
        "    .select('EXPOSURE_DATETIME','distinct_visitors','visitors_rolling')\n",
        "    .withColumnRenamed('distinct_visitors','distinct_visitors_control')\n",
        "    .withColumnRenamed('visitors_rolling','visitors_rolling_control'))\n",
        "\n",
        "    exposure_rolling = exposure_rolling.join(exposure_control, on='EXPOSURE_DATETIME', how='left')\n",
        "    exposure_rolling = (exposure_rolling\n",
        "    .withColumn('distinct_visitors_diff', F.col('distinct_visitors') - F.col('distinct_visitors_control'))\n",
        "    .withColumn('visitors_rolling_diff', F.col('visitors_rolling') - F.col('visitors_rolling_control')))\n",
        "\n",
        "\n",
        "    #==================================================================================================================================\n",
        "    # Problem Area 1:\n",
        "    #==================================================================================================================================\n",
        "    # exposure_rolling_df = exposure_rolling.toPandas()\n",
        "    # if exposure_rolling_df['VARIANT_ID'].unique().shape[0] <= 2:\n",
        "        # exposure_diff = exposure_rolling_df[exposure_rolling_df['VARIANT_ID'] != 'VARIANT A'][['EXPOSURE_DATETIME','distinct_visitors_diff', 'visitors_rolling_diff']]\n",
        "    # else:\n",
        "        # exposure_diff = exposure_rolling_df[exposure_rolling_df['VARIANT_ID'] != 'VARIANT A'][['VARIANT_ID','EXPOSURE_DATETIME','distinct_visitors_diff', 'visitors_rolling_diff']]\n",
        "\n",
        "\n",
        "\n",
        "    #======================================================================================================================================\n",
        "    # Solution For Area 1:\n",
        "    # This logic is now performed by Spark in a distributed manner,Benefit: This logic is now performed by Spark in a distributed manner\n",
        "    #======================================================================================================================================\n",
        "\n",
        "    exposure_diff_intermediate_sp = exposure_rolling.filter(F.col('VARIANT_ID') != 'VARIANT A')\n",
        "    unique_variant_count_sp = exposure_rolling.select('VARIANT_ID').distinct().count() # Calculate in Spark\n",
        "    if unique_variant_count_sp <= 2:\n",
        "        exposure_diff_base_sp = exposure_diff_intermediate_sp.select(\n",
        "    'EXPOSURE_DATETIME',\n",
        "    'distinct_visitors_diff',\n",
        "    'visitors_rolling_diff'\n",
        "    ).distinct() # Add distinct if needed, as filtering out 'VARIANT A' might leave multiple rows for the other variant per date if it was part of a multi-variant group.\n",
        "    else:\n",
        "        exposure_diff_base_sp = exposure_diff_intermediate_sp.select(\n",
        "    'VARIANT_ID', # Keep VARIANT_ID if multiple non-control variants\n",
        "        'EXPOSURE_DATETIME',\n",
        "    'distinct_visitors_diff',\n",
        "    'visitors_rolling_diff'\n",
        "    )\n",
        "    #==================================================================================================================================\n",
        "    # Problem Area 2:\n",
        "    #==================================================================================================================================\n",
        "\n",
        "    # for i in list(exposure_rolling_df['EXPOSURE_DATETIME'].unique()):\n",
        "    # visitor_counts = exposure_rolling_df.loc[exposure_rolling_df['EXPOSURE_DATETIME'] == i, 'visitors_rolling']\n",
        "    # exposure_diff.loc[exposure_diff['EXPOSURE_DATETIME'] == i, 'ADOBE_SRM p-value'] = round(srm_test(visitor_counts,exp_platform = EXP_PLATFORM)[1],4)\n",
        "\n",
        "    # display(exposure_diff.sort_values('EXPOSURE_DATETIME'))\n",
        "\n",
        "    #======================================================================================================================================\n",
        "    # Solution For Area 2:\n",
        "    # This logic is now performed by Spark in a distributed manner,Benefit: This logic is now performed by Spark in a distributed manner\n",
        "    #======================================================================================================================================\n",
        "    # Apply UDF to the relevant part of 'exposure_rolling' Spark DataFrame\n",
        "    srm_p_values_sp = exposure_rolling.select(\"EXPOSURE_DATETIME\", \"VARIANT_ID\", \"visitors_rolling\") \\\n",
        "    .groupBy(\"EXPOSURE_DATETIME\") \\\n",
        "    .apply(calculate_srm_for_date_udf)\n",
        "\n",
        "\n",
        "    #======================================================================================================================================\n",
        "    # Solution For Area 2: Build Sparkand UDF Function\n",
        "    # This function goes in stats.py\n",
        "    #======================================================================================================================================\n",
        "    # from pyspark.sql.types import StructType, StructField, DoubleType, DateType\n",
        "    # import pandas as pd_udf_internal # For UDF\n",
        "    # import scipy.stats as scipy_stats_udf # For UDF\n",
        "    # # (srm_test function)\n",
        "    # # (EXP_PLATFORM is a global variable)\n",
        "    # srm_by_date_schema = StructType([\n",
        "    # StructField(\"EXPOSURE_DATETIME_udf\", DateType(), True),\n",
        "    # StructField(\"ADOBE_SRM_p_value_udf\", DoubleType(), True)\n",
        "    # ])\n",
        "    # @pandas_udf(srm_by_date_schema, PandasUDFType.GROUPED_MAP)\n",
        "    # def calculate_srm_for_date_udf(pdf_group_for_srm):\n",
        "    # # pdf_group_for_srm is a Pandas DataFrame for a single EXPOSURE_DATETIME.\n",
        "    # # It contains 'VARIANT_ID' and 'visitors_rolling' for that date.\n",
        "    # # --- srm_test_internal_udf (your srm_test function) ---\n",
        "    # def srm_test_internal_udf(observed, exp_platform_udf_val):\n",
        "    # number_of_treatments = len(observed)\n",
        "    # if observed.empty or number_of_treatments == 0 or observed.sum() == 0:\n",
        "        # return (None, float('nan')) # Or however srm_test handles this.\n",
        "    # if (number_of_treatments == 3) and ((exp_platform_udf_val == 'Adobe') or (exp_platform_udf_val == 'Decision Engine')): expected_proportion = [.34, .33, .33]\n",
        "        # elif (number_of_treatments == 6) and (exp_platform_udf_val == 'Adobe'): expected_proportion = [.17, .17, .17, .17, .16, .16]\n",
        "        # else: expected_proportion = [1/number_of_treatments] * number_of_treatments\n",
        "        # total_traffic= sum(observed)\n",
        "    # expected_traffic = pd_udf_internal.Series(expected_proportion)*float(total_traffic)\n",
        "    # chi = scipy_stats_udf.chisquare(observed.to_numpy(), f_exp=expected_traffic.to_numpy())\n",
        "    # return chi\n",
        "    # # --- End srm_test_internal_udf ---\n",
        "    # current_date = pdf_group_for_srm['EXPOSURE_DATETIME'].iloc[0]\n",
        "    # observed_counts = pdf_group_for_srm.sort_values('VARIANT_ID')['visitors_rolling'] # Ensure consistent order\n",
        "    # srm_result = srm_test_internal_udf(observed_counts, EXP_PLATFORM) # EXP_PLATFORM is global\n",
        "    # p_value = round(srm_result[1], 4) if srm_result and not pd_udf_internal.isna(srm_result[1]) else float('nan')\n",
        "        # return pd_udf_internal.DataFrame([[current_date, p_value]], columns=[\"EXPOSURE_DATETIME_udf\", \"ADOBE_SRM_p_value_udf\"])\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql import Window\n",
        " \n",
        "if EXP_PLATFORM != 'Household IDs':\n",
        "  distinct_counts = (exposure_sp.groupBy('VARIANT_ID', 'EXPOSURE_DATETIME').agg(F.countDistinct(visitor_unit).alias('distinct_visitors')))\n",
        "  windowval = (Window.partitionBy('VARIANT_ID').orderBy('EXPOSURE_DATETIME').rangeBetween(Window.unboundedPreceding, 0))\n",
        " \n",
        "  exposure_rolling = (distinct_counts\n",
        "                      .withColumn('visitors_rolling', F.sum('distinct_visitors').over(windowval)))\n",
        "  \n",
        "  exposure_control = (exposure_rolling.filter(F.col('VARIANT_ID') == 'VARIANT A')\n",
        "                      .select('EXPOSURE_DATETIME','distinct_visitors','visitors_rolling')\n",
        "                      .withColumnRenamed('distinct_visitors','distinct_visitors_control')\n",
        "                      .withColumnRenamed('visitors_rolling','visitors_rolling_control'))\n",
        "  \n",
        "  exposure_rolling = exposure_rolling.join(exposure_control, on='EXPOSURE_DATETIME', how='left')\n",
        "  exposure_rolling = (exposure_rolling\n",
        "                      .withColumn('distinct_visitors_diff', F.col('distinct_visitors') - F.col('distinct_visitors_control'))\n",
        "                      .withColumn('visitors_rolling_diff', F.col('visitors_rolling') - F.col('visitors_rolling_control')))\n",
        " \n",
        " \n",
        "  exposure_rolling_df = exposure_rolling.toPandas()\n",
        "  if exposure_rolling_df['VARIANT_ID'].unique().shape[0] <= 2:\n",
        "    exposure_diff = exposure_rolling_df[exposure_rolling_df['VARIANT_ID'] != 'VARIANT A'][['EXPOSURE_DATETIME','distinct_visitors_diff', 'visitors_rolling_diff']]\n",
        "  else:\n",
        "    exposure_diff = exposure_rolling_df[exposure_rolling_df['VARIANT_ID'] != 'VARIANT A'][['VARIANT_ID','EXPOSURE_DATETIME','distinct_visitors_diff', 'visitors_rolling_diff']]\n",
        " \n",
        "  for i in list(exposure_rolling_df['EXPOSURE_DATETIME'].unique()):\n",
        "    visitor_counts = exposure_rolling_df.loc[exposure_rolling_df['EXPOSURE_DATETIME'] == i, 'visitors_rolling']\n",
        "    exposure_diff.loc[exposure_diff['EXPOSURE_DATETIME'] == i, 'ADOBE_SRM p-value'] = round(srm_test(visitor_counts,exp_platform = EXP_PLATFORM)[1],4)\n",
        " \n",
        "  display(exposure_diff.sort_values('EXPOSURE_DATETIME'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if EXP_PLATFORM == 'Adobe':\n",
        "    srm_check_raw = exposure_sp.groupBy('VARIANT_ID').agg(\n",
        "    F.countDistinct('HOUSEHOLD_ID').alias(\"HH_COUNT\"),\n",
        "    F.countDistinct('ADOBE_VISITOR_ID').alias(\"Adobe_Count\")).sort('VARIANT_ID').toPandas()\n",
        "\n",
        "    display(srm_check_raw)\n",
        "\n",
        "    print('Adobe SRM p-value:', round(srm_test(srm_check_raw['Adobe_Count'],exp_platform = EXP_PLATFORM)[1],4))\n",
        "    print('HH SRM p-value:', round(srm_test(srm_check_raw['HH_COUNT'],exp_platform = EXP_PLATFORM)[1],4))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exposure Filter\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if EXPOSURE_FILTER == 'BNC':\n",
        "  filter_query = (\n",
        "    f\"\"\"\n",
        "    SELECT HHS_ID AS HOUSEHOLD_ID,\n",
        "          SEGMENT_1 AS SEGMENTATION\n",
        "    FROM gcp-abs-udco-bsvw-prod-prj-01.udco_ds_bizops.ESA_HOLISTIC_SCORECARD_TXN_CUSTOMER_SEGMENT\n",
        "    WHERE TXN_DTE BETWEEN '{EXP_START_DATE}' AND '{EXP_END_DATE}'  \n",
        "      AND SEGMENT_1 LIKE'BNC TO ECOMM'\n",
        "    \"\"\")\n",
        "  \n",
        "elif EXPOSURE_FILTER == 'SNAP':\n",
        "  filter_query = (\n",
        "    f\"\"\"\n",
        "    SELECT DISTINCT\n",
        "            smv.HOUSEHOLD_ID\n",
        "    FROM gcp-abs-udco-bqvw-prod-prj-01.udco_ds_retl.TXN_HDR_COMBINED AS f\n",
        "    JOIN gcp-abs-udco-bqvw-prod-prj-01.udco_ds_edw.LU_DAY_MERGE AS b\n",
        "      ON CAST(f.TXN_DTE AS DATE) = b.D_DATE\n",
        "    JOIN gcp-abs-udco-bsvw-prod-prj-01.udco_ds_bizops.LU_STOREID_DIVISION AS d\n",
        "      ON f.STORE_ID = d.STORE_ID\n",
        "    JOIN (SELECT DISTINCT HOUSEHOLD_ID, LOYALTY_PROGRAM_CARD_NBR FROM gcp-abs-udco-bqvw-prod-prj-01.udco_ds_cust.SMV_RETAIL_CUSTOMER_LOYALTY_PROGRAM_HOUSEHOLD) as smv\n",
        "      ON f.CARD_NBR = smv.LOYALTY_PROGRAM_CARD_NBR\n",
        "    WHERE 1=1\n",
        "      AND (f.TXN_DTE >= '{EXP_START_DATE}' AND f.TXN_DTE <= '{EXP_END_DATE}')\n",
        "      AND f.TXN_HDR_SRC_CD = 0\n",
        "      AND f.REGISTER_NBR IN (99,173,174,999,16,17,18,19,20,49,50,51,52,53,54,93,94,95,96,97,98,151,152,153,154,175,176,177,178,179,180,181,182,195,196,197,198,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,116,117,118,119,120,121,122,123,124,125)\n",
        "      AND (f.TENDER_AMT_FOODSTAMPS + f.TENDER_AMT_EBT) > 0\n",
        "      \"\"\")\n",
        " \n",
        "elif EXPOSURE_FILTER == 'Page or OS platform':\n",
        "  filter_query = f\"\"\"\n",
        "    SELECT DISTINCT SAFE_CAST(RETAIL_CUSTOMER_HHID_TXT AS INT) AS HOUSEHOLD_ID\n",
        "    FROM gcp-abs-udco-bqvw-prod-prj-01.udco_ds_cust.CLICK_STREAM_VISIT_VIEW\n",
        "    WHERE DATE(TIMESTAMP(visit_start_ts),'America/Denver') BETWEEN '{EXP_START_DATE}' AND '{EXP_END_DATE}'\n",
        "    {page_filter}\n",
        "    {os_platform_filter}\n",
        "    \"\"\"\n",
        " \n",
        "elif EXPOSURE_FILTER == 'Custom':\n",
        "  filter_query = (\n",
        "    \"\"\"\n",
        "    INPUT FILTER QUERY HERE\n",
        "    \"\"\"\n",
        "  )  \n",
        " \n",
        "if EXPOSURE_FILTER != 'None':\n",
        " \n",
        "  print(filter_query)\n",
        "  filter_sp = bc.read_gcp_table(filter_query)\n",
        " \n",
        "  exposure_filter_sp = exposure_sp.join(filter_sp,on='HOUSEHOLD_ID',how='inner')\n",
        "  exposure_filter_sp.createOrReplaceTempView(\"exposure_filter\")\n",
        "  exposure_table = \"exposure_filter\"\n",
        "  exposure_filter_sp.display(10)\n",
        " \n",
        "else:\n",
        "  exposure_table = 'exposure_sp'\n",
        "  # exposure_table = 'experimentation.EXPOSURE_SP_{}'.format(EXPERIMENT_ID)\n",
        " \n",
        "print('Exposure Table: {}'.format(exposure_table))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Remove HHs that see 2 Variants but Include Null HH_IDs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "exposure_deduped_sp = spark.sql(f\"\"\"\n",
        "WITH\n",
        "HH_DEDUPED as(\n",
        "SELECT HOUSEHOLD_ID\n",
        "from {exposure_table}\n",
        "GROUP BY HOUSEHOLD_ID HAVING count(distinct VARIANT_ID) = 1\n",
        ")\n",
        "\n",
        "SELECT *\n",
        "from {exposure_table}\n",
        "WHERE HOUSEHOLD_ID IS NULL\n",
        "OR HOUSEHOLD_ID IN (SELECT * FROM HH_DEDUPED)\n",
        "\"\"\")\n",
        "\n",
        "exposure_table = 'exposure_deduped_with_nulls'\n",
        "exposure_deduped_sp.createOrReplaceTempView(f\"{exposure_table}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DeDuped SRM Check\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if EXP_PLATFORM == 'Adobe':\n",
        "    srm_check_deduped = exposure_deduped_sp.groupBy('VARIANT_ID').agg(\n",
        "    F.countDistinct('HOUSEHOLD_ID').alias(\"HH_COUNT\"),\n",
        "    F.countDistinct('ADOBE_VISITOR_ID').alias(\"ADOBE_VISITORS\"),\n",
        "    F.countDistinct(F.when(F.col('HOUSEHOLD_ID').isNull(), F.col('ADOBE_VISITOR_ID')).otherwise(None)).alias(\"ADOBE_VISITORS_wNULL_HH\"),\n",
        "    F.countDistinct(F.when(F.col('HOUSEHOLD_ID').isNotNull(), F.col('ADOBE_VISITOR_ID')).otherwise(None)).alias(\"ADOBE_VISITORS_noNULL_HH\")).sort('VARIANT_ID').toPandas()\n",
        "\n",
        "    srm_check_deduped['VISITOR_COUNT'] = srm_check_deduped['ADOBE_VISITORS_wNULL_HH'] + srm_check_deduped['HH_COUNT']\n",
        "\n",
        "    display(srm_check_deduped)\n",
        "\n",
        "    print('Adobe SRM p-value:', round(srm_test(srm_check_deduped['ADOBE_VISITORS'],exp_platform = EXP_PLATFORM)[1],4))\n",
        "    print('Adobe wNull SRM p-value:', round(srm_test(srm_check_deduped['ADOBE_VISITORS_wNULL_HH'],exp_platform = EXP_PLATFORM)[1],4))\n",
        "    print('HH SRM p-value:', round(srm_test(srm_check_deduped['HH_COUNT'],exp_platform = EXP_PLATFORM)[1],4))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exposure Split by HH and Adobe Visitor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#======================================================================================================================================\n",
        "# UNION DISTINCT TO UNION ALL\n",
        "#======================================================================================================================================\n",
        "\n",
        "\n",
        "#### For Adobe tests, break exposure table into visitors with household_ids and those without. ####\n",
        "if EXP_PLATFORM == 'Adobe':\n",
        "  exposure_split = (spark.sql(f\"\"\"\n",
        "  WITH\n",
        "  -- Ensure that we join on only 1 HOUSEHOLD_ID\n",
        "  EXPOSURE_HH AS(\n",
        "  SELECT HOUSEHOLD_ID\n",
        "      , ID\n",
        "      , ADOBE_VISITOR_ID\n",
        "      , VARIANT_ID\n",
        "      , EXPOSURE_DATETIME\n",
        "  FROM {exposure_table}\n",
        "  WHERE HOUSEHOLD_ID IS NOT NULL\n",
        "  QUALIFY ROW_NUMBER() OVER(PARTITION BY HOUSEHOLD_ID ORDER BY EXPOSURE_DATETIME ASC) = 1\n",
        "  ),\n",
        " \n",
        "  -- GRAB ADOBE_VISITOR_IDs with NULL HOUSEHOLD_ID\n",
        "  EXPOSURE_VISITORS AS(\n",
        "  SELECT HOUSEHOLD_ID\n",
        "      , ID\n",
        "      , ADOBE_VISITOR_ID\n",
        "      , VARIANT_ID\n",
        "      , EXPOSURE_DATETIME\n",
        "  FROM {exposure_table}\n",
        "  WHERE HOUSEHOLD_ID IS NULL\n",
        "  QUALIFY ROW_NUMBER() OVER(PARTITION BY ADOBE_VISITOR_ID ORDER BY EXPOSURE_DATETIME ASC) = 1\n",
        "  )\n",
        " \n",
        "  -- UNION to get overall exposure.\n",
        "  SELECT * FROM EXPOSURE_HH\n",
        "  UNION DISTINCT SELECT * FROM EXPOSURE_VISITORS\n",
        "  \"\"\"))\n",
        "else:\n",
        "  \n",
        "  exposure_split = (spark.sql(f\"\"\"\n",
        "  -- Ensure that we join on only 1 HOUSEHOLD_ID\n",
        "  SELECT *\n",
        "  FROM {exposure_table}\n",
        "  QUALIFY ROW_NUMBER() OVER(PARTITION BY HOUSEHOLD_ID ORDER BY EXPOSURE_DATETIME ASC) = 1\n",
        "  \"\"\"))\n",
        " \n",
        " \n",
        "exposure_split.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(f'''experimentation.exposure_sp_{EXPERIMENT_ID}''')\n",
        "exposure_split_sp = spark.table(f\"experimentation.exposure_sp_{EXPERIMENT_ID}\")\n",
        " \n",
        "exposure_table = 'exposure_split_sp'\n",
        "exposure_split_sp.createOrReplaceTempView(f\"{exposure_table}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Final SRM Check\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if EXP_PLATFORM == 'Adobe':\n",
        "    srm_check_final = exposure_split_sp.groupBy('VARIANT_ID').agg(\n",
        "    F.countDistinct('HOUSEHOLD_ID').alias(\"HH_COUNT\"),\n",
        "    F.countDistinct('ADOBE_VISITOR_ID').alias(\"ADOBE_VISITORS\"),\n",
        "    F.countDistinct(F.when(F.col('HOUSEHOLD_ID').isNull(), F.col('ADOBE_VISITOR_ID')).otherwise(None)).alias(\"ADOBE_VISITORS_wNULL_HH\"),\n",
        "    F.countDistinct(F.when(F.col('HOUSEHOLD_ID').isNotNull(), F.col('ADOBE_VISITOR_ID')).otherwise(None)).alias(\"ADOBE_VISITORS_noNULL_HH\")).sort('VARIANT_ID').toPandas()\n",
        "\n",
        "    srm_check_final['VISITOR_COUNT'] = srm_check_final['ADOBE_VISITORS_wNULL_HH'] + srm_check_final['HH_COUNT']\n",
        "\n",
        "    display(srm_check_final)\n",
        "\n",
        "    print('VISITOR SRM p-value:', round(srm_test(srm_check_final['ADOBE_VISITORS'],exp_platform = EXP_PLATFORM)[1],4))\n",
        "    print('Adobe wNull SRM p-value:', round(srm_test(srm_check_final['ADOBE_VISITORS_wNULL_HH'],exp_platform = EXP_PLATFORM)[1],4))\n",
        "    print('HH SRM p-value:', round(srm_test(srm_check_final['HH_COUNT'],exp_platform = EXP_PLATFORM)[1],4))\n",
        "\n",
        "else:\n",
        "    srm_check_final = exposure_split_sp.groupBy('VARIANT_ID').agg(\n",
        "    F.countDistinct('HOUSEHOLD_ID').alias(\"HH_COUNT\")).sort('VARIANT_ID').toPandas()\n",
        "\n",
        "    display(srm_check_final)\n",
        "\n",
        "    print('HH SRM p-value:', round(srm_test(srm_check_final['HH_COUNT'],exp_platform = EXP_PLATFORM)[1],4))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SRM Display\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if EXP_PLATFORM == 'Adobe':\n",
        "  if round(srm_test(srm_check_raw['Adobe_Count'],exp_platform = EXP_PLATFORM)[1],4) < 0.01:\n",
        "    srm_message_raw = f\"\"\"<h3><font color=\"red\"> SAMPLE RATIO MISMATCH FOUND! p-value < 0.01 for Raw Adobe_Visitor_ID's </font></h3>\"\"\"\n",
        "  else:\n",
        "    srm_message_raw = f\"\"\"<h3><font color=\"green\"> No Sample Ratio Mismatch Found for Raw Adobe_Visitor_ID's </font></h3>\"\"\"\n",
        " \n",
        "  if round(srm_test(srm_check_final['ADOBE_VISITORS'],exp_platform = EXP_PLATFORM)[1],4) < 0.01:\n",
        "    srm_message_adobe = f\"\"\"<h3><font color=\"red\"> SAMPLE RATIO MISMATCH FOUND! p-value < 0.01 for Deduped Adobe_Visitor_ID's </font></h3>\"\"\"\n",
        "  else:\n",
        "    srm_message_adobe = f\"\"\"<h3><font color=\"green\"> No Sample Ratio Mismatch Found for Deduped Adobe_Visitor_ID's </font></h3>\"\"\"\n",
        " \n",
        "  if round(srm_test(srm_check_final['HH_COUNT'],exp_platform = EXP_PLATFORM)[1],4) < 0.01:\n",
        "    srm_message_hh = f\"\"\"<h3><font color=\"red\"> SAMPLE RATIO MISMATCH FOUND! p-value < 0.01 for Deduped Household_ID's </font></h3>\"\"\"\n",
        "  else:\n",
        "    srm_message_hh = f\"\"\"<h3><font color=\"green\"> No Sample Ratio Mismatch Found for Deduped Household_ID's </font></h3>\"\"\"\n",
        " \n",
        "else:\n",
        "  srm_message_raw = \"\"\n",
        "  srm_message_adobe = \"\"\n",
        "  if round(srm_test(srm_check_final['HH_COUNT'],exp_platform = EXP_PLATFORM)[1],4) < 0.01:\n",
        "    srm_message_hh = f\"\"\"<h3><font color=\"red\"> SAMPLE RATIO MISMATCH FOUND! p-value < 0.01 for Deduped Household_ID's </font></h3>\"\"\"\n",
        "  else:\n",
        "    srm_message_hh = f\"\"\"<h3><font color=\"green\"> No Sample Ratio Mismatch Found for Deduped Household_ID's </font></h3>\"\"\"\n",
        " \n",
        "displayHTML(f\"\"\"{srm_message_raw}\n",
        "            {srm_message_adobe}\n",
        "            {srm_message_hh}\"\"\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Metric Aggregation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Engagement Aggregation on Adobe_Visitor_ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#======================================================================================================================================\n",
        "# We can make it more efficient by reading cart_coupon_table only once.\n",
        "\n",
        "#1. Combine Reads\n",
        "# ALL_EVENTS AS (\n",
        "# SELECT\n",
        "# {visitor_id},\n",
        "# DTE,\n",
        "# SUM(CASE WHEN EVENT_TYPE = 'CART_ADD' THEN 1 ELSE 0 END) AS CART_ADDS,\n",
        "# SUM(CASE WHEN EVENT_TYPE = 'COUPON_CLIP' THEN 1 ELSE 0 END) AS COUPON_CLIPS,\n",
        "# SUM(CASE WHEN EVENT_TYPE = 'SEARCH' THEN 1 ELSE 0 END) AS NUM_SEARCHES\n",
        "# FROM {cart_coupon_table}\n",
        "# WHERE\n",
        "# EVENT_TYPE IN ('CART_ADD', 'COUPON_CLIP', 'SEARCH') -- Filter for relevant events first\n",
        "# AND (DATE(DTE) >= '{EXP_START_DATE}' AND DATE(DTE) <= '{EXP_END_DATE}')\n",
        "# GROUP BY {visitor_id}, DTE\n",
        "# )\n",
        "\n",
        "#2. New Agg Query\n",
        "# visitor_id = \"\"\"ID\"\"\"\n",
        "# join_str = \"\"\"CAST(e.ID AS STRING) = CAST(v.ID AS STRING)\"\"\"\n",
        "# agg_daily_AV_query = (f\"\"\"\n",
        "# WITH\n",
        "# EXPOSURE_VISITORS AS (\n",
        "# SELECT * FROM {exposure_table} WHERE HOUSEHOLD_ID IS NULL\n",
        "# ),\n",
        "# VISITS AS (\n",
        "# SELECT\n",
        "# VISIT_ID,\n",
        "# {visitor_id},\n",
        "# VISIT_START_TS,\n",
        "# VISIT_END_TS,\n",
        "# SUM(NUM_ORDERS) AS NUM_ORDERS,\n",
        "# SUM(TOT_REVENUE) AS TOT_REVENUE,\n",
        "# SUM(NUM_UNITS) AS NUM_UNITS\n",
        "# FROM {visit_order_table}\n",
        "# WHERE 1=1\n",
        "# AND (\n",
        "# (DATE(visit_start_ts) >= '{EXP_START_DATE}' AND DATE(visit_start_ts) <= '{EXP_END_DATE}')\n",
        "# OR (DATE(visit_end_ts) >= '{EXP_START_DATE}' AND DATE(visit_end_ts) <= '{EXP_END_DATE}')\n",
        "# )\n",
        "# GROUP BY 1,2,3,4\n",
        "# ),\n",
        "# EXPOSURES_VISITS AS (\n",
        "# SELECT\n",
        "# e.*,\n",
        "# COALESCE(COUNT(DISTINCT v.VISIT_ID), 0) AS NUM_VISITS,\n",
        "# COALESCE(SUM(NUM_ORDERS), 0) AS NUM_ORDERS,\n",
        "# COALESCE(SUM(TOT_REVENUE), 0) AS TOT_REVENUE,\n",
        "# COALESCE(SUM(NUM_UNITS), 0) AS NUM_UNITS\n",
        "# FROM EXPOSURE_VISITORS AS e\n",
        "# LEFT JOIN VISITS as v\n",
        "# ON {join_str}\n",
        "# AND v.VISIT_END_TS >= e.EXPOSURE_DATETIME\n",
        "# WHERE e.VARIANT_ID IS NOT NULL\n",
        "# GROUP BY all\n",
        "# ),\n",
        "# -- **** OPTIMIZATION: Scan cart_coupon_table only ONCE ****\n",
        "# ALL_EVENTS AS (\n",
        "# SELECT\n",
        "# {visitor_id},\n",
        "# DTE,\n",
        "# SUM(CASE WHEN EVENT_TYPE = 'CART_ADD' THEN 1 ELSE 0 END) AS CART_ADDS,\n",
        "# SUM(CASE WHEN EVENT_TYPE = 'COUPON_CLIP' THEN 1 ELSE 0 END) AS COUPON_CLIPS,\n",
        "# SUM(CASE WHEN EVENT_TYPE = 'SEARCH' THEN 1 ELSE 0 END) AS NUM_SEARCHES\n",
        "# FROM {cart_coupon_table}\n",
        "# WHERE\n",
        "# EVENT_TYPE IN ('CART_ADD', 'COUPON_CLIP', 'SEARCH') -- Filter for all relevant events at once\n",
        "# AND (DATE(DTE) >= '{EXP_START_DATE}' AND DATE(DTE) <= '{EXP_END_DATE}')\n",
        "# GROUP BY {visitor_id}, DTE\n",
        "# )\n",
        "# -- **** END OPTIMIZATION ****\n",
        "# -- Final SELECT with a single join to the aggregated events table\n",
        "# SELECT\n",
        "# ev.*,\n",
        "# COALESCE(SUM(ae.CART_ADDS), 0) AS NUM_CART_ADDS,\n",
        "# COALESCE(SUM(ae.COUPON_CLIPS), 0) AS COUPON_CLIPS,\n",
        "# COALESCE(SUM(ae.NUM_SEARCHES), 0) AS NUM_SEARCHES\n",
        "# FROM EXPOSURES_VISITS AS ev\n",
        "# LEFT JOIN ALL_EVENTS AS ae\n",
        "# ON ev.{visitor_id} = ae.{visitor_id}\n",
        "# AND TIMESTAMP(ev.EXPOSURE_DATETIME) <= TIMESTAMP(ae.DTE)\n",
        "# GROUP BY all\n",
        "# \"\"\")\n",
        "#======================================================================================================================================\n",
        "\n",
        "\n",
        "\n",
        "visitor_id = \"\"\"ID\"\"\"\n",
        "join_str = \"\"\"CAST(e.ID AS STRING) = CAST(v.ID AS STRING)\"\"\"\n",
        "\n",
        "agg_daily_AV_query = (f\"\"\"\n",
        "WITH\n",
        "EXPOSURE_VISITORS AS(\n",
        "SELECT * FROM {exposure_table} WHERE HOUSEHOLD_ID IS NULL\n",
        "),\n",
        "\n",
        "VISITS AS(\n",
        "SELECT\n",
        "VISIT_ID,\n",
        "{visitor_id},\n",
        "VISIT_START_TS,\n",
        "VISIT_END_TS,\n",
        "SUM(NUM_ORDERS) AS NUM_ORDERS,\n",
        "SUM(TOT_REVENUE) AS TOT_REVENUE,\n",
        "SUM(NUM_UNITS) AS NUM_UNITS\n",
        "FROM\n",
        "{visit_order_table}\n",
        "WHERE 1=1\n",
        "AND (\n",
        "(DATE(visit_start_ts) >= '{EXP_START_DATE}' AND DATE(visit_start_ts) <= '{EXP_END_DATE}')\n",
        "OR\n",
        "(DATE(visit_end_ts) >= '{EXP_START_DATE}' AND DATE(visit_end_ts) <= '{EXP_END_DATE}')\n",
        ")\n",
        "GROUP BY 1,2,3,4\n",
        "),\n",
        "\n",
        "EXPOSURES_VISITS AS(\n",
        "SELECT\n",
        "e.*,\n",
        "COALESCE(COUNT(DISTINCT v.VISIT_ID),0) AS NUM_VISITS,\n",
        "COALESCE(SUM(NUM_ORDERS),0) AS NUM_ORDERS,\n",
        "COALESCE(SUM(TOT_REVENUE),0) AS TOT_REVENUE,\n",
        "COALESCE(SUM(NUM_UNITS),0) AS NUM_UNITS\n",
        "FROM EXPOSURE_VISITORS AS e\n",
        "LEFT JOIN VISITS as v\n",
        "ON {join_str}\n",
        "AND v.VISIT_END_TS >= e.EXPOSURE_DATETIME\n",
        "WHERE e.VARIANT_ID IS NOT NULL\n",
        "GROUP BY all\n",
        "),\n",
        "\n",
        "CART_ADDS AS(\n",
        "SELECT\n",
        "{visitor_id},\n",
        "DTE,\n",
        "COUNT(*) AS CART_ADDS\n",
        "FROM {cart_coupon_table}\n",
        "WHERE EVENT_TYPE = 'CART_ADD'\n",
        "AND (DATE(DTE) >= '{EXP_START_DATE}' AND DATE(DTE) <= '{EXP_END_DATE}')\n",
        "GROUP BY all\n",
        "),\n",
        "\n",
        "COUPON_CLIPS AS(\n",
        "SELECT\n",
        "{visitor_id},\n",
        "DTE,\n",
        "COUNT(*) AS COUPON_CLIPS\n",
        "FROM {cart_coupon_table}\n",
        "WHERE EVENT_TYPE = 'COUPON_CLIP'\n",
        "AND (DATE(DTE) >= '{EXP_START_DATE}' AND DATE(DTE) <= '{EXP_END_DATE}')\n",
        "GROUP BY all\n",
        "),\n",
        "\n",
        "SEARCHES AS(\n",
        "SELECT\n",
        "{visitor_id},\n",
        "DTE,\n",
        "COUNT(*) AS NUM_SEARCHES\n",
        "FROM {cart_coupon_table}\n",
        "WHERE EVENT_TYPE = 'SEARCH'\n",
        "AND (DATE(DTE) >= '{EXP_START_DATE}' AND DATE(DTE) <= '{EXP_END_DATE}')\n",
        "GROUP BY all\n",
        "),\n",
        "\n",
        "EXPOSURES_VISITS_CART_ADDS AS(\n",
        "SELECT\n",
        "ev.*,\n",
        "COALESCE(SUM(ca.CART_ADDS),0) AS NUM_CART_ADDS\n",
        "FROM EXPOSURES_VISITS AS ev\n",
        "LEFT JOIN CART_ADDS AS ca\n",
        "ON ev.{visitor_id} = ca.{visitor_id}\n",
        "AND TIMESTAMP(ev.EXPOSURE_DATETIME) <= TIMESTAMP(ca.DTE)\n",
        "GROUP BY all\n",
        "),\n",
        "\n",
        "EXPOSURES_VISITS_COUPON_CLIPS AS(\n",
        "SELECT\n",
        "evca.*,\n",
        "COALESCE(SUM(cc.COUPON_CLIPS),0) AS COUPON_CLIPS\n",
        "FROM EXPOSURES_VISITS_CART_ADDS evca\n",
        "LEFT JOIN COUPON_CLIPS AS cc\n",
        "ON evca.{visitor_id} = cc.{visitor_id}\n",
        "AND TIMESTAMP(evca.EXPOSURE_DATETIME) <= TIMESTAMP(cc.DTE)\n",
        "GROUP BY all\n",
        ")\n",
        "\n",
        "SELECT\n",
        "evcc.*,\n",
        "COALESCE(SUM(ss.NUM_SEARCHES),0) AS NUM_SEARCHES\n",
        "FROM EXPOSURES_VISITS_COUPON_CLIPS evcc\n",
        "LEFT JOIN SEARCHES AS ss\n",
        "ON evcc.{visitor_id} = ss.{visitor_id}\n",
        "AND TIMESTAMP(evcc.EXPOSURE_DATETIME) <= TIMESTAMP(ss.DTE)\n",
        "GROUP BY all\n",
        "\"\"\")\n",
        "\n",
        "if EXP_PLATFORM == 'Adobe':\n",
        "  agg_daily_AV = spark.sql(agg_daily_AV_query)\n",
        "  agg_daily_AV.createOrReplaceTempView(\"engagement_agg_AV\")\n",
        "\n",
        "\n",
        "\n",
        "        # # Before running any metric queries, ensure you have cached the final exposure table\n",
        "        # exposure_split_sp.cache()\n",
        "        # exposure_split_sp.count() # Action to trigger the cache\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Engagement Aggregation on HOUSEHOLD_ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "#======================================================================================================================================\n",
        "# We can make it more efficient by reading cart_coupon_table only once.\n",
        "\n",
        "#1. Combine Reads\n",
        "# ALL_EVENTS AS (\n",
        "# SELECT\n",
        "# {visitor_id},\n",
        "# DTE,\n",
        "# SUM(CASE WHEN EVENT_TYPE = 'CART_ADD' THEN 1 ELSE 0 END) AS CART_ADDS,\n",
        "# SUM(CASE WHEN EVENT_TYPE = 'COUPON_CLIP' THEN 1 ELSE 0 END) AS COUPON_CLIPS,\n",
        "# SUM(CASE WHEN EVENT_TYPE = 'SEARCH' THEN 1 ELSE 0 END) AS NUM_SEARCHES\n",
        "# FROM {cart_coupon_table}\n",
        "# WHERE\n",
        "# EVENT_TYPE IN ('CART_ADD', 'COUPON_CLIP', 'SEARCH') -- Filter for relevant events first\n",
        "# AND (DATE(DTE) >= '{EXP_START_DATE}' AND DATE(DTE) <= '{EXP_END_DATE}')\n",
        "# GROUP BY {visitor_id}, DTE\n",
        "# )\n",
        "\n",
        "#2. New Agg Query\n",
        "# visitor_id = \"\"\"ID\"\"\"\n",
        "# join_str = \"\"\"CAST(e.ID AS STRING) = CAST(v.ID AS STRING)\"\"\"\n",
        "# agg_daily_AV_query = (f\"\"\"\n",
        "# WITH\n",
        "# EXPOSURE_VISITORS AS (\n",
        "# SELECT * FROM {exposure_table} WHERE HOUSEHOLD_ID IS NULL\n",
        "# ),\n",
        "# VISITS AS (\n",
        "# SELECT\n",
        "# VISIT_ID,\n",
        "# {visitor_id},\n",
        "# VISIT_START_TS,\n",
        "# VISIT_END_TS,\n",
        "# SUM(NUM_ORDERS) AS NUM_ORDERS,\n",
        "# SUM(TOT_REVENUE) AS TOT_REVENUE,\n",
        "# SUM(NUM_UNITS) AS NUM_UNITS\n",
        "# FROM {visit_order_table}\n",
        "# WHERE 1=1\n",
        "# AND (\n",
        "# (DATE(visit_start_ts) >= '{EXP_START_DATE}' AND DATE(visit_start_ts) <= '{EXP_END_DATE}')\n",
        "# OR (DATE(visit_end_ts) >= '{EXP_START_DATE}' AND DATE(visit_end_ts) <= '{EXP_END_DATE}')\n",
        "# )\n",
        "# GROUP BY all,3,4\n",
        "# ),\n",
        "# EXPOSURES_VISITS AS (\n",
        "# SELECT\n",
        "# e.*,\n",
        "# COALESCE(COUNT(DISTINCT v.VISIT_ID), 0) AS NUM_VISITS,\n",
        "# COALESCE(SUM(NUM_ORDERS), 0) AS NUM_ORDERS,\n",
        "# COALESCE(SUM(TOT_REVENUE), 0) AS TOT_REVENUE,\n",
        "# COALESCE(SUM(NUM_UNITS), 0) AS NUM_UNITS\n",
        "# FROM EXPOSURE_VISITORS AS e\n",
        "# LEFT JOIN VISITS as v\n",
        "# ON {join_str}\n",
        "# AND v.VISIT_END_TS >= e.EXPOSURE_DATETIME\n",
        "# WHERE e.VARIANT_ID IS NOT NULL\n",
        "# GROUP BY all\n",
        "# ),\n",
        "# -- **** OPTIMIZATION: Scan cart_coupon_table only ONCE ****\n",
        "# ALL_EVENTS AS (\n",
        "# SELECT\n",
        "# {visitor_id},\n",
        "# DTE,\n",
        "# SUM(CASE WHEN EVENT_TYPE = 'CART_ADD' THEN 1 ELSE 0 END) AS CART_ADDS,\n",
        "# SUM(CASE WHEN EVENT_TYPE = 'COUPON_CLIP' THEN 1 ELSE 0 END) AS COUPON_CLIPS,\n",
        "# SUM(CASE WHEN EVENT_TYPE = 'SEARCH' THEN 1 ELSE 0 END) AS NUM_SEARCHES\n",
        "# FROM {cart_coupon_table}\n",
        "# WHERE\n",
        "# EVENT_TYPE IN ('CART_ADD', 'COUPON_CLIP', 'SEARCH') -- Filter for all relevant events at once\n",
        "# AND (DATE(DTE) >= '{EXP_START_DATE}' AND DATE(DTE) <= '{EXP_END_DATE}')\n",
        "# GROUP BY {visitor_id}, DTE\n",
        "# )\n",
        "# -- **** END OPTIMIZATION ****\n",
        "# -- Final SELECT with a single join to the aggregated events table\n",
        "# SELECT\n",
        "# ev.*,\n",
        "# COALESCE(SUM(ae.CART_ADDS), 0) AS NUM_CART_ADDS,\n",
        "# COALESCE(SUM(ae.COUPON_CLIPS), 0) AS COUPON_CLIPS,\n",
        "# COALESCE(SUM(ae.NUM_SEARCHES), 0) AS NUM_SEARCHES\n",
        "# FROM EXPOSURES_VISITS AS ev\n",
        "# LEFT JOIN ALL_EVENTS AS ae\n",
        "# ON ev.{visitor_id} = ae.{visitor_id}\n",
        "# AND TIMESTAMP(ev.EXPOSURE_DATETIME) <= TIMESTAMP(ae.DTE)\n",
        "# GROUP BY all\n",
        "# \"\"\")\n",
        "#======================================================================================================================================\n",
        "\n",
        "#======================================================================================================================================\n",
        "# Advanced Optimization\n",
        "\n",
        "# Advanced Optimization: Consolidate Logic Further\n",
        "# Since both your anonymous (agg_daily_AV_query) and authenticated (agg_daily_HH_query) queries perform the exact same event aggregation logic, you can optimize even further by creating the aggregated events table once and using it in both queries.\n",
        "# This approach is highly recommended as it follows the \"Don't Repeat Yourself\" (DRY) principle and is very efficient.\n",
        "# Step 1: Create a single, combined events DataFrame before running either query.\n",
        "# # This code would go BEFORE your 'Engagement Aggregation on Adobe_Visitor_ID' cell\n",
        "# from pyspark.sql.functions import sum as _sum, col, when\n",
        "# # Create the all_events DataFrame using the DataFrame API\n",
        "# all_events_sp = (\n",
        "# spark.table(cart_coupon_table)\n",
        "# .filter(\n",
        "# (col(\"EVENT_TYPE\").isin('CART_ADD', 'COUPON_CLIP', 'SEARCH')) &\n",
        "# (col(\"DTE\").between(EXP_START_DATE, EXP_END_DATE))\n",
        "# )\n",
        "# .groupBy(\"ID\", \"HOUSEHOLD_ID\", \"DTE\") # Group by both ID types\n",
        "# .agg(\n",
        "# _sum(when(col(\"EVENT_TYPE\") == 'CART_ADD', 1).otherwise(0)).alias(\"CART_ADDS\"),\n",
        "# _sum(when(col(\"EVENT_TYPE\") == 'COUPON_CLIP', 1).otherwise(0)).alias(\"COUPON_CLIPS\"),\n",
        "# _sum(when(col(\"EVENT_TYPE\") == 'SEARCH', 1).otherwise(0)).alias(\"NUM_SEARCHES\")\n",
        "# )\n",
        "# )\n",
        "# # Create a temporary view that both of your big queries can use\n",
        "# all_events_sp.createOrReplaceTempView(\"all_events_view\")\n",
        "# Step 2: Simplify both of your large SQL queries.\n",
        "# Now, you would modify both agg_daily_AV_query and agg_daily_HH_query to remove their CART_ADDS, COUPON_CLIPS, and SEARCHES CTEs entirely and instead just join to all_events_view.\n",
        "# Example for agg_daily_HH_query:\n",
        "# -- Inside your agg_daily_HH_query string...\n",
        "# -- REMOVE the CART_ADDS, COUPON_CLIPS, SEARCHES, and chained join CTEs.\n",
        "# -- The query becomes much shorter.\n",
        "# -- ... (EXPOSURES_VISITS CTE remains the same) ...\n",
        "# -- Final SELECT joins directly to the pre-built view\n",
        "# SELECT\n",
        "# ev.*,\n",
        "# COALESCE(SUM(ae.CART_ADDS), 0) AS NUM_CART_ADDS,\n",
        "# COALESCE(SUM(ae.COUPON_CLIPS), 0) AS COUPON_CLIPS,\n",
        "# COALESCE(SUM(ae.NUM_SEARCHES), 0) AS NUM_SEARCHES\n",
        "# FROM EXPOSURES_VISITS AS ev\n",
        "# LEFT JOIN all_events_view AS ae -- Use the common view\n",
        "# ON ev.HOUSEHOLD_ID = ae.HOUSEHOLD_ID\n",
        "# AND TIMESTAMP(ev.EXPOSURE_DATETIME) <= TIMESTAMP(ae.DTE)\n",
        "# GROUP BY all\n",
        "# This advanced method is superior because it computes the event aggregations only once for the entire notebook run, making your code faster and much cleaner.\n",
        "\n",
        "\n",
        "#======================================================================================================================================\n",
        "\n",
        "\n",
        "visitor_id = \"\"\"HOUSEHOLD_ID\"\"\"\n",
        "join_str = \"\"\"e.HOUSEHOLD_ID = v.HOUSEHOLD_ID\"\"\"\n",
        "\n",
        "agg_daily_HH_query = (f\"\"\"\n",
        "WITH\n",
        "EXPOSURE_HH AS(\n",
        "SELECT * FROM {exposure_table} WHERE HOUSEHOLD_ID IS NOT NULL\n",
        "),\n",
        "\n",
        "VISITS AS(\n",
        "SELECT\n",
        "VISIT_ID,\n",
        "{visitor_id},\n",
        "VISIT_START_TS,\n",
        "VISIT_END_TS,\n",
        "SUM(NUM_ORDERS) AS NUM_ORDERS,\n",
        "SUM(TOT_REVENUE) AS TOT_REVENUE,\n",
        "SUM(NUM_UNITS) AS NUM_UNITS\n",
        "FROM\n",
        "{visit_order_table}\n",
        "WHERE 1=1\n",
        "AND (\n",
        "(DATE(visit_start_ts) >= '{EXP_START_DATE}' AND DATE(visit_start_ts) <= '{EXP_END_DATE}')\n",
        "OR\n",
        "(DATE(visit_end_ts) >= '{EXP_START_DATE}' AND DATE(visit_end_ts) <= '{EXP_END_DATE}')\n",
        ")\n",
        "GROUP BY all,3,4\n",
        "),\n",
        "\n",
        "EXPOSURES_VISITS AS(\n",
        "SELECT\n",
        "e.*,\n",
        "COALESCE(COUNT(DISTINCT v.VISIT_ID),0) AS NUM_VISITS,\n",
        "COALESCE(SUM(NUM_ORDERS),0) AS NUM_ORDERS,\n",
        "COALESCE(SUM(TOT_REVENUE),0) AS TOT_REVENUE,\n",
        "COALESCE(SUM(NUM_UNITS),0) AS NUM_UNITS\n",
        "FROM EXPOSURE_HH AS e\n",
        "LEFT JOIN VISITS as v\n",
        "ON {join_str}\n",
        "AND v.VISIT_END_TS >= e.EXPOSURE_DATETIME\n",
        "WHERE e.VARIANT_ID IS NOT NULL\n",
        "GROUP BY all\n",
        "),\n",
        "\n",
        "CART_ADDS AS(\n",
        "SELECT\n",
        "{visitor_id},\n",
        "DTE,\n",
        "COUNT(*) AS CART_ADDS\n",
        "FROM {cart_coupon_table}\n",
        "WHERE EVENT_TYPE = 'CART_ADD'\n",
        "AND (DATE(DTE) >= '{EXP_START_DATE}' AND DATE(DTE) <= '{EXP_END_DATE}')\n",
        "GROUP BY all\n",
        "),\n",
        "\n",
        "COUPON_CLIPS AS(\n",
        "SELECT\n",
        "{visitor_id},\n",
        "DTE,\n",
        "COUNT(*) AS COUPON_CLIPS\n",
        "FROM {cart_coupon_table}\n",
        "WHERE EVENT_TYPE = 'COUPON_CLIP'\n",
        "AND (DATE(DTE) >= '{EXP_START_DATE}' AND DATE(DTE) <= '{EXP_END_DATE}')\n",
        "GROUP BY all\n",
        "),\n",
        "\n",
        "SEARCHES AS(\n",
        "SELECT\n",
        "{visitor_id},\n",
        "DTE,\n",
        "COUNT(*) AS NUM_SEARCHES\n",
        "FROM {cart_coupon_table}\n",
        "WHERE EVENT_TYPE = 'SEARCH'\n",
        "AND (DATE(DTE) >= '{EXP_START_DATE}' AND DATE(DTE) <= '{EXP_END_DATE}')\n",
        "GROUP BY all\n",
        "),\n",
        "\n",
        "EXPOSURES_VISITS_CART_ADDS AS(\n",
        "SELECT\n",
        "ev.*,\n",
        "COALESCE(SUM(ca.CART_ADDS),0) AS NUM_CART_ADDS\n",
        "FROM EXPOSURES_VISITS AS ev\n",
        "LEFT JOIN CART_ADDS AS ca\n",
        "ON ev.{visitor_id} = ca.{visitor_id}\n",
        "AND TIMESTAMP(ev.EXPOSURE_DATETIME) <= TIMESTAMP(ca.DTE)\n",
        "GROUP BY all\n",
        "),\n",
        "\n",
        "EXPOSURES_VISITS_COUPON_CLIPS AS(\n",
        "SELECT\n",
        "evca.*,\n",
        "COALESCE(SUM(cc.COUPON_CLIPS),0) AS COUPON_CLIPS\n",
        "FROM EXPOSURES_VISITS_CART_ADDS evca\n",
        "LEFT JOIN COUPON_CLIPS AS cc\n",
        "ON evca.{visitor_id} = cc.{visitor_id}\n",
        "AND TIMESTAMP(evca.EXPOSURE_DATETIME) <= TIMESTAMP(cc.DTE)\n",
        "GROUP BY all\n",
        ")\n",
        "\n",
        "SELECT\n",
        "evcc.*,\n",
        "COALESCE(SUM(ss.NUM_SEARCHES),0) AS NUM_SEARCHES\n",
        "FROM EXPOSURES_VISITS_COUPON_CLIPS evcc\n",
        "LEFT JOIN SEARCHES AS ss\n",
        "ON evcc.{visitor_id} = ss.{visitor_id}\n",
        "AND TIMESTAMP(evcc.EXPOSURE_DATETIME) <= TIMESTAMP(ss.DTE)\n",
        "GROUP BY all\n",
        "\"\"\")\n",
        "\n",
        "agg_daily_HH = spark.sql(agg_daily_HH_query)\n",
        "agg_daily_HH.createOrReplaceTempView(\"engagement_agg_HH\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "visitor_id = \"\"\"HOUSEHOLD_ID\"\"\"\n",
        "join_str = \"\"\"e.HOUSEHOLD_ID = v.HOUSEHOLD_ID\"\"\"\n",
        " \n",
        "agg_daily_HH_query = (f\"\"\"\n",
        "    WITH\n",
        "    EXPOSURE_HH AS(\n",
        "      SELECT * FROM {exposure_table} WHERE HOUSEHOLD_ID IS NOT NULL\n",
        "    ),\n",
        "    \n",
        "    VISITS AS(\n",
        "      SELECT\n",
        "        VISIT_ID,\n",
        "        {visitor_id},\n",
        "        VISIT_START_TS,\n",
        "        VISIT_END_TS,\n",
        "        SUM(NUM_ORDERS) AS NUM_ORDERS,\n",
        "        SUM(TOT_REVENUE) AS TOT_REVENUE,\n",
        "        SUM(NUM_UNITS) AS NUM_UNITS\n",
        "      FROM\n",
        "        {visit_order_table}\n",
        "      WHERE 1=1\n",
        "          AND (\n",
        "                (DATE(visit_start_ts) >= '{EXP_START_DATE}' AND DATE(visit_start_ts) <= '{EXP_END_DATE}')\n",
        "                OR\n",
        "                (DATE(visit_end_ts) >= '{EXP_START_DATE}' AND DATE(visit_end_ts) <= '{EXP_END_DATE}')\n",
        "              )\n",
        "      GROUP BY 1,2,3,4\n",
        "      ),\n",
        " \n",
        "    EXPOSURES_VISITS AS(   \n",
        "      SELECT\n",
        "          e.*,\n",
        "          COALESCE(COUNT(DISTINCT v.VISIT_ID),0) AS NUM_VISITS,\n",
        "          COALESCE(SUM(NUM_ORDERS),0) AS NUM_ORDERS,\n",
        "          COALESCE(SUM(TOT_REVENUE),0) AS TOT_REVENUE,\n",
        "          COALESCE(SUM(NUM_UNITS),0) AS NUM_UNITS\n",
        "      FROM EXPOSURE_HH AS e\n",
        "      LEFT JOIN VISITS as v\n",
        "          ON {join_str}\n",
        "          AND v.VISIT_END_TS >= e.EXPOSURE_DATETIME\n",
        "      WHERE e.VARIANT_ID IS NOT NULL        \n",
        "      GROUP BY all\n",
        "      ),\n",
        " \n",
        "    CART_ADDS AS(\n",
        "      SELECT\n",
        "        {visitor_id},\n",
        "        DTE,\n",
        "        COUNT(*) AS CART_ADDS\n",
        "      FROM {cart_coupon_table}\n",
        "      WHERE EVENT_TYPE = 'CART_ADD'\n",
        "      AND (DATE(DTE) >= '{EXP_START_DATE}' AND DATE(DTE) <= '{EXP_END_DATE}')\n",
        "      GROUP BY 1,2\n",
        "    ),\n",
        " \n",
        "    COUPON_CLIPS AS(\n",
        "      SELECT\n",
        "        {visitor_id},\n",
        "        DTE,\n",
        "        COUNT(*) AS COUPON_CLIPS\n",
        "      FROM {cart_coupon_table}\n",
        "      WHERE EVENT_TYPE = 'COUPON_CLIP'\n",
        "      AND (DATE(DTE) >= '{EXP_START_DATE}' AND DATE(DTE) <= '{EXP_END_DATE}')\n",
        "      GROUP BY 1,2\n",
        "    ),\n",
        " \n",
        "    SEARCHES AS(\n",
        "      SELECT\n",
        "        {visitor_id},\n",
        "        DTE,\n",
        "        COUNT(*) AS NUM_SEARCHES\n",
        "      FROM {cart_coupon_table}\n",
        "      WHERE EVENT_TYPE = 'SEARCH'\n",
        "      AND (DATE(DTE) >= '{EXP_START_DATE}' AND DATE(DTE) <= '{EXP_END_DATE}')\n",
        "      GROUP BY 1,2\n",
        "    ),\n",
        " \n",
        "    EXPOSURES_VISITS_CART_ADDS AS(\n",
        "      SELECT\n",
        "        ev.*,\n",
        "        COALESCE(SUM(ca.CART_ADDS),0) AS NUM_CART_ADDS\n",
        "      FROM EXPOSURES_VISITS AS ev\n",
        "      LEFT JOIN CART_ADDS AS ca\n",
        "        ON ev.{visitor_id} = ca.{visitor_id}\n",
        "        AND TIMESTAMP(ev.EXPOSURE_DATETIME) <= TIMESTAMP(ca.DTE)\n",
        "      GROUP BY all\n",
        "      ),\n",
        " \n",
        "      EXPOSURES_VISITS_COUPON_CLIPS AS(\n",
        "        SELECT\n",
        "          evca.*,\n",
        "          COALESCE(SUM(cc.COUPON_CLIPS),0) AS COUPON_CLIPS\n",
        "        FROM  EXPOSURES_VISITS_CART_ADDS evca\n",
        "        LEFT JOIN COUPON_CLIPS AS cc\n",
        "          ON evca.{visitor_id} = cc.{visitor_id}\n",
        "          AND TIMESTAMP(evca.EXPOSURE_DATETIME) <= TIMESTAMP(cc.DTE)\n",
        "        GROUP BY all\n",
        "        )\n",
        " \n",
        "        SELECT\n",
        "          evcc.*,\n",
        "          COALESCE(SUM(ss.NUM_SEARCHES),0) AS NUM_SEARCHES\n",
        "        FROM  EXPOSURES_VISITS_COUPON_CLIPS evcc\n",
        "        LEFT JOIN SEARCHES AS ss\n",
        "          ON evcc.{visitor_id} = ss.{visitor_id}\n",
        "          AND TIMESTAMP(evcc.EXPOSURE_DATETIME) <= TIMESTAMP(ss.DTE)\n",
        "        GROUP BY all\n",
        "          \"\"\")\n",
        "  \n",
        "agg_daily_HH = spark.sql(agg_daily_HH_query)\n",
        "agg_daily_HH.createOrReplaceTempView(\"engagement_agg_HH\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if EXP_PLATFORM == 'Adobe':\n",
        "  engagement_sub_query = \"\"\"\n",
        "  SELECT * FROM engagement_agg_AV\n",
        "  UNION DISTINCT (SELECT * FROM engagement_agg_HH)\n",
        "  \"\"\"\n",
        "else:\n",
        "  engagement_sub_query = \"\"\"SELECT * FROM engagement_agg_HH\"\"\"\n",
        "\n",
        "engagement_agg_query = (f\"\"\"\n",
        "WITH\n",
        "COMBINED_ENGAGEMENT_AGG AS(\n",
        "{engagement_sub_query}\n",
        ")\n",
        " \n",
        "SELECT\n",
        "          VARIANT_ID,\n",
        "          COUNT(DISTINCT {visitor_unit}) AS VISITORS,\n",
        "          COUNT(DISTINCT HOUSEHOLD_ID) AS UNIQUE_HOUSEHOLDS,\n",
        "        -- VISITS\n",
        "          SUM(NUM_VISITS) AS VISITS_TOTAL,\n",
        "          AVG(NUM_VISITS) AS VISITS_MEAN,\n",
        "          STDDEV(NUM_VISITS) AS VISITS_SD,\n",
        "        -- ORDERS\n",
        "          SUM(NUM_ORDERS) AS ORDERS_TOTAL,\n",
        "          AVG(NUM_ORDERS) AS ORDERS_MEAN,\n",
        "          STDDEV(NUM_ORDERS) AS ORDERS_SD,\n",
        "          AVG(\n",
        "              CASE WHEN NUM_ORDERS = 0 THEN NULL\n",
        "              ELSE NUM_ORDERS\n",
        "              END\n",
        "          ) AS ORDERS_NONZERO_MEAN,\n",
        "          STDDEV(\n",
        "              CASE WHEN NUM_ORDERS = 0 THEN NULL\n",
        "              ELSE NUM_ORDERS\n",
        "              END\n",
        "          ) AS ORDERS_NONZERO_SD,\n",
        "          COUNT(DISTINCT CASE WHEN NUM_ORDERS > 0 THEN {visitor_unit} ELSE NULL END) AS UNIQUE_USERS_THAT_ORDER,\n",
        "          COUNT(DISTINCT CASE WHEN NUM_ORDERS > 0 THEN {visitor_unit} ELSE NULL END) / COUNT(DISTINCT {visitor_unit}) AS CVR,\n",
        "        -- UNITS\n",
        "          SUM(NUM_UNITS) AS UNITS_TOTAL,\n",
        "          AVG(NUM_UNITS) AS UNITS_MEAN,\n",
        "          STDDEV(NUM_UNITS) AS UNITS_SD,\n",
        "          AVG(\n",
        "              CASE WHEN NUM_UNITS = 0 THEN NULL\n",
        "              ELSE NUM_UNITS\n",
        "              END\n",
        "          ) AS UNITS_NONZERO_MEAN,\n",
        "          STDDEV(\n",
        "              CASE WHEN NUM_UNITS = 0 THEN NULL\n",
        "              ELSE NUM_UNITS\n",
        "              END\n",
        "          ) AS UNITS_NONZERO_SD,\n",
        "        -- REVENUE\n",
        "          SUM(TOT_REVENUE) AS REVENUE_TOTAL,\n",
        "          AVG(TOT_REVENUE) AS REVENUE_MEAN,\n",
        "          STDDEV(TOT_REVENUE) AS REVENUE_SD,\n",
        "          AVG(CASE WHEN TOT_REVENUE = 0 THEN NULL ELSE TOT_REVENUE END) AS REVENUE_NONZERO_MEAN,\n",
        "          STDDEV(CASE WHEN TOT_REVENUE = 0 THEN NULL ELSE TOT_REVENUE END) AS REVENUE_NONZERO_SD,\n",
        "        -- COUPON CLIPS\n",
        "          SUM(COUPON_CLIPS) as COUPON_CLIPS_TOTAL,\n",
        "          AVG(COUPON_CLIPS) AS COUPON_CLIPS_MEAN,\n",
        "          STDDEV(COUPON_CLIPS) AS COUPON_CLIPS_SD,\n",
        "          AVG(\n",
        "              CASE WHEN COUPON_CLIPS = 0 THEN NULL\n",
        "              ELSE COUPON_CLIPS\n",
        "              END\n",
        "          ) AS COUPON_CLIPS_NONZERO_MEAN,\n",
        "          STDDEV(\n",
        "              CASE WHEN COUPON_CLIPS = 0 THEN NULL\n",
        "              ELSE COUPON_CLIPS\n",
        "              END\n",
        "            ) AS COUPON_CLIPS_NONZERO_SD,\n",
        "          COUNT(DISTINCT CASE WHEN COUPON_CLIPS > 0 THEN {visitor_unit} ELSE NULL END) AS UNIQUE_USERS_THAT_COUPON_CLIP,\n",
        "          COUNT(DISTINCT CASE WHEN COUPON_CLIPS > 0 THEN {visitor_unit} ELSE NULL END) / COUNT(DISTINCT {visitor_unit}) AS COUPON_CLIP_CVR,\n",
        "        -- CART ADDS\n",
        "          SUM(NUM_CART_ADDS) AS CART_ADDS_TOTAL,\n",
        "          AVG(NUM_CART_ADDS) AS CART_ADDS_MEAN,\n",
        "          STDDEV(NUM_CART_ADDS) AS CART_ADDS_SD,\n",
        "          AVG(\n",
        "              CASE WHEN NUM_CART_ADDS = 0 THEN NULL\n",
        "              ELSE NUM_CART_ADDS\n",
        "              END\n",
        "          ) AS CART_ADDS_CONDITIONAL_MEAN,\n",
        "          STDDEV(\n",
        "              CASE WHEN NUM_CART_ADDS = 0 THEN NULL\n",
        "              ELSE NUM_CART_ADDS\n",
        "              END\n",
        "          ) AS CART_ADDS_CONDITIONAL_SD,\n",
        "          COUNT(DISTINCT CASE WHEN NUM_CART_ADDS > 0 THEN {visitor_unit} ELSE NULL END) AS UNIQUE_USERS_THAT_ADDTOCART,\n",
        "          COUNT(DISTINCT CASE WHEN NUM_CART_ADDS > 0 THEN {visitor_unit} ELSE NULL END) / COUNT(DISTINCT {visitor_unit}) AS CART_ADDS_CVR,\n",
        "        -- SEARCHES    \n",
        "          SUM(NUM_SEARCHES) AS SEARCHES_TOTAL,\n",
        "          AVG(NUM_SEARCHES) AS SEARCHES_MEAN,\n",
        "          STDDEV(NUM_SEARCHES) AS SEARCHES_SD,\n",
        "          AVG(\n",
        "              CASE WHEN NUM_SEARCHES = 0 THEN NULL\n",
        "              ELSE NUM_SEARCHES\n",
        "              END\n",
        "          ) AS SEARCHES_CONDITIONAL_MEAN,\n",
        "          STDDEV(\n",
        "              CASE WHEN NUM_SEARCHES = 0 THEN NULL\n",
        "              ELSE NUM_SEARCHES\n",
        "              END\n",
        "          ) AS SEARCHES_CONDITIONAL_SD,\n",
        "          COUNT(DISTINCT CASE WHEN NUM_SEARCHES > 0 THEN {visitor_unit} ELSE NULL END) AS UNIQUE_USERS_THAT_SEARCH,\n",
        "          COUNT(DISTINCT CASE WHEN NUM_SEARCHES > 0 THEN {visitor_unit} ELSE NULL END) / COUNT(DISTINCT {visitor_unit}) AS SEARCHES_CVR,\n",
        "        -- Ratio Metrics\n",
        "          AVG(IFF(TOT_REVENUE > 0, TOT_REVENUE, NULL)) / AVG(IFF(NUM_ORDERS > 0, NUM_ORDERS, NULL)) AS AOV,\n",
        "          COVAR_SAMP(TOT_REVENUE,NUM_ORDERS) AS COV_REVENUE_ORDERS,\n",
        "          AVG(IFF(NUM_UNITS > 0, NUM_UNITS, NULL)) / AVG(IFF(NUM_ORDERS > 0 , NUM_ORDERS, NULL)) AS UPO,\n",
        "          COVAR_SAMP(NUM_UNITS,NUM_ORDERS) AS COV_UNITS_ORDERS,\n",
        "          SUM(TOT_REVENUE) / COUNT(DISTINCT {visitor_unit}) AS RPV\n",
        "        FROM COMBINED_ENGAGEMENT_AGG\n",
        "        GROUP BY all\n",
        "        ORDER BY 1                                            \n",
        "\"\"\")\n",
        " \n",
        "agg_daily_sp = spark.sql(engagement_agg_query)\n",
        "# agg_daily_sp.display()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Margin Aggregation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#======================================================================================================================================\n",
        "# Prefilter and Join Aggregates\n",
        "#======================================================================================================================================\n",
        "\n",
        "# Refactored Query:\n",
        "# margin_agg_query = (\n",
        "# f\"\"\"\n",
        "# WITH\n",
        "# EXPOSURE_BASE AS (\n",
        "# -- The GROUP BY is removed here for clarity as input is already unique per HH\n",
        "# SELECT *,\n",
        "# DATEADD(DAY,-1,DATE_TRUNC('WEEK',DATEADD(DAY,1,EXPOSURE_DATETIME))) AS EXP_WEEK_START\n",
        "# FROM {exposure_table}\n",
        "# WHERE HOUSEHOLD_ID IS NOT NULL\n",
        "# ),\n",
        "# -- OPTIMIZATION: Pre-filter the large margin table before the complex join\n",
        "# MARGIN_FILTERED AS (\n",
        "# SELECT HOUSEHOLD_ID, FISCAL_WEEK_START_DT, AGP_VAL_WITHOUT_INSTA_ADJ\n",
        "# FROM {margin_table}\n",
        "# -- Filter to only include households present in the experiment\n",
        "# WHERE HOUSEHOLD_ID IN (SELECT HOUSEHOLD_ID FROM EXPOSURE_BASE)\n",
        "# -- Filter to the overall time window of the experiment\n",
        "# AND FISCAL_WEEK_START_DT <= '{EXP_END_DATE}'\n",
        "# ),\n",
        "# BASE AS (\n",
        "# SELECT\n",
        "# e.*,\n",
        "# -- The SUM is now over the pre-filtered margin table\n",
        "# COALESCE(SUM(m.AGP_VAL_WITHOUT_INSTA_ADJ), 0) as margin\n",
        "# FROM EXPOSURE_BASE as e\n",
        "# LEFT JOIN MARGIN_FILTERED as m\n",
        "# ON e.HOUSEHOLD_ID = m.HOUSEHOLD_ID\n",
        "# -- The expensive condition is now applied to a smaller, pre-filtered table\n",
        "# AND m.FISCAL_WEEK_START_DT BETWEEN e.EXP_WEEK_START AND '{EXP_END_DATE}'\n",
        "# GROUP BY all\n",
        "# )\n",
        "# -- The final aggregation remains the same\n",
        "# SELECT\n",
        "# VARIANT_ID,\n",
        "# COUNT(DISTINCT {visitor_unit}) AS VISITORS,\n",
        "# --- MARGIN\n",
        "# SUM(margin) AS MARGIN_TOTAL,\n",
        "# AVG(margin) AS MARGIN_MEAN,\n",
        "# STDDEV(margin) AS MARGIN_SD,\n",
        "# AVG(CASE WHEN margin = 0 THEN NULL ELSE margin END) AS MARGIN_NONZERO_MEAN,\n",
        "# STDDEV(CASE WHEN margin = 0 THEN NULL ELSE margin END) AS MARGIN_NONZERO_SD\n",
        "# FROM BASE\n",
        "# GROUP BY all\n",
        "# ORDER BY 1\n",
        "# \"\"\")\n",
        "\n",
        "\n",
        "\n",
        "margin_agg_query = (\n",
        "f\"\"\"\n",
        "WITH\n",
        "EXPOSURE_BASE AS(\n",
        "SELECT * FROM {exposure_table}\n",
        "),\n",
        "\n",
        "EXPOSURE_WEEK AS(\n",
        "SELECT e.*,\n",
        "DATEADD(DAY,-1,DATE_TRUNC('WEEK',DATEADD(DAY,1,EXPOSURE_DATETIME))) AS EXP_WEEK_START\n",
        "FROM EXPOSURE_BASE AS e\n",
        "GROUP BY all\n",
        "),\n",
        "\n",
        "BASE AS(\n",
        "SELECT e.*,\n",
        "COALESCE(sum(m.AGP_VAL_WITHOUT_INSTA_ADJ),0) as margin\n",
        "FROM EXPOSURE_WEEK as e\n",
        "LEFT JOIN {margin_table} as m\n",
        "ON e.HOUSEHOLD_ID = m.HOUSEHOLD_ID\n",
        "AND m.FISCAL_WEEK_START_DT BETWEEN e.EXP_WEEK_START AND '{EXP_END_DATE}'\n",
        "WHERE e.HOUSEHOLD_ID IS NOT NULL\n",
        "GROUP BY all\n",
        ")\n",
        "\n",
        "SELECT\n",
        "VARIANT_ID,\n",
        "COUNT(DISTINCT {visitor_unit}) AS VISITORS,\n",
        "--- MARGIN\n",
        "SUM(margin) AS MARGIN_TOTAL,\n",
        "AVG(margin) AS MARGIN_MEAN,\n",
        "STDDEV(margin) AS MARGIN_SD,\n",
        "AVG(CASE WHEN margin = 0 THEN NULL ELSE margin END) AS MARGIN_NONZERO_MEAN,\n",
        "STDDEV(CASE WHEN margin = 0 THEN NULL ELSE margin END) AS MARGIN_NONZERO_SD\n",
        "FROM BASE\n",
        "GROUP BY all\n",
        "ORDER BY 1\n",
        "\"\"\")\n",
        "\n",
        "margin_agg_sp = spark.sql(margin_agg_query)\n",
        "#margin_agg_sp.display()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AGP Aggregation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#======================================================================================================================================\n",
        "# Prefilter and Join Aggregates\n",
        "#======================================================================================================================================\n",
        "\n",
        "# agp_agg_query = (f\"\"\"\n",
        "# WITH\n",
        "# EXPOSURE_BASE AS (\n",
        "# SELECT * FROM {exposure_table}\n",
        "# ),\n",
        "# -- OPTIMIZATION: Pre-filter the large AGP table before the complex join\n",
        "# AGP_FILTERED AS (\n",
        "# SELECT HOUSEHOLD_ID, TXN_DTE, NET_SALES, AGP_TOT\n",
        "# FROM {agp_table}\n",
        "# -- Filter to only include households present in the experiment\n",
        "# WHERE HOUSEHOLD_ID IN (SELECT HOUSEHOLD_ID FROM EXPOSURE_BASE WHERE HOUSEHOLD_ID IS NOT NULL)\n",
        "# -- Filter to the overall time window of the experiment\n",
        "# AND TXN_DTE <= '{EXP_END_DATE}'\n",
        "# ),\n",
        "# BASE AS (\n",
        "# SELECT\n",
        "# e.*,\n",
        "# COALESCE(SUM(m.NET_SALES), 0) as NET_SALES,\n",
        "# COALESCE(SUM(m.AGP_TOT), 0) as AGP\n",
        "# FROM EXPOSURE_BASE as e\n",
        "# LEFT JOIN AGP_FILTERED as m\n",
        "# ON e.HOUSEHOLD_ID = m.HOUSEHOLD_ID\n",
        "# -- The expensive condition is now applied to a smaller, pre-filtered table\n",
        "# AND m.TXN_DTE BETWEEN DATE(e.EXPOSURE_DATETIME) AND '{EXP_END_DATE}'\n",
        "# GROUP BY all\n",
        "# )\n",
        "# -- The final aggregation remains the same\n",
        "# SELECT\n",
        "# VARIANT_ID,\n",
        "# COUNT(DISTINCT {visitor_unit}) AS VISITORS,\n",
        "# --- NET_SALES\n",
        "# SUM(NET_SALES) AS NET_SALES_TOTAL,\n",
        "# AVG(NET_SALES) AS NET_SALES_MEAN,\n",
        "# STDDEV(NET_SALES) AS NET_SALES_SD,\n",
        "# --- AGP\n",
        "# SUM(AGP) AS AGP_TOTAL,\n",
        "# AVG(AGP) AS AGP_MEAN,\n",
        "# STDDEV(AGP) AS AGP_SD\n",
        "# FROM BASE\n",
        "# GROUP BY all\n",
        "# ORDER BY 1\n",
        "# \"\"\")\n",
        "\n",
        "\n",
        "agp_agg_query = (f\"\"\"\n",
        "WITH\n",
        "EXPOSURE_BASE AS(\n",
        "SELECT * FROM {exposure_table}\n",
        "),\n",
        "\n",
        "BASE AS(\n",
        "SELECT\n",
        "e.*,\n",
        "COALESCE(sum(m.NET_SALES),0) as NET_SALES,\n",
        "COALESCE(sum(m.AGP_TOT),0) as AGP\n",
        "FROM EXPOSURE_BASE as e\n",
        "LEFT JOIN {agp_table} as m\n",
        "ON e.HOUSEHOLD_ID = m.HOUSEHOLD_ID\n",
        "AND m.TXN_DTE BETWEEN DATE(e.EXPOSURE_DATETIME) AND '{EXP_END_DATE}'\n",
        "GROUP BY all\n",
        ")\n",
        "\n",
        "SELECT\n",
        "VARIANT_ID,\n",
        "COUNT(DISTINCT {visitor_unit}) AS VISITORS,\n",
        "--- NET_SALES\n",
        "SUM(NET_SALES) AS NET_SALES_TOTAL,\n",
        "AVG(NET_SALES) AS NET_SALES_MEAN,\n",
        "STDDEV(NET_SALES) AS NET_SALES_SD,\n",
        "--- AGP\n",
        "SUM(AGP) AS AGP_TOTAL,\n",
        "AVG(AGP) AS AGP_MEAN,\n",
        "STDDEV(AGP) AS AGP_SD\n",
        "FROM BASE\n",
        "GROUP BY all\n",
        "ORDER BY 1\n",
        "\"\"\")\n",
        "\n",
        "agp_agg_sp = spark.sql(agp_agg_query)\n",
        "# agp_agg_sp.display()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Ecomm Aggregation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#======================================================================================================================================\n",
        "# Prefilter and Join Aggregates\n",
        "#======================================================================================================================================\n",
        "# ecomm_txns_agg = (\n",
        "# f\"\"\"\n",
        "# WITH\n",
        "# EXPOSURE_BASE AS (\n",
        "# SELECT * FROM {exposure_table}\n",
        "# ),\n",
        "# -- STEP 1: Create a single, rich, pre-joined transaction detail table for E-commerce\n",
        "# ECOMM_TXN_DETAILS AS (\n",
        "# SELECT\n",
        "# t.HOUSEHOLD_ID,\n",
        "# t.TXN_ID,\n",
        "# t.TXN_DTE,\n",
        "# t.REVENUE,\n",
        "# t.ITEMS,\n",
        "# t.SNAP_TENDER,\n",
        "# r.CLIENT_OFFER_ID,\n",
        "# r.OFFER_TYPE_MOD,\n",
        "# r.MKDN,\n",
        "# b.BNC_SEGMENT -- Join BNC data here at the transaction level\n",
        "# FROM {combined_txn_table} as t\n",
        "# LEFT JOIN {redemptions_table} as r\n",
        "# ON t.TXN_ID = r.TXN_ID\n",
        "# AND t.TXN_DTE = r.TXN_DTE -- Assuming join on date helps if tables are partitioned\n",
        "# LEFT JOIN {bnc_table} as b\n",
        "# ON t.HOUSEHOLD_ID = b.HOUSEHOLD_ID\n",
        "# AND t.TXN_DTE = b.TXN_DTE -- And transaction ID if possible\n",
        "# WHERE t.TXN_LOCATION = 'ECOMM'\n",
        "# AND (t.TXN_DTE >= '{EXP_START_DATE}' AND t.TXN_DTE <= '{EXP_END_DATE}')\n",
        "# ),\n",
        "# -- STEP 2: Join exposures to details ONCE and aggregate ALL metrics at the household level\n",
        "# HOUSEHOLD_LEVEL_AGG AS (\n",
        "# SELECT\n",
        "# e.*,\n",
        "# -- Transaction Metrics\n",
        "# COUNT(DISTINCT d.TXN_ID) AS NUM_ORDERS,\n",
        "# SUM(d.REVENUE) AS TOT_REVENUE,\n",
        "# SUM(d.ITEMS) AS NUM_UNITS,\n",
        "# SUM(d.SNAP_TENDER) AS TOT_SNAP,\n",
        "# COUNT(DISTINCT CASE WHEN d.BNC_SEGMENT IS NOT NULL THEN d.TXN_ID ELSE NULL END) AS ECOMM_BNC_COUNT,\n",
        "# -- Redemption Metrics (all calculated in the same aggregation)\n",
        "# COUNT(DISTINCT CASE WHEN d.OFFER_TYPE_MOD = 'PD' THEN d.CLIENT_OFFER_ID || d.txn_id ELSE NULL END) as pd_redemptions,\n",
        "# COUNT(DISTINCT CASE WHEN d.OFFER_TYPE_MOD = 'GR' THEN d.CLIENT_OFFER_ID || d.txn_id ELSE NULL END) as gr_redemptions,\n",
        "# COUNT(DISTINCT CASE WHEN d.OFFER_TYPE_MOD = 'MF' THEN d.CLIENT_OFFER_ID || d.txn_id ELSE NULL END) as mf_redemptions,\n",
        "# COUNT(DISTINCT CASE WHEN d.OFFER_TYPE_MOD = 'SPD' THEN d.CLIENT_OFFER_ID || d.txn_id ELSE NULL END) as spd_redemptions,\n",
        "# COUNT(DISTINCT CASE WHEN d.OFFER_TYPE_MOD = 'PZN' THEN d.CLIENT_OFFER_ID || d.txn_id ELSE NULL END) as pzn_redemptions,\n",
        "# COUNT(DISTINCT CASE WHEN d.OFFER_TYPE_MOD = 'SC' THEN d.CLIENT_OFFER_ID || d.txn_id ELSE NULL END) as sc_redemptions,\n",
        "# COUNT(DISTINCT d.CLIENT_OFFER_ID || d.txn_id) as total_redemptions,\n",
        "# -- Markdown Metrics (all calculated in the same aggregation)\n",
        "# SUM(CASE WHEN d.OFFER_TYPE_MOD = 'PD' THEN d.MKDN ELSE 0 END) as pd_MKDN,\n",
        "# SUM(CASE WHEN d.OFFER_TYPE_MOD = 'GR' THEN d.MKDN ELSE 0 END) as gr_MKDN,\n",
        "# SUM(CASE WHEN d.OFFER_TYPE_MOD = 'MF' THEN d.MKDN ELSE 0 END) as mf_MKDN,\n",
        "# SUM(CASE WHEN d.OFFER_TYPE_MOD = 'SPD' THEN d.MKDN ELSE 0 END) as spd_MKDN,\n",
        "# SUM(CASE WHEN d.OFFER_TYPE_MOD = 'PZN' THEN d.MKDN ELSE 0 END) as pzn_MKDN,\n",
        "# SUM(CASE WHEN d.OFFER_TYPE_MOD = 'SC' THEN d.MKDN ELSE 0 END) as sc_MKDN,\n",
        "# SUM(d.MKDN) as total_mkdn\n",
        "# FROM EXPOSURE_BASE AS e\n",
        "# LEFT JOIN ECOMM_TXN_DETAILS AS d\n",
        "# ON e.HOUSEHOLD_ID = d.HOUSEHOLD_ID\n",
        "# AND DATE(e.EXPOSURE_DATETIME) <= d.TXN_DTE\n",
        "# GROUP BY all\n",
        "# ),\n",
        "# -- STEP 3: Calculate Winsorization value from the single household-level aggregation\n",
        "# WINZ AS (\n",
        "# SELECT APPROX_PERCENTILE(TOT_REVENUE, 0.99) AS TOT_REVENUE_WIN99\n",
        "# FROM HOUSEHOLD_LEVEL_AGG\n",
        "# WHERE HOUSEHOLD_ID IS NOT NULL\n",
        "# )\n",
        "# -- STEP 4: Perform the final aggregation to the variant level\n",
        "# SELECT\n",
        "# h.* EXCEPT ( -- Exclude per-household metrics before final aggregation\n",
        "# HOUSEHOLD_ID, ID, ADOBE_VISITOR_ID, EXPOSURE_DATETIME,\n",
        "# NUM_ORDERS, TOT_REVENUE, NUM_UNITS, TOT_SNAP, ECOMM_BNC_COUNT,\n",
        "# pd_redemptions, gr_redemptions, mf_redemptions, spd_redemptions, pzn_redemptions, sc_redemptions, total_redemptions,\n",
        "# pd_MKDN, gr_MKDN, mf_MKDN, spd_MKDN, pzn_MKDN, sc_MKDN, total_mkdn\n",
        "# ),\n",
        "# -- Transaction Aggregates\n",
        "# COUNT(DISTINCT h.HOUSEHOLD_ID) AS PURCHASING_CUSTOMERS_COUNT, -- Renamed to avoid confusion\n",
        "# SUM(h.NUM_ORDERS) AS ECOMM_ORDERS_TOTAL,\n",
        "# AVG(h.NUM_ORDERS) AS ECOMM_ORDERS_MEAN,\n",
        "# STDDEV(h.NUM_ORDERS) AS ECOMM_ORDERS_SD,\n",
        "# SUM(h.NUM_UNITS) AS ECOMM_UNITS_TOTAL,\n",
        "# -- ... continue for all other transaction and redemption/markdown metrics ...\n",
        "# -- Use the same SUM(), AVG(), STDDEV() logic as your original TXN_AGG and REDEMPTIONS_AGG CTEs\n",
        "# -- Revenue Aggregates (with Winsorization)\n",
        "# SUM(CASE WHEN h.TOT_REVENUE > (SELECT TOT_REVENUE_WIN99 FROM WINZ) THEN (SELECT TOT_REVENUE_WIN99 FROM WINZ) ELSE h.TOT_REVENUE END) AS ECOMM_REVENUE_TOTAL,\n",
        "# -- ... and so on for all your final metrics ...\n",
        "# AVG(IFF(h.NUM_UNITS > 0, h.NUM_UNITS, NULL)) / AVG(IFF(h.NUM_ORDERS > 0 , h.NUM_ORDERS, NULL)) AS ECOMM_UPO,\n",
        "# COVAR_SAMP(CASE WHEN h.TOT_REVENUE > (SELECT TOT_REVENUE_WIN99 FROM WINZ) THEN (SELECT TOT_REVENUE_WIN99 FROM WINZ) ELSE h.TOT_REVENUE END, h.NUM_ORDERS) AS ECOMM_COV_UNITS_ORDERS\n",
        "# FROM HOUSEHOLD_LEVEL_AGG h\n",
        "# GROUP BY all\n",
        "# \"\"\"\n",
        "# )\n",
        "# # Note: The final part of the refactored query is conceptual. You would fill in all the SUM/AVG/STDDEV\n",
        "# # calculations from your original TXN_AGG and REDEMPTIONS_AGG, but apply them to the columns\n",
        "# # from the new, unified HOUSEHOLD_LEVEL_AGG CTE.\n",
        "############ Justin. --- We can have multiple redemptions per transactions that why we do it seperately ##############\n",
        "\n",
        "ecomm_txns_agg = (\n",
        "f\"\"\"\n",
        "WITH\n",
        "EXPOSURE_BASE AS(\n",
        "SELECT * FROM {exposure_table}\n",
        "),\n",
        "\n",
        "-- Join TXNs to exposures\n",
        "TXNS AS(\n",
        "SELECT e.*,\n",
        "COALESCE(COUNT(DISTINCT t.TXN_ID),0) AS NUM_ORDERS,\n",
        "COALESCE(SUM(t.REVENUE),0) AS TOT_REVENUE,\n",
        "COALESCE(SUM(t.ITEMS),0) AS NUM_UNITS,\n",
        "COALESCE(SUM(t.SNAP_TENDER),0) AS TOT_SNAP,\n",
        "COALESCE(SUM(CASE WHEN BNC_SEGMENT IS NOT NULL THEN 1 ELSE 0 END)) AS ECOMM_BNC_COUNT\n",
        "FROM EXPOSURE_BASE as e\n",
        "LEFT JOIN (SELECT * FROM {combined_txn_table} WHERE TXN_LOCATION = 'ECOMM' AND (TXN_DTE >= '{EXP_START_DATE}' AND TXN_DTE <= '{EXP_END_DATE}') ) as t\n",
        "ON e.HOUSEHOLD_ID = t.HOUSEHOLD_ID\n",
        "AND DATE(e.EXPOSURE_DATETIME) <= t.TXN_DTE\n",
        "LEFT JOIN (SELECT * FROM {bnc_table} WHERE TXN_DTE BETWEEN '{EXP_START_DATE}' AND '{EXP_END_DATE}') as b\n",
        "ON e.HOUSEHOLD_ID = b.HOUSEHOLD_ID\n",
        "GROUP BY all\n",
        "),\n",
        "\n",
        "WINZ AS(\n",
        "SELECT\n",
        "APPROX_PERCENTILE(TOT_REVENUE,0.99) AS TOT_REVENUE_WIN99\n",
        "FROM TXNS\n",
        "WHERE HOUSEHOLD_ID IS NOT NULL\n",
        "),\n",
        "\n",
        "-- Join TXN info to Redemptions (primarily for TXN_DTE)\n",
        "TXNS_REDEMPTIONS AS(\n",
        "SELECT t.*\n",
        ", CLIENT_OFFER_ID\n",
        ", OFFER_TYPE_MOD\n",
        ", MKDN\n",
        "FROM {combined_txn_table} as t\n",
        "LEFT JOIN {redemptions_table} as r\n",
        "ON t.TXN_ID = r.TXN_ID\n",
        "AND (t.TXN_DTE >= '{EXP_START_DATE}' AND t.TXN_DTE <= '{EXP_END_DATE}')\n",
        "WHERE TXN_LOCATION = 'ECOMM'\n",
        "),\n",
        "\n",
        "-- Filter Redemptions by Exposure Datetime\n",
        "REDEMPTIONS_FILTERED AS(\n",
        "SELECT e.*,\n",
        "-- REDEMPTIONS\n",
        "COALESCE(COUNT(DISTINCT CASE WHEN offer_type_mod = 'PD' THEN (e.HOUSEHOLD_ID || CLIENT_OFFER_ID || t.txn_id) ELSE NULL END),0) as pd_redemptions\n",
        ", COALESCE(COUNT(DISTINCT CASE WHEN offer_type_mod = 'GR' THEN (e.HOUSEHOLD_ID || CLIENT_OFFER_ID || t.txn_id) ELSE NULL END),0) as gr_redemptions\n",
        ", COALESCE(COUNT(DISTINCT CASE WHEN offer_type_mod = 'MF' THEN (e.HOUSEHOLD_ID || CLIENT_OFFER_ID || t.txn_id) ELSE NULL END),0) as mf_redemptions\n",
        ", COALESCE(COUNT(DISTINCT CASE WHEN offer_type_mod = 'SPD' THEN (e.HOUSEHOLD_ID || CLIENT_OFFER_ID || t.txn_id) ELSE NULL END),0) as spd_redemptions\n",
        ", COALESCE(COUNT(DISTINCT CASE WHEN offer_type_mod = 'PZN' THEN (e.HOUSEHOLD_ID || CLIENT_OFFER_ID || t.txn_id) ELSE NULL END),0) as pzn_redemptions\n",
        ", COALESCE(COUNT(DISTINCT CASE WHEN offer_type_mod = 'SC' THEN (e.HOUSEHOLD_ID || CLIENT_OFFER_ID || t.txn_id) ELSE NULL END),0) as sc_redemptions\n",
        ", COALESCE(COUNT(DISTINCT (e.HOUSEHOLD_ID || CLIENT_OFFER_ID || t.txn_id)),0) as total_redemptions\n",
        "-- MKDN\n",
        ", COALESCE(SUM(CASE WHEN offer_type_mod = 'PD' THEN MKDN ELSE 0 END),0) as pd_MKDN\n",
        ", COALESCE(SUM(CASE WHEN offer_type_mod = 'GR' THEN MKDN ELSE 0 END),0) as gr_MKDN\n",
        ", COALESCE(SUM(CASE WHEN offer_type_mod = 'MF' THEN MKDN ELSE 0 END),0) as mf_MKDN\n",
        ", COALESCE(SUM(CASE WHEN offer_type_mod = 'SPD' THEN MKDN ELSE 0 END),0) as spd_MKDN\n",
        ", COALESCE(SUM(CASE WHEN offer_type_mod = 'PZN' THEN MKDN ELSE 0 END),0) as pzn_MKDN\n",
        ", COALESCE(SUM(CASE WHEN offer_type_mod = 'SC' THEN MKDN ELSE 0 END),0) as sc_MKDN\n",
        ", COALESCE(SUM(MKDN),0) as total_mkdn\n",
        "FROM EXPOSURE_BASE as e\n",
        "LEFT JOIN TXNS_REDEMPTIONS as t\n",
        "ON e.HOUSEHOLD_ID = t.HOUSEHOLD_ID\n",
        "AND DATE(e.EXPOSURE_DATETIME) <= t.TXN_DTE\n",
        "GROUP BY all\n",
        "),\n",
        "\n",
        "TXN_AGG AS(\n",
        "SELECT VARIANT_ID,\n",
        "COUNT(DISTINCT {visitor_unit}) AS VISITORS\n",
        ", COUNT(DISTINCT IFF(NUM_ORDERS > 0, HOUSEHOLD_ID, NULL)) AS PURCHASING_CUSTOMERS\n",
        ", COUNT(DISTINCT IFF(TOT_REVENUE > (SELECT TOT_REVENUE_WIN99 FROM WINZ), HOUSEHOLD_ID, NULL)) AS WINSORIZED_CUSTOMERS\n",
        ", (SELECT TOT_REVENUE_WIN99 FROM WINZ) AS WINSORIZATION_THRESHOLD\n",
        "-- ORDERS\n",
        ", SUM(NUM_ORDERS) AS ECOMM_ORDERS_TOTAL\n",
        ", AVG(NUM_ORDERS) AS ECOMM_ORDERS_MEAN\n",
        ", STDDEV(NUM_ORDERS) AS ECOMM_ORDERS_SD\n",
        "-- UNITS\n",
        ", SUM(NUM_UNITS) AS ECOMM_UNITS_TOTAL\n",
        ", AVG(NUM_UNITS) AS ECOMM_UNITS_MEAN\n",
        ", STDDEV(NUM_UNITS) AS ECOMM_UNITS_SD\n",
        "--- REVENUE\n",
        ", SUM({metric_rpc_sql}) AS ECOMM_REVENUE_TOTAL\n",
        ", AVG({metric_rpc_sql}) AS ECOMM_REVENUE_MEAN\n",
        ", SUM({metric_rpc_sql}) / COUNT(DISTINCT HOUSEHOLD_ID) AS ECOMM_RPV\n",
        ", SUM(\n",
        "CASE WHEN TOT_REVENUE = 0\n",
        "THEN NULL\n",
        "ELSE {metric_rpc_sql} END) / COUNT(DISTINCT HOUSEHOLD_ID)\n",
        "AS ECOMM_NONZERO_RPV\n",
        ", STDDEV({metric_rpc_sql}) AS ECOMM_REVENUE_SD\n",
        ", AVG(CASE WHEN TOT_REVENUE = 0 THEN NULL ELSE {metric_rpc_sql} END) AS ECOMM_REVENUE_NONZERO_MEAN\n",
        ", STDDEV(CASE WHEN TOT_REVENUE = 0 THEN NULL ELSE {metric_rpc_sql} END) AS ECOMM_REVENUE_NONZERO_SD\n",
        "--- SNAP\n",
        ", SUM(TOT_SNAP) AS ECOMM_SNAP_TOTAL\n",
        ", AVG(TOT_SNAP) AS ECOMM_SNAP_MEAN\n",
        ", STDDEV(TOT_SNAP) AS ECOMM_SNAP_SD\n",
        "--- BNC COUNT\n",
        ",SUM(ECOMM_BNC_COUNT) as ECOMM_BNC_TOTAL\n",
        ",STDDEV(CASE WHEN ECOMM_BNC_COUNT = 0 THEN NULL ELSE ECOMM_BNC_COUNT END) AS ECOMM_BNC_TOTAL_SD\n",
        "--- Ratio Metrics\n",
        " --- TOTAL MARKDOWN (from combined_txns - ALL markdowns)\n",
        "       , SUM(TOT_TOTAL_MARKDOWN) AS ECOMM_TOTAL_MARKDOWN_SUM\n",
        "       , AVG(TOT_TOTAL_MARKDOWN) AS ECOMM_TOTAL_MARKDOWN_MEAN\n",
        "       , STDDEV(TOT_TOTAL_MARKDOWN) AS ECOMM_TOTAL_MARKDOWN_SD\n",
        "  --- Ratio Metrics\n",
        "        , AVG(IFF(TOT_REVENUE > 0, {metric_rpc_sql}, NULL)) / AVG(IFF(NUM_ORDERS > 0, NUM_ORDERS, NULL)) AS ECOMM_AOV\n",
        ", COVAR_SAMP({metric_rpc_sql},NUM_ORDERS) AS ECOMM_COV_REVENUE_ORDERS\n",
        ", AVG(IFF(NUM_UNITS > 0, NUM_UNITS, NULL)) / AVG(IFF(NUM_ORDERS > 0 , NUM_ORDERS, NULL)) AS ECOMM_UPO\n",
        ", COVAR_SAMP(NUM_UNITS,NUM_ORDERS) AS ECOMM_COV_UNITS_ORDERS\n",
        "FROM TXNS\n",
        "GROUP BY all\n",
        "),\n",
        "\n",
        "\n",
        "REDEMPTIONS_AGG AS (\n",
        "SELECT VARIANT_ID,\n",
        "--- UNIQUE_REDEEMING_HH\n",
        "COUNT(DISTINCT IFF(TOTAL_REDEMPTIONS > 0, HOUSEHOLD_ID, NULL)) AS REDEEMING_COUNT_ECOMM\n",
        ", AVG(IFF(TOTAL_REDEMPTIONS > 0, TOTAL_REDEMPTIONS, NULL)) AS RPO_ECOMM_MEAN\n",
        ", STD(IFF(TOTAL_REDEMPTIONS > 0, TOTAL_REDEMPTIONS, NULL)) AS RPO_EECOMM_SD\n",
        "--- REDEMPTIONS TOTAL\n",
        ", SUM(TOTAL_REDEMPTIONS) as ECOMM_REDEMPTIONS_TOTAL\n",
        ", SUM(TOTAL_REDEMPTIONS) / COUNT(DISTINCT HOUSEHOLD_ID) as ECOMM_REDEMPTIONS_AVG_REAL\n",
        ", AVG(TOTAL_REDEMPTIONS) as ECOMM_REDEMPTIONS_MEAN\n",
        ", STDDEV(TOTAL_REDEMPTIONS) as ECOMM_REDEMPTIONS_SD\n",
        "--- MARKDOWN TOTAL\n",
        ", SUM(TOTAL_MKDN) as ECOMM_MKDN_TOTAL\n",
        ", SUM(TOTAL_MKDN) / COUNT(DISTINCT HOUSEHOLD_ID) as ECOMM_MKDN_AVG_REAL\n",
        ", AVG(TOTAL_MKDN) as ECOMM_MKDN_MEAN\n",
        ", STDDEV(TOTAL_MKDN) as ECOMM_MKDN_SD\n",
        "--- REDEMPTIONS BREAKDOWN\n",
        ", SUM(pd_redemptions) AS pd_redemptions_TOTAL\n",
        ", SUM(gr_redemptions) AS gr_redemptions_TOTAL\n",
        ", SUM(mf_redemptions) AS mf_redemptions_TOTAL\n",
        ", SUM(spd_redemptions) AS spd_redemptions_TOTAL\n",
        ", SUM(pzn_redemptions) AS pzn_redemptions_TOTAL\n",
        ", SUM(sc_redemptions) AS sc_redemptions_TOTAL\n",
        ", AVG(pd_redemptions) AS pd_redemptions_MEAN\n",
        ", AVG(gr_redemptions) AS gr_redemptions_MEAN\n",
        ", AVG(mf_redemptions) AS mf_redemptions_MEAN\n",
        ", AVG(spd_redemptions) AS spd_redemptions_MEAN\n",
        ", AVG(pzn_redemptions) AS pzn_redemptions_MEAN\n",
        ", AVG(sc_redemptions) AS sc_redemptions_MEAN\n",
        ", STDDEV(pd_redemptions) AS pd_redemptions_SD\n",
        ", STDDEV(gr_redemptions) AS gr_redemptions_SD\n",
        ", STDDEV(mf_redemptions) AS mf_redemptions_SD\n",
        ", STDDEV(spd_redemptions) AS spd_redemptions_SD\n",
        ", STDDEV(pzn_redemptions) AS pzn_redemptions_SD\n",
        ", STDDEV(sc_redemptions) AS sc_redemptions_SD\n",
        "--- MARKDOWN BREAKDOWN\n",
        ", SUM(pd_MKDN) AS pd_MKDN_TOTAL\n",
        ", SUM(gr_MKDN) AS gr_MKDN_TOTAL\n",
        ", SUM(mf_MKDN) AS mf_MKDN_TOTAL\n",
        ", SUM(spd_MKDN) AS spd_MKDN_TOTAL\n",
        ", SUM(pzn_MKDN) AS pzn_MKDN_TOTAL\n",
        ", SUM(sc_MKDN) AS sc_MKDN_TOTAL\n",
        ", AVG(pd_MKDN) AS pd_MKDN_MEAN\n",
        ", AVG(gr_MKDN) AS gr_MKDN_MEAN\n",
        ", AVG(mf_MKDN) AS mf_MKDN_MEAN\n",
        ", AVG(spd_MKDN) AS spd_MKDN_MEAN\n",
        ", AVG(pzn_MKDN) AS pzn_MKDN_MEAN\n",
        ", AVG(sc_MKDN) AS sc_MKDN_MEAN\n",
        ", STDDEV(pd_MKDN) AS pd_MKDN_SD\n",
        ", STDDEV(gr_MKDN) AS gr_MKDN_SD\n",
        ", STDDEV(mf_MKDN) AS mf_MKDN_SD\n",
        ", STDDEV(spd_MKDN) AS spd_MKDN_SD\n",
        ", STDDEV(pzn_MKDN) AS pzn_MKDN_SD\n",
        ", STDDEV(sc_MKDN) AS sc_MKDN_SD\n",
        "FROM REDEMPTIONS_FILTERED\n",
        "GROUP BY all\n",
        ")\n",
        "\n",
        "SELECT t.*, r.* EXCEPT(r.VARIANT_ID)\n",
        "FROM TXN_AGG as t\n",
        "JOIN REDEMPTIONS_AGG as r\n",
        "ON t.VARIANT_ID = r.VARIANT_ID\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "ecomm_agg_sp = spark.sql(ecomm_txns_agg)\n",
        "# ecomm_agg_sp.display()\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# In-Store Aggregation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#======================================================================================================================================\n",
        "# Prefilter and Join Aggregates\n",
        "#======================================================================================================================================\n",
        "\n",
        "# store_txns_agg = (\n",
        "# f\"\"\"\n",
        "# WITH\n",
        "# EXPOSURE_BASE AS (\n",
        "# SELECT * FROM {exposure_table}\n",
        "# ),\n",
        "# -- STEP 1: Create a single, rich, pre-joined transaction detail table for In-Store\n",
        "# STORE_TXN_DETAILS AS (\n",
        "# SELECT\n",
        "# t.HOUSEHOLD_ID,\n",
        "# t.TXN_ID,\n",
        "# t.TXN_DTE,\n",
        "# t.REVENUE,\n",
        "# t.ITEMS,\n",
        "# t.SNAP_TENDER,\n",
        "# r.CLIENT_OFFER_ID,\n",
        "# r.OFFER_TYPE_MOD,\n",
        "# r.MKDN\n",
        "# FROM {combined_txn_table} as t\n",
        "# LEFT JOIN {redemptions_table} as r\n",
        "# ON t.TXN_ID = r.TXN_ID\n",
        "# AND t.TXN_DTE = r.TXN_DTE -- Join on date can help if tables are partitioned\n",
        "# WHERE t.TXN_LOCATION = 'STORE' -- Filter for In-Store transactions\n",
        "# AND (t.TXN_DTE >= '{EXP_START_DATE}' AND t.TXN_DTE <= '{EXP_END_DATE}')\n",
        "# ),\n",
        "# -- STEP 2: Join exposures to details ONCE and aggregate ALL metrics at the household level\n",
        "# HOUSEHOLD_LEVEL_AGG AS (\n",
        "# SELECT\n",
        "# e.*,\n",
        "# -- Transaction Metrics\n",
        "# COUNT(DISTINCT d.TXN_ID) AS NUM_ORDERS,\n",
        "# SUM(d.REVENUE) AS TOT_REVENUE,\n",
        "# SUM(d.ITEMS) AS NUM_UNITS,\n",
        "# SUM(d.SNAP_TENDER) AS TOT_SNAP,\n",
        "# -- Redemption & Markdown Metrics (calculated in the same aggregation)\n",
        "# COUNT(DISTINCT CASE WHEN d.OFFER_TYPE_MOD = 'PD' THEN d.CLIENT_OFFER_ID || d.txn_id ELSE NULL END) as pd_redemptions,\n",
        "# COUNT(DISTINCT CASE WHEN d.OFFER_TYPE_MOD = 'GR' THEN d.CLIENT_OFFER_ID || d.txn_id ELSE NULL END) as gr_redemptions,\n",
        "# -- ... and so on for all redemption and markdown types ...\n",
        "# SUM(d.MKDN) as total_mkdn\n",
        "# FROM EXPOSURE_BASE AS e\n",
        "# LEFT JOIN STORE_TXN_DETAILS AS d\n",
        "# ON e.HOUSEHOLD_ID = d.HOUSEHOLD_ID\n",
        "# AND DATE(e.EXPOSURE_DATETIME) <= d.TXN_DTE\n",
        "# GROUP BY all\n",
        "# ),\n",
        "# -- STEP 3: Calculate Winsorization value\n",
        "# WINZ AS (\n",
        "# SELECT APPROX_PERCENTILE(TOT_REVENUE, 0.99) AS TOT_REVENUE_WIN99\n",
        "# FROM HOUSEHOLD_LEVEL_AGG\n",
        "# WHERE HOUSEHOLD_ID IS NOT NULL\n",
        "# )\n",
        "# -- STEP 4: Perform the final aggregation to the variant level\n",
        "# -- This CTE would contain all the final SUM/AVG/STDDEV calculations\n",
        "# -- from your original TXN_AGG and REDEMPTIONS_AGG, but applied to the\n",
        "# -- columns from the new, unified HOUSEHOLD_LEVEL_AGG CTE.\n",
        "# SELECT\n",
        "# h.VARIANT_ID,\n",
        "# COUNT(DISTINCT h.{visitor_unit}) AS VISITORS,\n",
        "# -- (The rest of your final aggregation logic goes here) ...\n",
        "# SUM(CASE WHEN h.TOT_REVENUE > (SELECT TOT_REVENUE_WIN99 FROM WINZ) THEN (SELECT TOT_REVENUE_WIN99 FROM WINZ) ELSE h.TOT_REVENUE END) AS STORE_REVENUE_TOTAL,\n",
        "# SUM(h.total_mkdn) AS STORE_MKDN_TOTAL\n",
        "# -- ... etc.\n",
        "# FROM HOUSEHOLD_LEVEL_AGG h\n",
        "# GROUP BY h.VARIANT_ID\n",
        "# \"\"\"\n",
        "# )\n",
        "# # Note: The final SELECT in the refactored query is conceptual. You would fill it\n",
        "# # with the complete list of metrics from your original query.\n",
        "\n",
        "\n",
        "\n",
        "store_txns_agg = (\n",
        "f\"\"\"\n",
        "WITH\n",
        "EXPOSURE_BASE AS(\n",
        "SELECT * FROM {exposure_table}\n",
        "),\n",
        "\n",
        "-- Join TXNs to exposures\n",
        "TXNS AS(\n",
        "SELECT e.*,\n",
        "COALESCE(COUNT(DISTINCT t.TXN_ID),0) AS NUM_ORDERS\n",
        ", COALESCE(SUM(t.REVENUE),0) AS TOT_REVENUE\n",
        ", COALESCE(SUM(t.ITEMS),0) AS NUM_UNITS\n",
        ", COALESCE(SUM(t.SNAP_TENDER),0) AS TOT_SNAP,\n",
        "     COALESCE(SUM(t.TOTAL_MARKDOWN),0) AS TOT_TOTAL_MARKDOWN\n",
        "FROM EXPOSURE_BASE as e\n",
        "LEFT JOIN (SELECT * FROM {combined_txn_table} WHERE TXN_LOCATION = 'STORE' AND (TXN_DTE >= '{EXP_START_DATE}' AND TXN_DTE <= '{EXP_END_DATE}') ) as t\n",
        "ON e.HOUSEHOLD_ID = t.HOUSEHOLD_ID\n",
        "AND DATE(e.EXPOSURE_DATETIME) <= t.TXN_DTE\n",
        "GROUP BY all\n",
        "),\n",
        "\n",
        "WINZ AS(\n",
        "SELECT\n",
        "APPROX_PERCENTILE(TOT_REVENUE,0.99) AS TOT_REVENUE_WIN99\n",
        "FROM TXNS\n",
        "WHERE HOUSEHOLD_ID IS NOT NULL\n",
        "),\n",
        "\n",
        "-- Join TXN info to Redemptions (primarily for TXN_DTE)\n",
        "TXNS_REDEMPTIONS AS(\n",
        "SELECT t.*\n",
        ", CLIENT_OFFER_ID\n",
        ", OFFER_TYPE_MOD\n",
        ", MKDN\n",
        "FROM {combined_txn_table} as t\n",
        "LEFT JOIN {redemptions_table} as r\n",
        "ON t.TXN_ID = r.TXN_ID\n",
        "AND (t.TXN_DTE >= '{EXP_START_DATE}' AND t.TXN_DTE <= '{EXP_END_DATE}')\n",
        "WHERE TXN_LOCATION = 'STORE'\n",
        "),\n",
        "\n",
        "-- Filter Redemptions by Exposure Datetime\n",
        "REDEMPTIONS_FILTERED AS(\n",
        "SELECT e.*,\n",
        "-- REDEMPTIONS\n",
        "COALESCE(COUNT(DISTINCT CASE WHEN offer_type_mod = 'PD' THEN (e.HOUSEHOLD_ID || CLIENT_OFFER_ID || t.txn_id) ELSE NULL END),0) as pd_redemptions\n",
        ", COALESCE(COUNT(DISTINCT CASE WHEN offer_type_mod = 'GR' THEN (e.HOUSEHOLD_ID || CLIENT_OFFER_ID || t.txn_id) ELSE NULL END),0) as gr_redemptions\n",
        ", COALESCE(COUNT(DISTINCT CASE WHEN offer_type_mod = 'MF' THEN (e.HOUSEHOLD_ID || CLIENT_OFFER_ID || t.txn_id) ELSE NULL END),0) as mf_redemptions\n",
        ", COALESCE(COUNT(DISTINCT CASE WHEN offer_type_mod = 'SPD' THEN (e.HOUSEHOLD_ID || CLIENT_OFFER_ID || t.txn_id) ELSE NULL END),0) as spd_redemptions\n",
        ", COALESCE(COUNT(DISTINCT CASE WHEN offer_type_mod = 'PZN' THEN (e.HOUSEHOLD_ID || CLIENT_OFFER_ID || t.txn_id) ELSE NULL END),0) as pzn_redemptions\n",
        ", COALESCE(COUNT(DISTINCT CASE WHEN offer_type_mod = 'SC' THEN (e.HOUSEHOLD_ID || CLIENT_OFFER_ID || t.txn_id) ELSE NULL END),0) as sc_redemptions\n",
        ", COALESCE(COUNT(DISTINCT (e.HOUSEHOLD_ID || CLIENT_OFFER_ID || t.txn_id)),0) as total_redemptions\n",
        "-- MKDN\n",
        ", COALESCE(SUM(CASE WHEN offer_type_mod = 'PD' THEN MKDN ELSE 0 END),0) as pd_MKDN\n",
        ", COALESCE(SUM(CASE WHEN offer_type_mod = 'GR' THEN MKDN ELSE 0 END),0) as gr_MKDN\n",
        ", COALESCE(SUM(CASE WHEN offer_type_mod = 'MF' THEN MKDN ELSE 0 END),0) as mf_MKDN\n",
        ", COALESCE(SUM(CASE WHEN offer_type_mod = 'SPD' THEN MKDN ELSE 0 END),0) as spd_MKDN\n",
        ", COALESCE(SUM(CASE WHEN offer_type_mod = 'PZN' THEN MKDN ELSE 0 END),0) as pzn_MKDN\n",
        ", COALESCE(SUM(CASE WHEN offer_type_mod = 'SC' THEN MKDN ELSE 0 END),0) as sc_MKDN\n",
        ", COALESCE(SUM(MKDN),0) as total_mkdn\n",
        "FROM EXPOSURE_BASE as e\n",
        "LEFT JOIN TXNS_REDEMPTIONS as t\n",
        "ON e.HOUSEHOLD_ID = t.HOUSEHOLD_ID\n",
        "AND DATE(e.EXPOSURE_DATETIME) <= t.TXN_DTE\n",
        "GROUP BY all\n",
        "),\n",
        "\n",
        "TXN_AGG AS(\n",
        "SELECT VARIANT_ID,\n",
        "COUNT(DISTINCT {visitor_unit}) AS VISITORS\n",
        ", COUNT(DISTINCT IFF(NUM_ORDERS > 0, HOUSEHOLD_ID, NULL)) AS PURCHASING_CUSTOMERS\n",
        ", COUNT(DISTINCT IFF(TOT_REVENUE > (SELECT TOT_REVENUE_WIN99 FROM WINZ), HOUSEHOLD_ID, NULL)) AS WINSORIZED_CUSTOMERS\n",
        ", (SELECT TOT_REVENUE_WIN99 FROM WINZ) AS WINSORIZATION_THRESHOLD\n",
        "-- ORDERS\n",
        ", SUM(NUM_ORDERS) AS STORE_ORDERS_TOTAL\n",
        ", AVG(NUM_ORDERS) AS STORE_ORDERS_MEAN\n",
        ", STDDEV(NUM_ORDERS) AS STORE_ORDERS_SD\n",
        "-- UNITS\n",
        ", SUM(NUM_UNITS) AS STORE_UNITS_TOTAL\n",
        ", AVG(NUM_UNITS) AS STORE_UNITS_MEAN\n",
        ", STDDEV(NUM_UNITS) AS STORE_UNITS_SD\n",
        "--- REVENUE\n",
        ", SUM({metric_rpc_sql}) AS STORE_REVENUE_TOTAL\n",
        ", AVG({metric_rpc_sql}) AS STORE_REVENUE_MEAN\n",
        ", SUM({metric_rpc_sql}) / COUNT(DISTINCT HOUSEHOLD_ID) AS STORE_RPV\n",
        ", SUM(\n",
        "CASE WHEN TOT_REVENUE = 0\n",
        "THEN NULL ELSE {metric_rpc_sql} END) / COUNT(DISTINCT HOUSEHOLD_ID) AS STORE_NONZERO_RPV\n",
        ", STDDEV({metric_rpc_sql}) AS STORE_REVENUE_SD\n",
        "--- NON ZERO REVENUE\n",
        ",AVG(CASE WHEN TOT_REVENUE = 0 THEN NULL ELSE {metric_rpc_sql} END) AS STORE_REVENUE_NONZERO_MEAN\n",
        ",STDDEV(CASE WHEN TOT_REVENUE = 0 THEN NULL ELSE {metric_rpc_sql} END) AS STORE_REVENUE_NONZERO_SD\n",
        "\n",
        "--- SNAP\n",
        ", SUM(TOT_SNAP) AS STORE_SNAP_TOTAL\n",
        ", AVG(TOT_SNAP) AS STORE_SNAP_MEAN\n",
        ", STDDEV(TOT_SNAP) AS STORE_SNAP_SD\n",
        "--- Ratio Metrics\n",
        " --- TOTAL MARKDOWN (from combined_txns - ALL markdowns)\n",
        "       , SUM(TOT_TOTAL_MARKDOWN) AS STORE_TOTAL_MARKDOWN_SUM\n",
        "       , AVG(TOT_TOTAL_MARKDOWN) AS STORE_TOTAL_MARKDOWN_MEAN\n",
        "       , STDDEV(TOT_TOTAL_MARKDOWN) AS STORE_TOTAL_MARKDOWN_SD\n",
        "  --- Ratio Metrics\n",
        "        , AVG(IFF(TOT_REVENUE > 0, {metric_rpc_sql}, NULL)) / AVG(IFF(NUM_ORDERS > 0, NUM_ORDERS, NULL)) AS STORE_AOV\n",
        ", COVAR_SAMP({metric_rpc_sql},NUM_ORDERS) AS STORE_COV_REVENUE_ORDERS\n",
        ", AVG(IFF(NUM_UNITS > 0, NUM_UNITS, NULL)) / AVG(IFF(NUM_ORDERS > 0 , NUM_ORDERS, NULL)) AS STORE_UPO\n",
        ", COVAR_SAMP(NUM_UNITS,NUM_ORDERS) AS STORE_COV_UNITS_ORDERS\n",
        "FROM TXNS\n",
        "GROUP BY all\n",
        "),\n",
        "\n",
        "REDEMPTIONS_AGG AS (\n",
        "SELECT VARIANT_ID,\n",
        "--- UNIQUE_REDEEMING_HH\n",
        "COUNT(DISTINCT IFF(TOTAL_REDEMPTIONS > 0, HOUSEHOLD_ID, NULL)) AS REDEEMING_COUNT_STORE\n",
        ", AVG(IFF(TOTAL_REDEMPTIONS > 0, TOTAL_REDEMPTIONS, NULL)) AS RPO_STORE_MEAN\n",
        ", STD(IFF(TOTAL_REDEMPTIONS > 0, TOTAL_REDEMPTIONS, NULL)) AS RPO_ESTORE_SD\n",
        "--- REDEMPTIONS TOTAL\n",
        ", SUM(TOTAL_REDEMPTIONS) as STORE_REDEMPTIONS_TOTAL\n",
        ", SUM(TOTAL_REDEMPTIONS) / COUNT(DISTINCT HOUSEHOLD_ID) as STORE_REDEMPTIONS_AVG_REAL\n",
        ", AVG(TOTAL_REDEMPTIONS) as STORE_REDEMPTIONS_MEAN\n",
        ", STDDEV(TOTAL_REDEMPTIONS) as STORE_REDEMPTIONS_SD\n",
        "--- MARKDOWN TOTAL\n",
        ", SUM(TOTAL_MKDN) as STORE_MKDN_TOTAL\n",
        ", SUM(TOTAL_MKDN) / COUNT(DISTINCT HOUSEHOLD_ID) as STORE_MKDN_AVG_REAL\n",
        ", AVG(TOTAL_MKDN) as STORE_MKDN_MEAN\n",
        ", STDDEV(TOTAL_MKDN) as STORE_MKDN_SD\n",
        "--- REDEMPTIONS BREAKDOWN\n",
        ", SUM(pd_redemptions) AS pd_redemptions_TOTAL\n",
        ", SUM(gr_redemptions) AS gr_redemptions_TOTAL\n",
        ", SUM(mf_redemptions) AS mf_redemptions_TOTAL\n",
        ", SUM(spd_redemptions) AS spd_redemptions_TOTAL\n",
        ", SUM(pzn_redemptions) AS pzn_redemptions_TOTAL\n",
        ", SUM(sc_redemptions) AS sc_redemptions_TOTAL\n",
        ", AVG(pd_redemptions) AS pd_redemptions_MEAN\n",
        ", AVG(gr_redemptions) AS gr_redemptions_MEAN\n",
        ", AVG(mf_redemptions) AS mf_redemptions_MEAN\n",
        ", AVG(spd_redemptions) AS spd_redemptions_MEAN\n",
        ", AVG(pzn_redemptions) AS pzn_redemptions_MEAN\n",
        ", AVG(sc_redemptions) AS sc_redemptions_MEAN\n",
        ", STDDEV(pd_redemptions) AS pd_redemptions_SD\n",
        ", STDDEV(gr_redemptions) AS gr_redemptions_SD\n",
        ", STDDEV(mf_redemptions) AS mf_redemptions_SD\n",
        ", STDDEV(spd_redemptions) AS spd_redemptions_SD\n",
        ", STDDEV(pzn_redemptions) AS pzn_redemptions_SD\n",
        ", STDDEV(sc_redemptions) AS sc_redemptions_SD\n",
        "--- MARKDOWN BREAKDOWN\n",
        ", SUM(pd_MKDN) AS pd_MKDN_TOTAL\n",
        ", SUM(gr_MKDN) AS gr_MKDN_TOTAL\n",
        ", SUM(mf_MKDN) AS mf_MKDN_TOTAL\n",
        ", SUM(spd_MKDN) AS spd_MKDN_TOTAL\n",
        ", SUM(pzn_MKDN) AS pzn_MKDN_TOTAL\n",
        ", SUM(sc_MKDN) AS sc_MKDN_TOTAL\n",
        ", AVG(pd_MKDN) AS pd_MKDN_MEAN\n",
        ", AVG(gr_MKDN) AS gr_MKDN_MEAN\n",
        ", AVG(mf_MKDN) AS mf_MKDN_MEAN\n",
        ", AVG(spd_MKDN) AS spd_MKDN_MEAN\n",
        ", AVG(pzn_MKDN) AS pzn_MKDN_MEAN\n",
        ", AVG(sc_MKDN) AS sc_MKDN_MEAN\n",
        ", STDDEV(pd_MKDN) AS pd_MKDN_SD\n",
        ", STDDEV(gr_MKDN) AS gr_MKDN_SD\n",
        ", STDDEV(mf_MKDN) AS mf_MKDN_SD\n",
        ", STDDEV(spd_MKDN) AS spd_MKDN_SD\n",
        ", STDDEV(pzn_MKDN) AS pzn_MKDN_SD\n",
        ", STDDEV(sc_MKDN) AS sc_MKDN_SD\n",
        "FROM REDEMPTIONS_FILTERED\n",
        "GROUP BY all\n",
        ")\n",
        "\n",
        "SELECT t.*, r.* EXCEPT(r.VARIANT_ID)\n",
        "FROM TXN_AGG as t\n",
        "JOIN REDEMPTIONS_AGG as r\n",
        "ON t.VARIANT_ID = r.VARIANT_ID\n",
        "\"\"\")\n",
        "\n",
        "store_agg_sp = spark.sql(store_txns_agg)\n",
        "# store_agg_sp.display()\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Super Query\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# WITH\n",
        "# -- 1. Prepare ALL transaction details (no e-comm or store filter yet)\n",
        "# ALL_TXN_DETAILS AS (\n",
        "# SELECT t.*, r.* FROM {combined_txn_table} t LEFT JOIN {redemptions_table} r ON ...\n",
        "# ),\n",
        "# -- 2. Join to exposures once and aggregate everything with conditional logic\n",
        "# HOUSEHOLD_SUPER_AGG AS (\n",
        "# SELECT\n",
        "# e.HOUSEHOLD_ID, e.VARIANT_ID,\n",
        "# -- Combined Metrics\n",
        "# COUNT(DISTINCT d.TXN_ID) as COMBINED_ORDERS,\n",
        "# -- E-comm Metrics\n",
        "# COUNT(DISTINCT CASE WHEN d.TXN_LOCATION = 'ECOMM' THEN d.TXN_ID ELSE NULL END) as ECOMM_ORDERS,\n",
        "# SUM(CASE WHEN d.TXN_LOCATION = 'ECOMM' THEN d.REVENUE ELSE 0 END) as ECOMM_REVENUE,\n",
        "# -- Store Metrics\n",
        "# COUNT(DISTINCT CASE WHEN d.TXN_LOCATION = 'STORE' THEN d.TXN_ID ELSE NULL END) as STORE_ORDERS,\n",
        "# SUM(CASE WHEN d.TXN_LOCATION = 'STORE' THEN d.REVENUE ELSE 0 END) as STORE_REVENUE\n",
        "# -- ... and so on for every single metric ...\n",
        "# FROM EXPOSURE_BASE e\n",
        "# LEFT JOIN ALL_TXN_DETAILS d ON ...\n",
        "# GROUP BY e.HOUSEHOLD_ID, e.VARIANT_ID\n",
        "# )\n",
        "# -- 3. Final variant-level aggregation\n",
        "# SELECT\n",
        "# VARIANT_ID,\n",
        "# -- Aggregate the combined columns\n",
        "# SUM(COMBINED_ORDERS) AS FINAL_COMBINED_ORDERS,\n",
        "# -- Aggregate the e-comm columns\n",
        "# SUM(ECOMM_ORDERS) AS FINAL_ECOMM_ORDERS,\n",
        "# -- Aggregate the store columns\n",
        "# SUM(STORE_ORDERS) AS FINAL_STORE_ORDERS\n",
        "# -- ... etc.\n",
        "# FROM HOUSEHOLD_SUPER_AGG\n",
        "# GROUP BY VARIANT_ID\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TXNS Aggregation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#======================================================================================================================================\n",
        "# Prefilter and Join Aggregates\n",
        "#======================================================================================================================================\n",
        "\n",
        "# This txns_agg query is the perfect example of the redundant work that the holistic optimization is designed to solve.\n",
        "# Analysis of This Query\n",
        "# * In Isolation: If this were the only transaction-related query in your notebook, it would be considered reasonably efficient. It still has the expensive non-equi join (DATE(e.EXPOSURE_DATETIME) <= t.TXN_DTE), but it gets the job done.\n",
        "# * In the Context of Your Notebook: This is the third time your notebook is performing a nearly identical, expensive operation:\n",
        "# * First, in ecomm_txns_agg (scanning {combined_txn_table} for E-comm data).\n",
        "# * Second, in store_txns_agg (scanning {combined_txn_table} for Store data).\n",
        "# * Third, here in txns_agg (scanning {combined_txn_table} again for all data).\n",
        "# Each of these runs involves reading a large table and performing a costly join. This repeated work is a primary contributor to your notebook's long runtime.\n",
        "# The Solution: Replace with the Holistic Approach\n",
        "# Instead of running this query, you should rely on the output from the single, unified query we discussed in the previous step.\n",
        "# Let's recap how that holistic query would produce the results for this block:\n",
        "# * Prepare Detailed Data: You create a single ALL_TXN_DETAILS CTE by joining {combined_txn_table} with {redemptions_table} once, without any TXN_LOCATION filter.\n",
        "# * Join to Exposures and Aggregate: You join this detailed data to your EXPOSURE_BASE once. In this step, you use conditional aggregation to create columns for E-comm, Store, and Combined totals at the household level.\n",
        "# -- Inside the HOUSEHOLD_SUPER_AGG CTE from the holistic approach...\n",
        "# SELECT\n",
        "# e.HOUSEHOLD_ID, e.VARIANT_ID,\n",
        "# -- Combined Metrics (for THIS query)\n",
        "# COUNT(DISTINCT d.TXN_ID) as COMBINED_ORDERS,\n",
        "# SUM(d.REVENUE) as COMBINED_REVENUE,\n",
        "# SUM(d.ITEMS) as COMBINED_UNITS,\n",
        "# SUM(d.SNAP_TENDER) as COMBINED_SNAP,\n",
        "# -- E-comm Metrics (for the ecomm_txns_agg query)\n",
        "# COUNT(DISTINCT CASE WHEN d.TXN_LOCATION = 'ECOMM' THEN d.TXN_ID ELSE NULL END) as ECOMM_ORDERS,\n",
        "# -- ... etc. for E-comm\n",
        "# -- Store Metrics (for the store_txns_agg query)\n",
        "# COUNT(DISTINCT CASE WHEN d.TXN_LOCATION = 'STORE' THEN d.TXN_ID ELSE NULL END) as STORE_ORDERS\n",
        "# -- ... etc. for Store\n",
        "# FROM EXPOSURE_BASE e\n",
        "# LEFT JOIN ALL_TXN_DETAILS d ON ...\n",
        "# GROUP BY e.HOUSEHOLD_ID, e.VARIANT_ID\n",
        "# * Final Aggregation: Your final variant-level aggregation would then calculate the SUM, AVG, and STDDEV for the COMBINED_... columns you just created.\n",
        "# Conclusion\n",
        "# This txns_agg query block should ideally be eliminated. The results it produces should be generated as part of the single, unified query that also calculates the E-comm and In-Store metrics.\n",
        "# By adopting that holistic strategy, you replace three long-running, resource-intensive queries with just one, which will be one of the most impactful optimizations for reducing your notebook's runtime.\n",
        "\n",
        "\n",
        "txns_agg = (\n",
        "f\"\"\"\n",
        "WITH\n",
        "EXPOSURE_BASE AS(\n",
        "SELECT * FROM {exposure_table}\n",
        "),\n",
        "\n",
        "TXNS AS(\n",
        "SELECT e.*\n",
        ", COALESCE(COUNT(DISTINCT t.TXN_ID),0) AS NUM_ORDERS\n",
        ", COALESCE(SUM(t.REVENUE),0) AS TOT_REVENUE\n",
        ", COALESCE(SUM(t.ITEMS),0) AS NUM_UNITS\n",
        ", COALESCE(SUM(t.SNAP_TENDER),0) AS TOT_SNAP,\n",
        "     COALESCE(SUM(t.TOTAL_MARKDOWN),0) AS TOT_TOTAL_MARKDOWN\n",
        "FROM EXPOSURE_BASE as e\n",
        "LEFT JOIN {combined_txn_table} as t\n",
        "ON e.HOUSEHOLD_ID = t.HOUSEHOLD_ID\n",
        "AND DATE(e.EXPOSURE_DATETIME) <= t.TXN_DTE\n",
        "AND (t.TXN_DTE >= '{EXP_START_DATE}' AND t.TXN_DTE <= '{EXP_END_DATE}')\n",
        "GROUP BY all\n",
        "),\n",
        "\n",
        "WINZ AS(\n",
        "SELECT\n",
        "APPROX_PERCENTILE(TOT_REVENUE,0.99) AS TOT_REVENUE_WIN99\n",
        "FROM TXNS\n",
        "WHERE HOUSEHOLD_ID IS NOT NULL\n",
        ")\n",
        "\n",
        "SELECT VARIANT_ID\n",
        ", COUNT(DISTINCT {visitor_unit}) AS VISITORS\n",
        ", COUNT(DISTINCT IFF(NUM_ORDERS > 0, HOUSEHOLD_ID, NULL)) AS PURCHASING_CUSTOMERS\n",
        ", COUNT(DISTINCT IFF(TOT_REVENUE > (SELECT TOT_REVENUE_WIN99 FROM WINZ), HOUSEHOLD_ID, NULL)) AS WINSORIZED_CUSTOMERS\n",
        ", (SELECT TOT_REVENUE_WIN99 FROM WINZ) AS WINSORIZATION_THRESHOLD\n",
        "-- ORDERS\n",
        ", SUM(NUM_ORDERS) AS COMBINED_ORDERS_TOTAL\n",
        ", AVG(NUM_ORDERS) AS COMBINED_ORDERS_MEAN\n",
        ", STDDEV(NUM_ORDERS) AS COMBINED_ORDERS_SD\n",
        "-- UNITS\n",
        ", SUM(NUM_UNITS) AS COMBINED_UNITS_TOTAL\n",
        ", AVG(NUM_UNITS) AS COMBINED_UNITS_MEAN\n",
        ", STDDEV(NUM_UNITS) AS COMBINED_UNITS_SD\n",
        "--- REVENUE\n",
        ", SUM({metric_rpc_sql}) AS COMBINED_REVENUE_TOTAL\n",
        ", AVG({metric_rpc_sql}) AS COMBINED_REVENUE_MEAN\n",
        ", SUM({metric_rpc_sql}) / COUNT(DISTINCT HOUSEHOLD_ID) AS COMBINED_RPV\n",
        ", SUM(CASE WHEN TOT_REVENUE > 0 THEN NULL ELSE {metric_rpc_sql} END) / COUNT(DISTINCT HOUSEHOLD_ID) AS COMBINED_NONZERO_RPV\n",
        ", STDDEV({metric_rpc_sql}) AS COMBINED_REVENUE_SD\n",
        "--- NON ZERO REVENUE\n",
        ",AVG(CASE WHEN TOT_REVENUE = 0 THEN NULL ELSE {metric_rpc_sql} END) AS COMBINED_REVENUE_NONZERO_MEAN\n",
        ",STDDEV(CASE WHEN TOT_REVENUE = 0 THEN NULL ELSE {metric_rpc_sql} END) AS COMBINED_REVENUE_NONZERO_SD\n",
        "--- SNAP\n",
        ", SUM(TOT_SNAP) AS COMBINED_SNAP_TOTAL\n",
        ", AVG(TOT_SNAP) AS COMBINED_SNAP_MEAN\n",
        ", STDDEV(TOT_SNAP) AS COMBINED_SNAP_SD\n",
        "--- Ratio Metrics\n",
        ", AVG(IFF(TOT_REVENUE > 0, {metric_rpc_sql}, NULL)) / AVG(IFF(NUM_ORDERS > 0, NUM_ORDERS, NULL)) AS COMBINED_AOV\n",
        ", COVAR_SAMP({metric_rpc_sql},NUM_ORDERS) AS COMBINED_COV_REVENUE_ORDERS\n",
        ", AVG(IFF(NUM_UNITS > 0, NUM_UNITS, NULL)) / AVG(IFF(NUM_ORDERS > 0 , NUM_ORDERS, NULL)) AS COMBINED_UPO\n",
        ", COVAR_SAMP(NUM_UNITS,NUM_ORDERS) AS COMBINED_COV_UNITS_ORDERS\n",
        "FROM TXNS\n",
        "GROUP BY all\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "txns_sp = spark.sql(txns_agg)\n",
        "#txns_sp.display()\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Redemptions Aggregation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#======================================================================================================================================\n",
        "# Super Query\n",
        "#======================================================================================================================================\n",
        "\n",
        "\n",
        "# This query follows a similar pattern to your previous transaction-based aggregations. While it is logically correct for calculating the combined redemption and markdown metrics, it represents another instance of redundant processing in the context of your full notebook.\n",
        "# Analysis\n",
        "# * Correct Logic: The query correctly joins transaction data with redemption data, then joins that to the exposed households, and performs a two-step aggregation (first to the household level, then to the variant level). This process accurately calculates the desired metrics.\n",
        "# * The Inefficiency: The issue is that this query re-scans the same large tables ({combined_txn_table}, {redemptions_table}, {exposure_table}) that were already processed for the E-comm and In-Store transaction queries. It also repeats the expensive non-equi join on the exposure date.\n",
        "# The Solution: Integrate into the Holistic Query\n",
        "# This redemptions_agg query is the final piece that should be merged into the single, unified \"super-query\" we discussed. All the calculations performed here can and should be done within that single, efficient data flow.\n",
        "# Here\u00e2\u20ac\u2122s how the logic from this query fits perfectly into that holistic approach:\n",
        "# * Prepare Detailed Data: The ALL_TXN_DETAILS CTE from the holistic approach already joins transactions with redemptions. This replaces your TXNS_REDEMPTIONS CTE.\n",
        "# -- From the holistic query...\n",
        "# ALL_TXN_DETAILS AS (\n",
        "# SELECT t.*, r.CLIENT_OFFER_ID, r.OFFER_TYPE_MOD, r.MKDN\n",
        "# FROM {combined_txn_table} t\n",
        "# LEFT JOIN {redemptions_table} r ON t.TXN_ID = r.TXN_ID ...\n",
        "# )\n",
        "# * Single Household-Level Aggregation: The household-level aggregation step in the holistic query is where you would add all the calculations from your REDEMPTIONS_FILTERED CTE.\n",
        "# -- Inside the unified HOUSEHOLD_SUPER_AGG CTE...\n",
        "# SELECT\n",
        "# e.HOUSEHOLD_ID, e.VARIANT_ID,\n",
        "# -- Transaction metrics (as discussed before)\n",
        "# COUNT(DISTINCT d.TXN_ID) as COMBINED_ORDERS,\n",
        "# SUM(CASE WHEN d.TXN_LOCATION = 'ECOMM' THEN d.REVENUE ELSE 0 END) as ECOMM_REVENUE,\n",
        "# -- ... etc ...\n",
        "# -- Redemption & Markdown metrics (from THIS query, now integrated)\n",
        "# COUNT(DISTINCT CASE WHEN d.OFFER_TYPE_MOD = 'PD' THEN d.CLIENT_OFFER_ID || d.txn_id ELSE NULL END) as pd_redemptions,\n",
        "# COUNT(DISTINCT CASE WHEN d.OFFER_TYPE_MOD = 'GR' THEN d.CLIENT_OFFER_ID || d.txn_id ELSE NULL END) as gr_redemptions,\n",
        "# -- ... and so on for all redemption types ...\n",
        "# SUM(d.MKDN) as total_mkdn,\n",
        "# SUM(CASE WHEN d.OFFER_TYPE_MOD = 'PD' THEN d.MKDN ELSE 0 END) as pd_MKDN,\n",
        "# -- ... and so on for all markdown types ...\n",
        "# FROM EXPOSURE_BASE e\n",
        "# LEFT JOIN ALL_TXN_DETAILS d ON e.HOUSEHOLD_ID = d.HOUSEHOLD_ID AND DATE(e.EXPOSURE_DATETIME) <= d.TXN_DTE\n",
        "# GROUP BY e.HOUSEHOLD_ID, e.VARIANT_ID\n",
        "# * Final Variant-Level Aggregation: The final SELECT in your holistic query would then perform the SUM, AVG, STDDEV on these newly created redemption and markdown columns.\n",
        "# Conclusion\n",
        "# This redemptions_agg query should be removed and not run as a separate step.\n",
        "# By incorporating its logic into the single, unified transaction query that handles E-comm, In-Store, and Combined metrics, you will:\n",
        "# * Eliminate redundant scans of your largest tables.\n",
        "# * Perform the expensive date-based join only once.\n",
        "# * Significantly reduce the overall runtime and resource consumption of your notebook.\n",
        "\n",
        "redemptions_agg = (\n",
        "f\"\"\"\n",
        "WITH\n",
        "EXPOSURE_BASE AS(\n",
        "SELECT * FROM {exposure_table}\n",
        "),\n",
        "\n",
        "-- Join TXN info to Redemptions (primarily for TXN_DTE)\n",
        "TXNS_REDEMPTIONS AS(\n",
        "SELECT t.*\n",
        ", CLIENT_OFFER_ID\n",
        ", OFFER_TYPE_MOD\n",
        ", MKDN\n",
        "FROM {combined_txn_table} as t\n",
        "LEFT JOIN {redemptions_table} as r\n",
        "ON t.TXN_ID = r.TXN_ID\n",
        "AND (t.TXN_DTE >= '{EXP_START_DATE}' AND t.TXN_DTE <= '{EXP_END_DATE}')\n",
        "),\n",
        "\n",
        "-- Filter Redemptions by Exposure Datetime\n",
        "REDEMPTIONS_FILTERED AS(\n",
        "SELECT e.*,\n",
        "-- REDEMPTIONS\n",
        "COALESCE(COUNT(DISTINCT CASE WHEN offer_type_mod = 'PD' THEN (e.HOUSEHOLD_ID || CLIENT_OFFER_ID || t.txn_id) ELSE NULL END),0) as pd_redemptions\n",
        ", COALESCE(COUNT(DISTINCT CASE WHEN offer_type_mod = 'GR' THEN (e.HOUSEHOLD_ID || CLIENT_OFFER_ID || t.txn_id) ELSE NULL END),0) as gr_redemptions\n",
        ", COALESCE(COUNT(DISTINCT CASE WHEN offer_type_mod = 'MF' THEN (e.HOUSEHOLD_ID || CLIENT_OFFER_ID || t.txn_id) ELSE NULL END),0) as mf_redemptions\n",
        ", COALESCE(COUNT(DISTINCT CASE WHEN offer_type_mod = 'SPD' THEN (e.HOUSEHOLD_ID || CLIENT_OFFER_ID || t.txn_id) ELSE NULL END),0) as spd_redemptions\n",
        ", COALESCE(COUNT(DISTINCT CASE WHEN offer_type_mod = 'PZN' THEN (e.HOUSEHOLD_ID || CLIENT_OFFER_ID || t.txn_id) ELSE NULL END),0) as pzn_redemptions\n",
        ", COALESCE(COUNT(DISTINCT CASE WHEN offer_type_mod = 'SC' THEN (e.HOUSEHOLD_ID || CLIENT_OFFER_ID || t.txn_id) ELSE NULL END),0) as sc_redemptions\n",
        ", COALESCE(COUNT(DISTINCT (e.HOUSEHOLD_ID || CLIENT_OFFER_ID || t.txn_id)),0) as total_redemptions\n",
        "-- MKDN\n",
        ", COALESCE(SUM(CASE WHEN offer_type_mod = 'PD' THEN MKDN ELSE 0 END),0) as pd_MKDN\n",
        ", COALESCE(SUM(CASE WHEN offer_type_mod = 'GR' THEN MKDN ELSE 0 END),0) as gr_MKDN\n",
        ", COALESCE(SUM(CASE WHEN offer_type_mod = 'MF' THEN MKDN ELSE 0 END),0) as mf_MKDN\n",
        ", COALESCE(SUM(CASE WHEN offer_type_mod = 'SPD' THEN MKDN ELSE 0 END),0) as spd_MKDN\n",
        ", COALESCE(SUM(CASE WHEN offer_type_mod = 'PZN' THEN MKDN ELSE 0 END),0) as pzn_MKDN\n",
        ", COALESCE(SUM(CASE WHEN offer_type_mod = 'SC' THEN MKDN ELSE 0 END),0) as sc_MKDN\n",
        ", COALESCE(SUM(MKDN),0) as total_mkdn\n",
        "FROM EXPOSURE_BASE as e\n",
        "LEFT JOIN TXNS_REDEMPTIONS as t\n",
        "ON e.HOUSEHOLD_ID = t.HOUSEHOLD_ID\n",
        "AND t.TXN_DTE BETWEEN DATE(e.EXPOSURE_DATETIME) AND '{EXP_END_DATE}'\n",
        "GROUP BY all\n",
        ")\n",
        "\n",
        "SELECT VARIANT_ID\n",
        ", COUNT(DISTINCT {visitor_unit}) AS VISITORS\n",
        "--- UNIQUE_REDEEMING_HH\n",
        ", COUNT(DISTINCT IFF(TOTAL_REDEMPTIONS > 0, HOUSEHOLD_ID, NULL)) AS REDEEMING_COUNT_COMBINED\n",
        ", AVG(IFF(TOTAL_REDEMPTIONS > 0, TOTAL_REDEMPTIONS, NULL)) AS RPO_COMBINED_MEAN\n",
        ", STD(IFF(TOTAL_REDEMPTIONS > 0, TOTAL_REDEMPTIONS, NULL)) AS RPO_ECOMBINED_SD\n",
        "--- REDEMPTIONS TOTAL\n",
        ", SUM(TOTAL_REDEMPTIONS) as REDEMPTIONS_TOTAL\n",
        ", SUM(TOTAL_REDEMPTIONS) / COUNT(DISTINCT HOUSEHOLD_ID) as REDEMPTIONS_AVG_REAL\n",
        ", AVG(TOTAL_REDEMPTIONS) as REDEMPTIONS_MEAN\n",
        ", STDDEV(TOTAL_REDEMPTIONS) as REDEMPTIONS_SD\n",
        "--- MARKDOWN TOTAL\n",
        ", SUM(TOTAL_MKDN) as MKDN_TOTAL\n",
        ", SUM(TOTAL_MKDN) / COUNT(DISTINCT HOUSEHOLD_ID) as MKDN_AVG_REAL\n",
        ", AVG(TOTAL_MKDN) as MKDN_MEAN\n",
        ", STDDEV(TOTAL_MKDN) as MKDN_SD\n",
        "--- REDEMPTIONS BREAKDOWN\n",
        ", SUM(pd_redemptions) AS pd_redemptions_TOTAL\n",
        ", SUM(gr_redemptions) AS gr_redemptions_TOTAL\n",
        ", SUM(mf_redemptions) AS mf_redemptions_TOTAL\n",
        ", SUM(spd_redemptions) AS spd_redemptions_TOTAL\n",
        ", SUM(pzn_redemptions) AS pzn_redemptions_TOTAL\n",
        ", SUM(sc_redemptions) AS sc_redemptions_TOTAL\n",
        ", AVG(pd_redemptions) AS pd_redemptions_MEAN\n",
        ", AVG(gr_redemptions) AS gr_redemptions_MEAN\n",
        ", AVG(mf_redemptions) AS mf_redemptions_MEAN\n",
        ", AVG(spd_redemptions) AS spd_redemptions_MEAN\n",
        ", AVG(pzn_redemptions) AS pzn_redemptions_MEAN\n",
        ", AVG(sc_redemptions) AS sc_redemptions_MEAN\n",
        ", STDDEV(pd_redemptions) AS pd_redemptions_SD\n",
        ", STDDEV(gr_redemptions) AS gr_redemptions_SD\n",
        ", STDDEV(mf_redemptions) AS mf_redemptions_SD\n",
        ", STDDEV(spd_redemptions) AS spd_redemptions_SD\n",
        ", STDDEV(pzn_redemptions) AS pzn_redemptions_SD\n",
        ", STDDEV(sc_redemptions) AS sc_redemptions_SD\n",
        "--- MARKDOWN BREAKDOWN\n",
        ", SUM(pd_MKDN) AS pd_MKDN_TOTAL\n",
        ", SUM(gr_MKDN) AS gr_MKDN_TOTAL\n",
        ", SUM(mf_MKDN) AS mf_MKDN_TOTAL\n",
        ", SUM(spd_MKDN) AS spd_MKDN_TOTAL\n",
        ", SUM(pzn_MKDN) AS pzn_MKDN_TOTAL\n",
        ", SUM(sc_MKDN) AS sc_MKDN_TOTAL\n",
        ", AVG(pd_MKDN) AS pd_MKDN_MEAN\n",
        ", AVG(gr_MKDN) AS gr_MKDN_MEAN\n",
        ", AVG(mf_MKDN) AS mf_MKDN_MEAN\n",
        ", AVG(spd_MKDN) AS spd_MKDN_MEAN\n",
        ", AVG(pzn_MKDN) AS pzn_MKDN_MEAN\n",
        ", AVG(sc_MKDN) AS sc_MKDN_MEAN\n",
        ", STDDEV(pd_MKDN) AS pd_MKDN_SD\n",
        ", STDDEV(gr_MKDN) AS gr_MKDN_SD\n",
        ", STDDEV(mf_MKDN) AS mf_MKDN_SD\n",
        ", STDDEV(spd_MKDN) AS spd_MKDN_SD\n",
        ", STDDEV(pzn_MKDN) AS pzn_MKDN_SD\n",
        ", STDDEV(sc_MKDN) AS sc_MKDN_SD\n",
        "FROM REDEMPTIONS_FILTERED\n",
        "GROUP BY all\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "redemptions_sp = spark.sql(redemptions_agg)\n",
        "#redemptions_sp.display()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Bonus Points Query\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bp_query = f\"\"\"\n",
        "select SAFE_CAST(t.HOUSEHOLD_ID as BIGINT) as HOUSEHOLD_ID\n",
        ", DATE(t.TRANSACTION_TS) as TXN_DTE\n",
        ", SUM(p.POINTS_EARNED_NBR) as BONUS_POINTS_EARNED\n",
        "from gcp-abs-udco-bqvw-prod-prj-01.udco_ds_loyl.EPE_TRANSACTION_HEADER as t\n",
        "JOIN gcp-abs-udco-bqvw-prod-prj-01.udco_ds_loyl.EPE_TRANSACTION_HEADER_SAVING_POINTS as p\n",
        "ON t.TRANSACTION_INTEGRATION_ID = p.TRANSACTION_INTEGRATION_ID\n",
        "where 1=1\n",
        "and OFFER_ID not in (44646442,64035934,90120515) --- Base points offer ID that should be excluded to get to Bonus points only\n",
        "and DATE(TRANSACTION_TS) BETWEEN '{EXP_START_DATE}' AND '{EXP_END_DATE}'\n",
        "and p.dw_current_version_ind = True\n",
        "and p.dw_logical_delete_ind = False\n",
        "GROUP BY all\n",
        "\"\"\"\n",
        "# print(bp_query)\n",
        "bp_sp = bc.read_gcp_table(bp_query)\n",
        "bp_sp.cache()\n",
        "bp_sp.createOrReplaceTempView(\"bonus_points_temp\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Bonus Points Aggregation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bp_agg_query = f\"\"\"\n",
        "WITH\n",
        "BONUS_POINTS_AGG AS(\n",
        "SELECT e.*\n",
        ", COALESCE(SUM(BONUS_POINTS_EARNED),0) as BONUS_POINTS\n",
        ", .015*COALESCE(SUM(BONUS_POINTS_EARNED),0) as ESTIMATED_MKDN\n",
        "FROM {exposure_table} as e\n",
        "LEFT JOIN bonus_points_temp as p\n",
        "on e.HOUSEHOLD_ID = p.household_id\n",
        "and e.exposure_datetime <= p.TXN_DTE\n",
        "group by all\n",
        ")\n",
        "\n",
        "SELECT VARIANT_ID\n",
        ", COUNT(DISTINCT CASE WHEN BONUS_POINTS > 0 THEN HOUSEHOLD_ID ELSE NULL END) as BP_EARNING_HOUSEHOLDS\n",
        ", SUM(BONUS_POINTS) as BONUS_POINTS_TOTAL\n",
        ", AVG(BONUS_POINTS) as BONUS_POINTS_MEAN\n",
        ", STDDEV(BONUS_POINTS) as BONUS_POINTS_SD\n",
        ", SUM(ESTIMATED_MKDN) as ESTIMATED_MKDN_TOTAL\n",
        ", AVG(ESTIMATED_MKDN) as ESTIMATED_MKDN_MEAN\n",
        ", STDDEV(ESTIMATED_MKDN) as ESTIMATED_MKDN_SD\n",
        "FROM BONUS_POINTS_AGG\n",
        "group by all\n",
        "\"\"\"\n",
        "bp_agg_sp = spark.sql(bp_agg_query)\n",
        "redemptions_sp = redemptions_sp.join(bp_agg_sp, on='VARIANT_ID', how='left')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Clips Aggregation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#======================================================================================================================================\n",
        "# Super Query\n",
        "#======================================================================================================================================\n",
        "\n",
        "# This query for aggregating clip data is well-structured and logically correct for its purpose. However, just like the previous metric queries (margin, agp, redemptions, etc.), it represents another separate, resource-intensive job that can be consolidated for much better performance.\n",
        "# Analysis\n",
        "# * Correct Logic: The query correctly joins exposed households with the {clips_table} and uses a two-step aggregation to calculate per-household and then per-variant clip metrics.\n",
        "# * The Inefficiency: The core inefficiency is that this is a standalone query. It performs its own scan of the {exposure_table} and executes its own expensive, non-equi join (e.EXPOSURE_DATETIME <= t.clip_ts).\n",
        "# In the context of your entire notebook, this is another piece of work that is largely redundant.\n",
        "# The Solution: Integrate into the Holistic \"Super-Query\"\n",
        "# This is the final key metric that should be merged into the single, unified query we've been designing. By doing this, you avoid running yet another separate, expensive Spark job.\n",
        "# How it would fit:\n",
        "# The clips_table would be the last data source joined in the main household-level aggregation step of your unified query.\n",
        "# -- Conceptual \"Super-Query\" that now includes Clips\n",
        "# WITH\n",
        "# -- 1. Prepare transaction/redemption details (as discussed before)\n",
        "# ALL_TXN_DETAILS AS ( ... ),\n",
        "# -- 2. Your cached bonus points table is ready (bonus_points_temp)\n",
        "# -- 3. The main household aggregation joins to ALL data sources ONCE\n",
        "# HOUSEHOLD_SUPER_AGG AS (\n",
        "# SELECT\n",
        "# e.HOUSEHOLD_ID, e.VARIANT_ID,\n",
        "# -- Transaction, Redemption, Markdown, and Bonus Points Metrics...\n",
        "# -- ... (from previous examples) ...\n",
        "# -- Clips Metrics (from THIS query, now integrated)\n",
        "# COALESCE(COUNT(DISTINCT CASE WHEN c.offer_type_mod = 'SC' THEN c.CLIENT_OFFER_ID || c.clip_ts ELSE NULL END),0) as sc_clips,\n",
        "# COALESCE(COUNT(DISTINCT CASE WHEN c.offer_type_mod = 'GR' THEN c.CLIENT_OFFER_ID || c.clip_ts ELSE NULL END),0) as gr_clips,\n",
        "# -- ... and so on for all clip types ...\n",
        "# COALESCE(COUNT(DISTINCT c.CLIENT_OFFER_ID || c.clip_ts),0) as total_clips\n",
        "# FROM EXPOSURE_BASE e\n",
        "# -- Join to transaction/redemption details\n",
        "# LEFT JOIN ALL_TXN_DETAILS d\n",
        "# ON e.HOUSEHOLD_ID = d.HOUSEHOLD_ID AND DATE(e.EXPOSURE_DATETIME) <= d.TXN_DTE\n",
        "# -- Join to bonus points details\n",
        "# LEFT JOIN bonus_points_temp bp\n",
        "# ON e.HOUSEHOLD_ID = bp.household_id AND e.exposure_datetime <= bp.TXN_DTE\n",
        "# -- Join to clips details\n",
        "# LEFT JOIN {clips_table} c\n",
        "# ON e.HOUSEHOLD_ID = c.HOUSEHOLD_ID AND e.EXPOSURE_DATETIME <= c.clip_ts\n",
        "# GROUP BY e.HOUSEHOLD_ID, e.VARIANT_ID\n",
        "# )\n",
        "# -- 4. The final SELECT would aggregate all metrics to the variant level\n",
        "# SELECT ... FROM HOUSEHOLD_SUPER_AGG GROUP BY VARIANT_ID\n",
        "# Conclusion\n",
        "# This query should be removed as a standalone step.\n",
        "# By integrating its logic into a single, unified query, you allow Spark to create one highly optimized execution plan. It will scan the large exposure table only once and intelligently join all the different metric tables (transactions, redemptions, bonus_points, clips) in the most efficient way possible. This consolidation is the key to drastically reducing your notebook's overall runtime.\n",
        "\n",
        "\n",
        "clips_agg = (\n",
        "f\"\"\"\n",
        "WITH\n",
        "EXPOSURE_BASE AS(\n",
        "SELECT * FROM {exposure_table}\n",
        "),\n",
        "\n",
        "-- Filter Clips by Exposure Datetime\n",
        "EXPOSURE_CLIPS AS(\n",
        "SELECT e.*,\n",
        "-- CLIPS\n",
        "COALESCE(COUNT(DISTINCT CASE WHEN offer_type_mod = 'SC' THEN (e.HOUSEHOLD_ID || CLIENT_OFFER_ID || t.clip_ts) ELSE NULL END),0) as sc_clips\n",
        ", COALESCE(COUNT(DISTINCT CASE WHEN offer_type_mod = 'GR' THEN (e.HOUSEHOLD_ID || CLIENT_OFFER_ID || t.clip_ts) ELSE NULL END),0) as gr_clips\n",
        ", COALESCE(COUNT(DISTINCT CASE WHEN offer_type_mod = 'MF' THEN (e.HOUSEHOLD_ID || CLIENT_OFFER_ID || t.clip_ts) ELSE NULL END),0) as mf_clips\n",
        ", COALESCE(COUNT(DISTINCT CASE WHEN offer_type_mod = 'PD' THEN (e.HOUSEHOLD_ID || CLIENT_OFFER_ID || t.clip_ts) ELSE NULL END),0) as pd_clips\n",
        ", COALESCE(COUNT(DISTINCT CASE WHEN offer_type_mod = 'SPD' THEN (e.HOUSEHOLD_ID || CLIENT_OFFER_ID || t.clip_ts) ELSE NULL END),0) as spd_clips\n",
        ", COALESCE(COUNT(DISTINCT CASE WHEN offer_type_mod = 'PZN' THEN (e.HOUSEHOLD_ID || CLIENT_OFFER_ID || t.clip_ts) ELSE NULL END),0) as pzn_clips\n",
        ", COALESCE(COUNT(DISTINCT (e.HOUSEHOLD_ID || CLIENT_OFFER_ID || t.clip_ts)),0) as total_clips\n",
        "FROM EXPOSURE_BASE as e\n",
        "LEFT JOIN {clips_table} as t\n",
        "ON e.HOUSEHOLD_ID = t.HOUSEHOLD_ID\n",
        "AND (e.EXPOSURE_DATETIME <= t.clip_ts) AND (t.clip_ts <= '{EXP_END_DATE}')\n",
        "GROUP BY all\n",
        ")\n",
        "\n",
        "SELECT VARIANT_ID,\n",
        "COUNT(DISTINCT {visitor_unit}) AS VISITORS\n",
        "--- UNIQUE_CLIPPING_HH\n",
        ", COUNT(DISTINCT IFF(total_clips > 0, HOUSEHOLD_ID, NULL)) AS UNIQUE_CLIPPING_HH\n",
        ", AVG(IFF(total_clips > 0, total_clips, NULL)) AS CLIPS_PER_HH_NON_ZERO_MEAN\n",
        ", STDDEV(IFF(total_clips > 0, total_clips, NULL)) AS CLIPS_PER_HH_NON_ZERO_SD\n",
        "--- CLIPS TOTAL\n",
        ", SUM(total_clips) as CLIPS_TOTAL\n",
        ", SUM(total_clips) / COUNT(DISTINCT HOUSEHOLD_ID) as CLIPS_AVG_REAL\n",
        ", AVG(total_clips) as CLIPS_MEAN\n",
        ", STDDEV(total_clips) as CLIPS_SD\n",
        "--- CLIPS BREAKDOWN\n",
        ", SUM(pd_clips) AS pd_clips_TOTAL\n",
        ", SUM(gr_clips) AS gr_clips_TOTAL\n",
        ", SUM(mf_clips) AS mf_clips_TOTAL\n",
        ", SUM(spd_clips) AS spd_clips_TOTAL\n",
        ", SUM(pzn_clips) AS pzn_clips_TOTAL\n",
        ", SUM(sc_clips) AS sc_clips_TOTAL\n",
        ", AVG(pd_clips) AS pd_clips_MEAN\n",
        ", AVG(gr_clips) AS gr_clips_MEAN\n",
        ", AVG(mf_clips) AS mf_clips_MEAN\n",
        ", AVG(spd_clips) AS spd_clips_MEAN\n",
        ", AVG(pzn_clips) AS pzn_clips_MEAN\n",
        ", AVG(sc_clips) AS sc_clips_MEAN\n",
        ", STDDEV(pd_clips) AS pd_clips_SD\n",
        ", STDDEV(gr_clips) AS gr_clips_SD\n",
        ", STDDEV(mf_clips) AS mf_clips_SD\n",
        ", STDDEV(spd_clips) AS spd_clips_SD\n",
        ", STDDEV(pzn_clips) AS pzn_clips_SD\n",
        ", STDDEV(sc_clips) AS sc_clips_SD\n",
        "FROM EXPOSURE_CLIPS\n",
        "GROUP BY all\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "clips_agg_sp = spark.sql(clips_agg)\n",
        "#clips_agg_sp.display()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Basket Health Aggregation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#======================================================================================================================================\n",
        "# Super Query\n",
        "#======================================================================================================================================\n",
        "# You've reached the last of the major metric aggregations, and it follows the exact same pattern as the others.\n",
        "# This query is logically correct for what it's calculating. However, in the context of your full notebook, it's another standalone, expensive operation that should be merged into the single, holistic \"super-query\" we've been designing.\n",
        "# Analysis\n",
        "# * The Bottleneck: Just like the margin and agp queries, the performance of this query is dominated by the non-equi join condition: AND bh.TXN_DTE BETWEEN DATE(e.EXPOSURE_DATETIME) AND '{EXP_END_DATE}'.\n",
        "# * The Inefficiency: Running this as a separate step means you are scanning your large {exposure_table} and performing another one of these expensive joins, which is work that can be consolidated.\n",
        "# The Final Piece of the Holistic \"Super-Query\"\n",
        "# This is the last piece of the puzzle. The {basket_health_table} is just one more data source that can be incorporated into your single, unified query.\n",
        "# * Prepare Detailed Data: The ALL_TXN_DETAILS CTE in your super-query would perform one more LEFT JOIN to bring in the OVERALL_CATEGORY for each transaction.\n",
        "# -- Inside the unified ALL_TXN_DETAILS CTE...\n",
        "# SELECT\n",
        "# t.*,\n",
        "# r.CLIENT_OFFER_ID, r.OFFER_TYPE_MOD, r.MKDN,\n",
        "# bh.OVERALL_CATEGORY -- Add basket health category\n",
        "# FROM {combined_txn_table} t\n",
        "# LEFT JOIN {redemptions_table} r ON t.TXN_ID = r.TXN_ID\n",
        "# LEFT JOIN {basket_health_table} bh ON t.TXN_ID = bh.TXN_ID -- Or however it joins\n",
        "# * Single Household-Level Aggregation: Your HOUSEHOLD_SUPER_AGG CTE would then calculate the basket health rates alongside all the other metrics.\n",
        "# -- Inside the unified HOUSEHOLD_SUPER_AGG CTE...\n",
        "# SELECT\n",
        "# e.HOUSEHOLD_ID, e.VARIANT_ID,\n",
        "# -- Transaction, Redemption, Clips, Bonus Points metrics...\n",
        "# -- ...\n",
        "# -- Basket Health metrics (from THIS query, now integrated)\n",
        "# COALESCE(COUNT(DISTINCT CASE WHEN d.OVERALL_CATEGORY = 'A - PERFECT' THEN d.TXN_ID ELSE NULL END)/COUNT(DISTINCT d.TXN_ID), 0) AS Perfect_rate,\n",
        "# -- ... and so on for all the other rates ...\n",
        "# COUNT(DISTINCT d.TXN_ID) AS TOTAL_TXNS\n",
        "# FROM EXPOSURE_BASE e\n",
        "# LEFT JOIN ALL_TXN_DETAILS d ON e.HOUSEHOLD_ID = d.HOUSEHOLD_ID AND DATE(e.EXPOSURE_DATETIME) <= d.TXN_DTE\n",
        "# -- ... other joins to bonus points, clips, etc. if they are separate ...\n",
        "# GROUP BY e.HOUSEHOLD_ID, e.VARIANT_ID\n",
        "# Final Recommendation\n",
        "# This basket_health_agg query should be removed as a standalone step.\n",
        "# By integrating this final piece of logic, you will have successfully consolidated what was previously 6-7 separate, large, and inefficient queries into a single, highly-optimized \"super-query\".\n",
        "# This unified approach is the key to drastically reducing your notebook's 15-minute runtime. It eliminates nearly all the redundant data scanning and repeated complex joins, allowing Spark to process all your metrics in one streamlined and efficient pass.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "basket_health_agg = (\n",
        "f\"\"\"\n",
        "WITH\n",
        "EXPOSURE_BASE AS(\n",
        "SELECT * FROM {exposure_table}\n",
        "),\n",
        "\n",
        "-- Filter Basket Health by Exposure Datetime\n",
        "BASKET_HEALTH_EXPOSED AS(\n",
        "SELECT e.*,\n",
        "COALESCE(COUNT(DISTINCT CASE WHEN bh.OVERALL_CATEGORY = 'A - PERFECT' THEN TXN_ID ELSE NULL END)/COUNT(DISTINCT TXN_ID), 0) AS Perfect_rate,\n",
        "COALESCE(COUNT(DISTINCT CASE WHEN bh.OVERALL_CATEGORY = 'B - GREAT' THEN TXN_ID ELSE NULL END)/COUNT(DISTINCT TXN_ID), 0) AS great_rate,\n",
        "COALESCE(COUNT(DISTINCT CASE WHEN bh.OVERALL_CATEGORY = 'C - ACCEPTABLE' THEN TXN_ID ELSE NULL END)/COUNT(DISTINCT TXN_ID), 0) AS acceptable_rate,\n",
        "COALESCE(COUNT(DISTINCT CASE WHEN bh.OVERALL_CATEGORY = 'D - NEEDS_IMPROVEMENT' THEN TXN_ID ELSE NULL END)/COUNT(DISTINCT TXN_ID), 0) AS needs_improvement_rate,\n",
        "COALESCE(COUNT(DISTINCT CASE WHEN bh.OVERALL_CATEGORY = 'E - POOR' THEN TXN_ID ELSE NULL END)/COUNT(DISTINCT TXN_ID), 0) AS poor_rate,\n",
        "-- Net_Basket_rate\n",
        "COALESCE(COUNT(DISTINCT CASE WHEN (bh.OVERALL_CATEGORY = 'A - PERFECT' OR bh.OVERALL_CATEGORY = 'B - GREAT') THEN TXN_ID ELSE NULL END)/COUNT(DISTINCT TXN_ID), 0) -\n",
        "COALESCE(COUNT(DISTINCT CASE WHEN (bh.OVERALL_CATEGORY = 'D - NEEDS_IMPROVEMENT' OR bh.OVERALL_CATEGORY = 'E - POOR') THEN TXN_ID ELSE NULL END)/COUNT(DISTINCT TXN_ID), 0) AS Net_Basket_rate,\n",
        "-- AB_RATE\n",
        "COALESCE(COUNT(DISTINCT CASE WHEN (bh.OVERALL_CATEGORY = 'A - PERFECT' OR bh.OVERALL_CATEGORY = 'B - GREAT') THEN TXN_ID ELSE NULL END)/COUNT(DISTINCT TXN_ID), 0) AS AB_RATE,\n",
        "-- DE_RATE\n",
        "COALESCE(COUNT(DISTINCT CASE WHEN (bh.OVERALL_CATEGORY = 'D - NEEDS_IMPROVEMENT' OR bh.OVERALL_CATEGORY = 'E - POOR') THEN TXN_ID ELSE NULL END)/COUNT(DISTINCT TXN_ID), 0) AS DE_RATE,\n",
        "COUNT(DISTINCT TXN_ID) AS TOTAL_TXNS\n",
        "FROM EXPOSURE_BASE AS e\n",
        "LEFT JOIN {basket_health_table} as bh\n",
        "ON e.HOUSEHOLD_ID = bh.HOUSEHOLD_ID\n",
        "AND bh.TXN_DTE BETWEEN DATE(e.EXPOSURE_DATETIME) AND '{EXP_END_DATE}'\n",
        "GROUP BY ALL\n",
        ")\n",
        "\n",
        "SELECT VARIANT_ID,\n",
        "COUNT(DISTINCT {visitor_unit}) AS VISITORS\n",
        ", COUNT(DISTINCT HOUSEHOLD_ID) AS UNIQUE_BASKET_HEALTH_HH\n",
        ", SUM(NET_BASKET_RATE) as BASKET_RATE_TOTAL\n",
        ", AVG(NET_BASKET_RATE) as BASKET_RATE_MEAN\n",
        ", STDDEV(NET_BASKET_RATE) as BASKET_RATE_SD\n",
        ", SUM(AB_RATE) as AB_RATE_TOTAL\n",
        ", AVG(AB_RATE) as AB_RATE_MEAN\n",
        ", STDDEV(AB_RATE) as AB_RATE_SD\n",
        ", SUM(DE_RATE) as DE_RATE_TOTAL\n",
        ", AVG(DE_RATE) as DE_RATE_MEAN\n",
        ", STDDEV(DE_RATE) as DE_RATE_SD\n",
        ", SUM(TOTAL_TXNS) as TOTAL_BASKET_HEALTH_TXNS\n",
        "FROM BASKET_HEALTH_EXPOSED\n",
        "GROUP BY ALL\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "basket_health_agg_sp = spark.sql(basket_health_agg)\n",
        "#clips_agg_sp.display()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Gas Aggregation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#======================================================================================================================================\n",
        "# Super Query\n",
        "#======================================================================================================================================\n",
        "\n",
        "# This final query for gas transactions follows the exact same pattern as all the previous metric aggregations. It is logically correct, but just like the others, it represents a standalone, expensive operation that should be consolidated for better performance.\n",
        "# Analysis\n",
        "# * The Bottleneck: The query's performance hinges on the LEFT JOIN with the non-equi (inequality) condition: e.EXPOSURE_DATETIME <= s.TXN_DTE.\n",
        "# * The Inefficiency: In the context of your notebook, this is the last of many separate queries that independently scan the large {exposure_table} and perform a costly join.\n",
        "# The Final Step in the Holistic Optimization\n",
        "# This is the last component to be merged into the single \"super-query.\" By integrating this logic, you complete the consolidation of all your major metric calculations.\n",
        "# The {gas_table} would be the final data source joined at the household-level aggregation step of your unified query.\n",
        "# Final Summary: The Path to a High-Performance Notebook\n",
        "# Let's recap the optimization journey for your entire notebook.\n",
        "# The Original Problem:\n",
        "# Your notebook had a series of ~7-8 large, separate SQL queries, each calculating a different metric (E-comm, In-Store, Redemptions, Margin, AGP, Clips, Bonus Points, Gas). Each query would:\n",
        "# * Read the large, exposed-user table.\n",
        "# * Read one or more large metric tables (transactions, clips, etc.).\n",
        "# * Perform a complex and computationally expensive join.\n",
        "# This resulted in a huge amount of redundant work, leading to the 15+ minute runtime.\n",
        "# The Solution: A Single, Unified \"Super-Query\"\n",
        "# The most impactful optimization is to replace all of those separate queries with a single, well-structured query that does everything at once.\n",
        "# The structure looks like this:\n",
        "# * Prepare Data (CTE): Create a single, detailed transaction table by joining {combined_txn_table} with {redemptions_table}, {basket_health_table}, etc. Also, prepare your cached bonus_points table.\n",
        "# * Aggregate at Household Level (CTE): Perform one join from your {exposure_table} to all the prepared data sources (transactions, clips, gas, bonus_points). In this single step, use conditional aggregation (SUM(CASE WHEN...)) to calculate every single per-household metric:\n",
        "# * ECOMM_REVENUE\n",
        "# * STORE_REVENUE\n",
        "# * COMBINED_REVENUE\n",
        "# * PD_REDEMPTIONS\n",
        "# * TOTAL_CLIPS\n",
        "# * BASKET_HEALTH_RATES\n",
        "# * GAS_ORDERS\n",
        "# * ...and so on for every metric.\n",
        "# * Aggregate at Variant Level (Final SELECT): Perform the final GROUP BY VARIANT_ID on the comprehensive household-level table to get your final summary statistics.\n",
        "# By adopting this holistic approach, you will achieve:\n",
        "# * Massive Efficiency Gains: You will scan your largest tables only once.\n",
        "# * Reduced Computation: You will perform the expensive date-based joins only once.\n",
        "# * Simplicity and Maintainability: Your logic will be consolidated into one place, making it easier to manage and debug.\n",
        "# This consolidation is the key to transforming your notebook from a 15-minute run to a much faster and more efficient process.\n",
        "\n",
        "gas_txns_agg = (\n",
        "f\"\"\"\n",
        "WITH\n",
        "EXPOSURE_BASE AS(\n",
        "SELECT * FROM {exposure_table}\n",
        "),\n",
        "\n",
        "GAS_BASE AS(\n",
        "SELECT e.*\n",
        ", COALESCE(COUNT(DISTINCT s.TXN_ID),0) AS NUM_ORDERS\n",
        ", COALESCE(SUM(s.REVENUE),0) AS TOT_REVENUE\n",
        ", COALESCE(SUM(s.GAS_REWARD_REDEMPTIONS),0) as GAS_REWARD_REDEMPTIONS\n",
        ", -1*COALESCE(SUM(s.GAS_MKDN),0) as GAS_MKDN\n",
        "FROM EXPOSURE_BASE as e\n",
        "LEFT JOIN {gas_table} as s\n",
        "ON e.HOUSEHOLD_ID = s.HOUSEHOLD_ID\n",
        "AND e.EXPOSURE_DATETIME <= s.TXN_DTE\n",
        "AND (s.TXN_DTE >= '{EXP_START_DATE}' AND s.TXN_DTE <= '{EXP_END_DATE}')\n",
        "GROUP BY all\n",
        ")\n",
        "\n",
        "SELECT VARIANT_ID AS VARIANT_ID_GAS\n",
        ", COUNT(DISTINCT {visitor_unit}) AS VISITORS\n",
        ", COUNT(DISTINCT IFF(NUM_ORDERS > 0, HOUSEHOLD_ID, NULL)) AS GAS_VISITORS\n",
        "-- ORDERS\n",
        ", SUM(NUM_ORDERS) AS GAS_ORDERS_TOTAL\n",
        ", AVG(NUM_ORDERS) AS GAS_ORDERS_MEAN\n",
        ", STDDEV(NUM_ORDERS) AS GAS_ORDERS_SD\n",
        "--- GAS REVENUE\n",
        ", SUM(TOT_REVENUE) AS GAS_REVENUE_TOTAL\n",
        ", AVG(TOT_REVENUE) AS GAS_REVENUE_MEAN\n",
        ", STDDEV(TOT_REVENUE) AS GAS_REVENUE_SD\n",
        "--- NON ZERO REVENUE\n",
        ",AVG(CASE WHEN TOT_REVENUE = 0 THEN NULL ELSE TOT_REVENUE END) AS GAS_REVENUE_NONZERO_MEAN\n",
        ",STDDEV(CASE WHEN TOT_REVENUE = 0 THEN NULL ELSE TOT_REVENUE END) AS GAS_REVENUE_NONZERO_SD\n",
        "--- GAS_REWARD_REDEMPTIONS\n",
        ", SUM(GAS_REWARD_REDEMPTIONS) AS GAS_REWARD_REDEMPTIONS_TOTAL\n",
        ", AVG(GAS_REWARD_REDEMPTIONS) AS GAS_REWARD_REDEMPTIONS_MEAN\n",
        ", STDDEV(GAS_REWARD_REDEMPTIONS) AS GAS_REWARD_REDEMPTIONS_SD\n",
        "--- GAS_MKDN\n",
        ", SUM(GAS_MKDN) AS GAS_MKDN_TOTAL\n",
        ", AVG(GAS_MKDN) AS GAS_MKDN_MEAN\n",
        ", STDDEV(GAS_MKDN) AS GAS_MKDN_SD\n",
        "--- Ratio Metrics\n",
        ", AVG(IFF(TOT_REVENUE > 0, TOT_REVENUE, NULL)) / AVG(IFF(NUM_ORDERS > 0, NUM_ORDERS, NULL)) AS GAS_AOV\n",
        ", COVAR_SAMP(TOT_REVENUE,NUM_ORDERS) AS GAS_COV_REVENUE_ORDERS\n",
        ", SUM(TOT_REVENUE) / COUNT(DISTINCT HOUSEHOLD_ID) AS GAS_RPV\n",
        ", SUM(CASE WHEN TOT_REVENUE = 0 THEN NULL ELSE TOT_REVENUE END) / COUNT(DISTINCT HOUSEHOLD_ID) AS GAS_NONZERO_RPV\n",
        ", SUM(GAS_REWARD_REDEMPTIONS) / COUNT(DISTINCT HOUSEHOLD_ID) AS GAS_REDEMPTIONS_PER_VISITOR\n",
        "FROM GAS_BASE\n",
        "GROUP BY all\n",
        "\"\"\")\n",
        "\n",
        "gas_agg_sp = spark.sql(gas_txns_agg)\n",
        "# gas_agg_sp.display()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Category Aggregation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "#======================================================================================================================================\n",
        "# Super Query\n",
        "#======================================================================================================================================\n",
        "\n",
        "# This is the last of the metric aggregation queries, and it follows the same principles we've discussed.\n",
        "# Analysis\n",
        "# * Correct Logic: The query is logically sound. It correctly joins the exposed households with the category-level transaction data and performs a two-step aggregation to calculate category breadth and depth.\n",
        "# * Efficient Join (in isolation): It's worth noting that the join condition here (t.TXN_DTE BETWEEN '{EXP_START_DATE}' AND '{EXP_END_DATE}') is much simpler and more efficient than the non-equi joins in the previous queries. It's a standard filter that the database can handle well.\n",
        "# * The Inefficiency (in context): The main inefficiency is the same as before: this is a separate, standalone query. It re-scans the {exposure_table} and performs work that could be consolidated with the other queries.\n",
        "# The Solution: Integrate into the Holistic \"Super-Query\"\n",
        "# This is the final component to merge into your single, unified query. The {category_table} is just another transaction-level detail that can be joined once and aggregated alongside everything else.\n",
        "# How it would fit:\n",
        "# * Prepare Detailed Data: Your ALL_TXN_DETAILS CTE would perform one last LEFT JOIN to the {category_table} to bring in smic_category_id and ITEM_QTY.\n",
        "# * Single Household-Level Aggregation: Your HOUSEHOLD_SUPER_AGG CTE would then add the category breadth and item counts to its list of calculations.\n",
        "# -- Inside the unified HOUSEHOLD_SUPER_AGG CTE...\n",
        "# SELECT\n",
        "# e.HOUSEHOLD_ID, e.VARIANT_ID,\n",
        "# -- Transaction, Redemption, Clips, Gas, Bonus Points metrics...\n",
        "# -- ... (from previous examples) ...\n",
        "# -- Category Metrics (from THIS query, now integrated)\n",
        "# COALESCE(COUNT(DISTINCT d.smic_category_id), 0) AS NUM_CATEGORIES,\n",
        "# COALESCE(SUM(d.ITEM_QTY), 0) AS ITEMS\n",
        "# FROM EXPOSURE_BASE e\n",
        "# LEFT JOIN ALL_TXN_DETAILS d ON e.HOUSEHOLD_ID = d.HOUSEHOLD_ID AND ... -- The single, complex join\n",
        "# GROUP BY e.HOUSEHOLD_ID, e.VARIANT_ID\n",
        "# * Final Variant-Level Aggregation: The final SELECT of your super-query would then calculate the AVG(NUM_CATEGORIES) (Category Breadth) and the ratio for CATEGORY_DEPTH.\n",
        "# Final Conclusion on Optimization\n",
        "# You have successfully identified the core pattern of inefficiency in your notebook. By replacing the ~8 separate, resource-intensive metric aggregation queries (ecomm, store, combined, redemptions, bonus_points, clips, basket_health, category) with a single, well-structured, holistic query, you will achieve a dramatic improvement in performance.\n",
        "# This consolidation is the key to reducing your notebook's runtime from 15+ minutes to a much more manageable duration.\n",
        "\n",
        "\n",
        "category_agg_query = (f\"\"\"\n",
        "WITH\n",
        "EXPOSURE_BASE AS(\n",
        "SELECT * FROM {exposure_table}\n",
        "),\n",
        "\n",
        "EXPOSURE_TXNS AS(\n",
        "SELECT e.*\n",
        ", COALESCE(COUNT(DISTINCT smic_category_id), 0) AS NUM_CATEGORIES\n",
        ", COALESCE(SUM(ITEM_QTY),0) AS ITEMS\n",
        "FROM EXPOSURE_BASE as e\n",
        "LEFT JOIN {category_table} as t\n",
        "ON e.HOUSEHOLD_ID = t.HOUSEHOLD_ID\n",
        "AND t.TXN_DTE BETWEEN '{EXP_START_DATE}' AND '{EXP_END_DATE}'\n",
        "GROUP BY ALL\n",
        ")\n",
        "\n",
        "SELECT VARIANT_ID\n",
        ", COUNT(DISTINCT {visitor_unit}) as VISITORS\n",
        ", COUNT(DISTINCT HOUSEHOLD_ID) as UNIQUE_HOUSEHOLDS\n",
        "-- Category Breadth\n",
        ", SUM(NUM_CATEGORIES) AS CATEGORIES_TOTAL\n",
        ", AVG(NUM_CATEGORIES) AS CATEGORIES_MEAN -- Category Breadth\n",
        ", STDDEV(NUM_CATEGORIES) AS CATEGORIES_SD\n",
        "-- Items Per Category\n",
        ", SUM(ITEMS) AS ITEMS_TOTAL\n",
        ", AVG(ITEMS) AS ITEMS_MEAN\n",
        ", STDDEV(ITEMS) AS ITEMS_SD\n",
        "--- Category Depth --- Similar to AOV\n",
        ", AVG(IFF(ITEMS > 0, ITEMS, NULL)) / AVG(IFF(NUM_CATEGORIES > 0,\n",
        "NUM_CATEGORIES, NULL)) AS CATEGORY_DEPTH\n",
        ", COVAR_SAMP(ITEMS, NUM_CATEGORIES) AS CD_COV_ITEMS_CATEGORIES\n",
        "FROM EXPOSURE_TXNS\n",
        "GROUP BY ALL\n",
        "\"\"\")\n",
        "\n",
        "category_agg_sp = spark.sql(category_agg_query)\n",
        "#display(category_agg_df)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Account Health Aggregation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#======================================================================================================================================\n",
        "# Super Query\n",
        "#======================================================================================================================================\n",
        "\n",
        "# This query for account health is simple, correct, and already very efficient on its own.\n",
        "# Analysis\n",
        "# This query stands out from the previous metric aggregations for one key reason:\n",
        "# * It uses a simple, fast join. The LEFT JOIN on eh.HOUSEHOLD_ID = ah.household_id is a standard equi-join. It does not have the complex and computationally expensive date-based inequality conditions (BETWEEN or <=) that were the main performance bottlenecks in all the other queries. Spark handles this type of join extremely well.\n",
        "# Should You Still Consolidate It?\n",
        "# Even though this query is fast by itself, the principle of creating a single, unified \"super-query\" still applies for two reasons:\n",
        "# * Efficiency: It still runs as a separate Spark job that scans the {exposure_table}. Merging it into your main query eliminates this last piece of redundant work.\n",
        "# * Maintainability: Having one query to generate all experiment metrics makes your notebook cleaner and easier to manage in the long run.\n",
        "# How it would fit:\n",
        "# Since account health is an attribute of the household, not the transaction, you would join the {account_health_table} at the beginning of your holistic query.\n",
        "# -- Conceptual \"Super-Query\" that now includes Account Health\n",
        "# WITH\n",
        "# -- 1. Redefine EXPOSURE_BASE to include account health attributes from the start\n",
        "# EXPOSURE_BASE AS (\n",
        "# SELECT\n",
        "# e.*,\n",
        "# ah.email_ind,\n",
        "# ah.phone_ind,\n",
        "# ah.fn_ln_ind,\n",
        "# ah.bday_ind,\n",
        "# ah.address_ind\n",
        "# FROM {exposure_table} e\n",
        "# LEFT JOIN {account_health_table} ah ON e.HOUSEHOLD_ID = ah.household_id\n",
        "# ),\n",
        "# -- 2. Prepare transaction details (as discussed before)\n",
        "# ALL_TXN_DETAILS AS ( ... ),\n",
        "# -- 3. The main household aggregation joins to transaction-level data\n",
        "# HOUSEHOLD_SUPER_AGG AS (\n",
        "# SELECT\n",
        "# -- The account health indicators are already here from EXPOSURE_BASE\n",
        "# e.HOUSEHOLD_ID, e.VARIANT_ID, e.email_ind, e.phone_ind, ...\n",
        "# -- All transaction, redemption, clips, etc. metrics\n",
        "# -- ...\n",
        "# FROM EXPOSURE_BASE e\n",
        "# LEFT JOIN ALL_TXN_DETAILS d ON ...\n",
        "# GROUP BY e.HOUSEHOLD_ID, e.VARIANT_ID, e.email_ind, e.phone_ind, ...\n",
        "# )\n",
        "# -- 4. The final SELECT would aggregate all metrics to the variant level\n",
        "# SELECT\n",
        "# VARIANT_ID,\n",
        "# -- ...\n",
        "# SUM(email_ind) AS email_count,\n",
        "# SUM(phone_ind) AS phone_count\n",
        "# -- ...\n",
        "# FROM HOUSEHOLD_SUPER_AGG\n",
        "# GROUP BY VARIANT_ID\n",
        "# Final Conclusion\n",
        "# While this specific query is not a major performance problem, integrating its logic into your single, holistic query is the final step to creating a truly streamlined and efficient notebook. This consolidation will reduce the number of Spark jobs to a minimum and make your code much easier to maintain.\n",
        "\n",
        "\n",
        "account_health_agg_query = (f\"\"\"\n",
        "WITH\n",
        "EXPOSURE_BASE AS(\n",
        "SELECT * FROM {exposure_table}\n",
        "),\n",
        "\n",
        "EXPOSURE_ACC_HEALTH AS (\n",
        "SELECT\n",
        "eh.*,\n",
        "ah.email_ind,\n",
        "ah.phone_ind,\n",
        "ah.fn_ln_ind,\n",
        "ah.bday_ind,\n",
        "ah.address_ind\n",
        "FROM EXPOSURE_BASE eh\n",
        "LEFT JOIN {account_health_table} ah\n",
        "ON eh.HOUSEHOLD_ID = ah.household_id\n",
        ")\n",
        "\n",
        "SELECT\n",
        "eah.VARIANT_ID,\n",
        "COUNT(DISTINCT {visitor_unit}) as VISITORS,\n",
        "COUNT(DISTINCT eah.HOUSEHOLD_ID) as UNIQUE_HOUSEHOLDS,\n",
        "SUM(eah.email_ind) AS email_count,\n",
        "SUM(eah.phone_ind) AS phone_count,\n",
        "SUM(CASE WHEN eah.email_ind > 0 AND eah.phone_ind > 0 THEN 1 ELSE 0 END) AS reachablity_score,\n",
        "SUM(eah.fn_ln_ind) AS fn_ln_count,\n",
        "SUM(eah.bday_ind) AS bday_count,\n",
        "SUM(eah.address_ind) AS address_count,\n",
        "AVG(eah.email_ind + eah.phone_ind + eah.fn_ln_ind + eah.bday_ind + eah.address_ind) as health_score,\n",
        "STDDEV(eah.email_ind + eah.phone_ind + eah.fn_ln_ind + eah.bday_ind + eah.address_ind) as stdev_health_score\n",
        "FROM EXPOSURE_ACC_HEALTH eah\n",
        "GROUP BY ALL\n",
        "\"\"\")\n",
        "\n",
        "account_health_agg_sp = spark.sql(account_health_agg_query)\n",
        "#display(account_health_agg_df)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Margin Refresh and Fiscal Period Data Checks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Margin Refresh Date\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "margin_refresh_query = (\n",
        "\"\"\"\n",
        "SELECT MAX(DATE(DATA_LOAD_DATE)) AS LAST_LOAD_DATE\n",
        ", MAX(DATE(fw.FISCAL_WEEK_END_DT)) AS REFRESH_DATE\n",
        "FROM gcp-abs-udco-bsvw-prod-prj-01.aamp_ds_datascience.EB_HH_STORE_WKLY_AGP_ALLDIV_VIEW as a\n",
        "LEFT JOIN gcp-abs-udco-bqvw-prod-prj-01.udco_ds_acct.D0_FISCAL_WEEK AS fw\n",
        "ON a.WEEK_ID = fw.FISCAL_WEEK_ID\n",
        "WHERE fw.FISCAL_WEEK_END_DT >= (CURRENT_DATE()-60)\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "# Execute the query\n",
        "margin_refresh = bc.read_gcp_table(margin_refresh_query)\n",
        "margin_refresh_df = margin_refresh.select(\"*\").toPandas()\n",
        "margin_refresh_date = margin_refresh_df['REFRESH_DATE'].iloc[0]\n",
        "margin_load_date = margin_refresh_df['LAST_LOAD_DATE'].iloc[0]\n",
        "print('Margin Valid Through:',margin_refresh_date, '\\nLast Margin Load Date:', margin_load_date)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fiscal Period Query\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fp_query = (\n",
        "\"\"\"\n",
        "SELECT FISCAL_PERIOD_NBR\n",
        ", DATETIME(FISCAL_PERIOD_START_DT) AS FISCAL_PERIOD_START_DT\n",
        ", DATETIME(FISCAL_PERIOD_END_DT) AS FISCAL_PERIOD_END_DT\n",
        "FROM gcp-abs-udco-bqvw-prod-prj-01.udco_ds_acct.D0_FISCAL_WEEK\n",
        "WHERE FISCAL_WEEK_START_DT BETWEEN (CURRENT_DATE()-7) AND CURRENT_DATE()\n",
        "ORDER BY 1\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "# Execute the query\n",
        "current_fp = bc.read_gcp_table(fp_query)\n",
        "current_fp_df = current_fp.select(\"*\").toPandas()\n",
        "\n",
        "try:\n",
        "    if margin_refresh_date > current_fp_df['FISCAL_PERIOD_START_DT'].iloc[0]:\n",
        "        next_margin_refresh_date = str(current_fp_df['FISCAL_PERIOD_END_DT'].iloc[0] + datetime.timedelta(days=14))\n",
        "    else:\n",
        "        next_margin_refresh_date = str(current_fp_df['FISCAL_PERIOD_START_DT'].iloc[0] + datetime.timedelta(days=14))\n",
        "except:\n",
        "    next_margin_refresh_date = \"NO MARGIN DATA AVALIABLE. PLEASE CHECK.\"\n",
        "\n",
        "print('Next Margin Refresh: ',next_margin_refresh_date)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Overall Statistics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Engagement (Click Hit Data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "base_df = agg_daily_sp.select(\"*\").toPandas()\n",
        "base_df = base_df.sort_values('VARIANT_ID')\n",
        "\n",
        "base_df_for_display = base_df[['VARIANT_ID','VISITS_TOTAL','VISITS_MEAN','VISITORS','UNIQUE_HOUSEHOLDS','CART_ADDS_TOTAL','CART_ADDS_MEAN','CART_ADDS_CVR','UNITS_TOTAL','ORDERS_TOTAL','COUPON_CLIPS_TOTAL','COUPON_CLIPS_MEAN','COUPON_CLIP_CVR','SEARCHES_TOTAL','SEARCHES_MEAN','SEARCHES_CVR','REVENUE_TOTAL','AOV','UPO','RPV']].copy()\n",
        "\n",
        "base_df_for_display ['AUTHENTICATED_RATE'] = base_df_for_display['UNIQUE_HOUSEHOLDS']/base_df_for_display['VISITORS']\n",
        "\n",
        "base_df_for_display ['VISITORS'] = ['{:,}'.format(i) for i in base_df_for_display ['VISITORS']]\n",
        "base_df_for_display ['UNIQUE_HOUSEHOLDS'] = ['{:,}'.format(i) for i in base_df_for_display ['UNIQUE_HOUSEHOLDS']]\n",
        "base_df_for_display ['AUTHENTICATED_RATE'] = base_df_for_display ['AUTHENTICATED_RATE'].apply(lambda x: '{:,.4f}%'.format(x*100))\n",
        "base_df_for_display ['VISITS_TOTAL'] = ['{:,}'.format(i) for i in base_df_for_display ['VISITS_TOTAL']]\n",
        "base_df_for_display ['VISITS_PER_CUSTOMER'] = base_df_for_display['VISITS_MEAN'].apply(lambda x: '{:,.4f}'.format(x))\n",
        "base_df_for_display ['CART_ADDS_TOTAL'] = base_df_for_display['CART_ADDS_TOTAL'].apply(lambda x: '{:,.0f}'.format(x))\n",
        "base_df_for_display ['CART_ADDS_PER_CUSTOMER'] = base_df_for_display['CART_ADDS_MEAN'].apply(lambda x: '{:,.4f}'.format(x))\n",
        "base_df_for_display ['CART_ADDS_CVR'] = base_df_for_display['CART_ADDS_CVR'].apply(lambda x: '{:,.4f}%'.format(x*100))\n",
        "base_df_for_display ['UNITS_TOTAL'] = base_df_for_display['UNITS_TOTAL'].apply(lambda x: '{:,}'.format(x))\n",
        "base_df_for_display ['ORDERS_TOTAL'] = base_df_for_display['ORDERS_TOTAL'].apply(lambda x: '{:,}'.format(x))\n",
        "base_df_for_display ['COUPON_CLIPS_TOTAL'] = base_df_for_display['COUPON_CLIPS_TOTAL'].apply(lambda x: '{:,}'.format(x))\n",
        "base_df_for_display ['COUPON_CLIPS_PER_CUSTOMER'] = base_df_for_display['COUPON_CLIPS_MEAN'].apply(lambda x: '{:,.4f}'.format(x))\n",
        "base_df_for_display ['COUPON_CLIPS_CVR'] = base_df_for_display['COUPON_CLIP_CVR'].apply(lambda x: '{:,.4f}%'.format(x*100))\n",
        "base_df_for_display ['SEARCHES_TOTAL'] = ['{:,}'.format(i) for i in base_df_for_display ['SEARCHES_TOTAL']]\n",
        "base_df_for_display ['SEARCHES_PER_CUSTOMER'] = base_df_for_display['SEARCHES_MEAN'].apply(lambda x: '{:,.4f}'.format(x))\n",
        "base_df_for_display ['SEARCHES_CVR'] = base_df_for_display['SEARCHES_CVR'].apply(lambda x: '{:,.4f}%'.format(x*100))\n",
        "base_df_for_display ['REVENUE_TOTAL'] = base_df_for_display['REVENUE_TOTAL'].apply(lambda x: '${:,.0f}'.format(x))\n",
        "base_df_for_display ['AOV'] = base_df_for_display['AOV'].apply(lambda x: '${:,.4f}'.format(x))\n",
        "base_df_for_display ['UPO'] = base_df_for_display['UPO'].apply(lambda x: '{:,.4f}'.format(x))\n",
        "base_df_for_display ['RPV'] = base_df_for_display['RPV'].apply(lambda x: '${:,.4f}'.format(x))\n",
        "\n",
        "display(base_df_for_display[['VARIANT_ID','VISITORS','UNIQUE_HOUSEHOLDS','AUTHENTICATED_RATE','VISITS_TOTAL','CART_ADDS_TOTAL','CART_ADDS_PER_CUSTOMER','CART_ADDS_CVR','COUPON_CLIPS_TOTAL','COUPON_CLIPS_PER_CUSTOMER','COUPON_CLIPS_CVR','SEARCHES_TOTAL','SEARCHES_PER_CUSTOMER','SEARCHES_CVR']])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Margin (Weekly)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "  margin_agg_df = margin_agg_sp.select(\"*\").toPandas()\n",
        "  margin_agg_df = margin_agg_df.sort_values('VARIANT_ID',ascending=True)\n",
        "      \n",
        "  margin_df_for_display = margin_agg_df[['VARIANT_ID','VISITORS','MARGIN_TOTAL','MARGIN_MEAN']].copy()\n",
        " \n",
        "  margin_df_for_display ['CUSTOMERS'] = ['{:,}'.format(i) for i in margin_df_for_display ['VISITORS']]\n",
        "  margin_df_for_display ['MARGIN_TOTAL'] = margin_df_for_display['MARGIN_TOTAL'].apply(lambda x: '${:,.0f}'.format(x))\n",
        "  margin_df_for_display ['MARGIN_PER_CUSTOMER'] = margin_df_for_display['MARGIN_MEAN'].apply(lambda x: '${:,.2f}'.format(x))\n",
        " \n",
        "  display(margin_df_for_display[['VARIANT_ID','MARGIN_TOTAL','MARGIN_PER_CUSTOMER']])\n",
        "except:\n",
        "  print(\"Margin is not computed for this test.  Please check the last time that margin was updated.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AGP\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "agp_agg_df = agp_agg_sp.select(\"*\").toPandas()\n",
        "agp_agg_df = agp_agg_df.sort_values('VARIANT_ID',ascending=True)\n",
        "\n",
        "agp_df_for_display = agp_agg_df[['VARIANT_ID','VISITORS','AGP_TOTAL','AGP_MEAN','NET_SALES_TOTAL','NET_SALES_MEAN']].copy()\n",
        "\n",
        "agp_df_for_display ['CUSTOMERS'] = ['{:,}'.format(i) for i in agp_df_for_display ['VISITORS']]\n",
        "agp_df_for_display ['AGP_TOTAL'] = agp_df_for_display['AGP_TOTAL'].apply(lambda x: '${:,.0f}'.format(x))\n",
        "agp_df_for_display ['AGP_PER_CUSTOMER'] = agp_df_for_display['AGP_MEAN'].apply(lambda x: '${:,.2f}'.format(x))\n",
        "agp_df_for_display ['NET_SALES_TOTAL'] = agp_df_for_display['NET_SALES_TOTAL'].apply(lambda x: '${:,.0f}'.format(x))\n",
        "agp_df_for_display ['NET_SALES_PER_CUSTOMER'] = agp_df_for_display['NET_SALES_MEAN'].apply(lambda x: '${:,.2f}'.format(x))\n",
        "\n",
        "display(agp_df_for_display[['VARIANT_ID','AGP_TOTAL','AGP_PER_CUSTOMER','NET_SALES_TOTAL','NET_SALES_PER_CUSTOMER']])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# eComm TXNs and Redemptions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ecomm_agg_df = ecomm_agg_sp.select(\"*\").toPandas()\n",
        "ecomm_agg_df = ecomm_agg_df.sort_values('VARIANT_ID')\n",
        "\n",
        "ecomm_df_for_display = ecomm_agg_df[['VARIANT_ID','VISITORS','PURCHASING_CUSTOMERS','ECOMM_UNITS_TOTAL','ECOMM_ORDERS_TOTAL','ECOMM_REVENUE_TOTAL','ECOMM_REVENUE_NONZERO_MEAN','ECOMM_BNC_TOTAL','ECOMM_AOV','ECOMM_UPO','ECOMM_RPV','REDEEMING_COUNT_ECOMM','ECOMM_REDEMPTIONS_TOTAL','ECOMM_MKDN_TOTAL','ECOMM_REDEMPTIONS_MEAN','ECOMM_MKDN_MEAN']].copy()\n",
        "\n",
        "ecomm_df_for_display ['ECOMM_CVR'] = ecomm_df_for_display['PURCHASING_CUSTOMERS']/ecomm_df_for_display['VISITORS']\n",
        "ecomm_df_for_display ['ECOMM_BNC_CVR'] = ecomm_df_for_display['ECOMM_BNC_TOTAL']/ecomm_df_for_display['VISITORS']\n",
        "ecomm_df_for_display ['PURCHASING_CUSTOMERS'] = ['{:,}'.format(i) for i in ecomm_df_for_display ['PURCHASING_CUSTOMERS']]\n",
        "ecomm_df_for_display ['ECOMM_UNITS_TOTAL'] = ['{:,}'.format(i) for i in ecomm_df_for_display ['ECOMM_UNITS_TOTAL']]\n",
        "ecomm_df_for_display ['ECOMM_TXNS_TOTAL'] = ['{:,}'.format(i) for i in ecomm_df_for_display ['ECOMM_ORDERS_TOTAL']]\n",
        "ecomm_df_for_display ['ECOMM_REVENUE_TOTAL'] = ecomm_df_for_display['ECOMM_REVENUE_TOTAL'].apply(lambda x: '${:,.0f}'.format(x))\n",
        "ecomm_df_for_display ['ECOMM_AOV'] = ecomm_df_for_display['ECOMM_AOV'].apply(lambda x: '${:,.4f}'.format(x))\n",
        "ecomm_df_for_display ['ECOMM_UPO'] = ecomm_df_for_display['ECOMM_UPO'].apply(lambda x: '{:,.4f}'.format(x))\n",
        "ecomm_df_for_display ['ECOMM_REVENUE_PER_CUSTOMER'] = ecomm_df_for_display['ECOMM_RPV'].apply(lambda x: '${:,.4f}'.format(x))\n",
        "ecomm_df_for_display ['ECOMM_NONZERO_REVENUE_PER_CUSTOMER'] = ecomm_df_for_display['ECOMM_REVENUE_NONZERO_MEAN'].apply(lambda x: '${:,.4f}'.format(x))\n",
        "ecomm_df_for_display ['UNIQUE_REEDEMERS_ECOMM'] = ['{:,}'.format(i) for i in ecomm_df_for_display ['REDEEMING_COUNT_ECOMM']]\n",
        "ecomm_df_for_display ['ECOMM_REDEMPTIONS_TOTAL'] = ['{:,}'.format(i) for i in ecomm_df_for_display ['ECOMM_REDEMPTIONS_TOTAL']]\n",
        "ecomm_df_for_display ['ECOMM_MKDN_TOTAL'] = ecomm_df_for_display['ECOMM_MKDN_TOTAL'].apply(lambda x: '${:,.0f}'.format(x))\n",
        "ecomm_df_for_display ['ECOMM_REDEMPTIONS_PER_CUSTOMER'] = ecomm_df_for_display['ECOMM_REDEMPTIONS_MEAN'].apply(lambda x: '{:,.4f}'.format(x))\n",
        "ecomm_df_for_display ['ECOMM_MKDN_PER_CUSTOMER'] = ecomm_df_for_display['ECOMM_MKDN_MEAN'].apply(lambda x: '${:,.4f}'.format(x))\n",
        "ecomm_df_for_display['ECOMM_CVR'] = ecomm_df_for_display['ECOMM_CVR'].apply(lambda x: '{:,.4f}'.format(x))\n",
        "\n",
        "display(ecomm_df_for_display[['VARIANT_ID','PURCHASING_CUSTOMERS','ECOMM_UNITS_TOTAL','ECOMM_TXNS_TOTAL','ECOMM_REVENUE_TOTAL','ECOMM_BNC_TOTAL','ECOMM_BNC_CVR','ECOMM_AOV','ECOMM_UPO','ECOMM_REVENUE_PER_CUSTOMER','ECOMM_NONZERO_REVENUE_PER_CUSTOMER','UNIQUE_REEDEMERS_ECOMM','ECOMM_REDEMPTIONS_TOTAL','ECOMM_MKDN_TOTAL','ECOMM_CVR']])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# In-Store TXNs and Redemptions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "store_agg_df = store_agg_sp.select(\"*\").toPandas()\n",
        "store_agg_df = store_agg_df.sort_values('VARIANT_ID')\n",
        "\n",
        "store_df_for_display = store_agg_df[['VARIANT_ID','PURCHASING_CUSTOMERS','STORE_UNITS_TOTAL','STORE_ORDERS_TOTAL','STORE_REVENUE_TOTAL','STORE_REVENUE_MEAN','STORE_REVENUE_NONZERO_MEAN','STORE_AOV','STORE_UPO','REDEEMING_COUNT_STORE','STORE_REDEMPTIONS_TOTAL','STORE_MKDN_TOTAL']].copy()\n",
        "\n",
        "store_df_for_display ['PURCHASING_CUSTOMERS'] = ['{:,}'.format(i) for i in store_df_for_display ['PURCHASING_CUSTOMERS']]\n",
        "store_df_for_display ['STORE_UNITS_TOTAL'] = ['{:,}'.format(i) for i in store_df_for_display ['STORE_UNITS_TOTAL']]\n",
        "store_df_for_display ['STORE_TXNS_TOTAL'] = ['{:,}'.format(i) for i in store_df_for_display ['STORE_ORDERS_TOTAL']]\n",
        "store_df_for_display ['STORE_REVENUE_TOTAL'] = store_df_for_display['STORE_REVENUE_TOTAL'].apply(lambda x: '${:,.0f}'.format(x))\n",
        "store_df_for_display ['STORE_AOV'] = store_df_for_display['STORE_AOV'].apply(lambda x: '${:,.4f}'.format(x))\n",
        "store_df_for_display ['STORE_UPO'] = store_df_for_display['STORE_UPO'].apply(lambda x: '{:,.4f}'.format(x))\n",
        "store_df_for_display ['STORE_REVENUE_PER_CUSTOMER'] = store_df_for_display['STORE_REVENUE_MEAN'].apply(lambda x: '${:,.4f}'.format(x))\n",
        "store_df_for_display ['UNIQUE_REDEEMERS_STORE'] = ['{:,}'.format(i) for i in store_df_for_display ['REDEEMING_COUNT_STORE']]\n",
        "store_df_for_display ['STORE_REDEMPTIONS_TOTAL'] = ['{:,}'.format(i) for i in store_df_for_display ['STORE_REDEMPTIONS_TOTAL']]\n",
        "store_df_for_display ['STORE_MKDN_TOTAL'] = store_df_for_display['STORE_MKDN_TOTAL'].apply(lambda x: '${:,.0f}'.format(x))\n",
        "store_df_for_display ['STORE_NONZERO_REVENUE_PER_CUSTOMER'] = store_df_for_display['STORE_REVENUE_NONZERO_MEAN'].apply(lambda x: '${:,.4f}'.format(x))\n",
        "\n",
        "display(store_df_for_display[['VARIANT_ID','PURCHASING_CUSTOMERS','STORE_UNITS_TOTAL','STORE_TXNS_TOTAL','STORE_REVENUE_TOTAL','STORE_AOV','STORE_UPO','STORE_REVENUE_PER_CUSTOMER','STORE_NONZERO_REVENUE_PER_CUSTOMER','UNIQUE_REDEEMERS_STORE','STORE_REDEMPTIONS_TOTAL','STORE_MKDN_TOTAL']])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Gas TXNs and Redemptions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    gas_agg_df = gas_agg_sp.select(\"*\").toPandas()\n",
        "    gas_agg_df = gas_agg_df.sort_values('VARIANT_ID_GAS')\n",
        "\n",
        "    gas_df_for_display = gas_agg_df[['VARIANT_ID_GAS','GAS_VISITORS','GAS_ORDERS_TOTAL','GAS_REVENUE_TOTAL','GAS_REVENUE_NONZERO_MEAN','GAS_AOV','GAS_RPV','GAS_MKDN_MEAN','GAS_REWARD_REDEMPTIONS_TOTAL']].copy()\n",
        "\n",
        "    gas_df_for_display ['PURCHASING_GAS_VISITORS'] = ['{:,}'.format(i) for i in gas_df_for_display ['GAS_VISITORS']]\n",
        "    gas_df_for_display ['GAS_TXNS_TOTAL'] = ['{:,}'.format(i) for i in gas_df_for_display ['GAS_ORDERS_TOTAL']]\n",
        "    gas_df_for_display ['GAS_REVENUE_TOTAL'] = gas_df_for_display['GAS_REVENUE_TOTAL'].apply(lambda x: '${:,.0f}'.format(x))\n",
        "    gas_df_for_display ['GAS_AOV'] = gas_df_for_display['GAS_AOV'].apply(lambda x: '${:,.4f}'.format(x))\n",
        "    gas_df_for_display ['GAS_REVENUE_PER_CUSTOMER'] = gas_df_for_display['GAS_RPV'].apply(lambda x: '${:,.4f}'.format(x))\n",
        "    gas_df_for_display ['GAS_MKDN_PER_CUSTOMER'] = gas_df_for_display['GAS_MKDN_MEAN'].apply(lambda x: '${:,.4f}'.format(x))\n",
        "    gas_df_for_display ['GAS_REWARD_REDEMPTIONS_TOTAL'] = ['{:,}'.format(i) for i in gas_df_for_display ['GAS_REWARD_REDEMPTIONS_TOTAL']]\n",
        "\n",
        "    display(gas_df_for_display[['VARIANT_ID_GAS','PURCHASING_GAS_VISITORS','GAS_TXNS_TOTAL','GAS_REVENUE_TOTAL','GAS_AOV','GAS_REVENUE_PER_CUSTOMER','GAS_MKDN_PER_CUSTOMER','GAS_REWARD_REDEMPTIONS_TOTAL']])\n",
        "except:\n",
        "    display(\"NO GAS TRANSACTION DATA.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Combined (Ecomm and In-Store) TXNs and Redemptions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "combined_df = txns_sp.select(\"*\").toPandas()\n",
        "combined_df = combined_df.sort_values(['VARIANT_ID'],ascending=True)\n",
        "combined_for_display = combined_df.copy()\n",
        "\n",
        "combined_for_display['CVR'] = combined_for_display['PURCHASING_CUSTOMERS']/combined_for_display['VISITORS']\n",
        "combined_for_display ['VISITORS'] = ['{:,}'.format(i) for i in combined_for_display ['VISITORS']]\n",
        "combined_for_display ['PURCHASING_CUSTOMERS'] = ['{:,}'.format(i) for i in combined_for_display ['PURCHASING_CUSTOMERS']]\n",
        "combined_for_display ['COMBINED_REVENUE_TOTAL'] = combined_for_display['COMBINED_REVENUE_TOTAL'].apply(lambda x: '${:,.0f}'.format(x))\n",
        "combined_for_display ['COMBINED_REVENUE_PER_CUSTOMER'] = combined_for_display['COMBINED_RPV'].apply(lambda x: '${:,.4f}'.format(x))\n",
        "combined_for_display ['TXNS_TOTAL'] = ['{:,}'.format(i) for i in combined_for_display ['COMBINED_ORDERS_TOTAL']]\n",
        "combined_for_display ['TXNS_PER_CUSTOMER'] = combined_for_display['COMBINED_ORDERS_MEAN'].apply(lambda x: '{:,.4f}'.format(x))\n",
        "combined_for_display ['UNITS_TOTAL'] = ['{:,}'.format(i) for i in combined_for_display ['COMBINED_UNITS_TOTAL']]\n",
        "combined_for_display ['UNITS_PER_CUSTOMER'] = combined_for_display['COMBINED_UNITS_MEAN'].apply(lambda x: '{:,.4f}'.format(x))\n",
        "combined_for_display ['AOV'] = combined_for_display['COMBINED_AOV'].apply(lambda x: '${:,.4f}'.format(x))\n",
        "combined_for_display ['UPO'] = combined_for_display['COMBINED_UPO'].apply(lambda x: '{:,.4f}'.format(x))\n",
        "combined_for_display ['COMBINED_NONZERO_REVENUE_PER_CUSTOMER'] = combined_for_display['COMBINED_REVENUE_NONZERO_MEAN'].apply(lambda x: '${:,.4f}'.format(x))\n",
        "combined_for_display['CVR'] = combined_for_display['CVR'].apply(lambda x: '{:,.4f}'.format(x))\n",
        "\n",
        "\n",
        "display(combined_for_display[['VARIANT_ID','COMBINED_REVENUE_TOTAL','CVR','COMBINED_REVENUE_PER_CUSTOMER','COMBINED_NONZERO_REVENUE_PER_CUSTOMER','TXNS_TOTAL','UNITS_TOTAL','UNITS_PER_CUSTOMER']])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Clips Breakdown\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "clips_df = clips_agg_sp.select(\"*\").toPandas()\n",
        "clips_df = clips_df.sort_values('VARIANT_ID', ascending=True)\n",
        "clips_for_display = clips_df[['VARIANT_ID','VISITORS','UNIQUE_CLIPPING_HH','CLIPS_TOTAL','CLIPS_MEAN','pd_clips_TOTAL','gr_clips_TOTAL','mf_clips_TOTAL', 'spd_clips_TOTAL', 'pzn_clips_TOTAL', 'sc_clips_TOTAL']].copy()\n",
        "\n",
        "clips_for_display['% CLIPPING'] = 100*clips_for_display ['UNIQUE_CLIPPING_HH'] / clips_for_display ['VISITORS']\n",
        "\n",
        "# clips_for_display ['VISITORS'] = ['{:,}'.format(i) for i in clips_for_display ['VISITORS']]\n",
        "clips_for_display ['UNIQUE_CLIPPING_HH'] = ['{:,}'.format(i) for i in clips_for_display ['UNIQUE_CLIPPING_HH']]\n",
        "clips_for_display ['% CLIPPING'] = clips_for_display['% CLIPPING'].apply(lambda x: '{:,.4f}'.format(x*100))\n",
        "\n",
        "clips_for_display ['CLIPS_TOTAL'] = ['{:,}'.format(i) for i in clips_for_display ['CLIPS_TOTAL']]\n",
        "clips_for_display ['CLIPS_PER_CUSTOMER'] = clips_for_display['CLIPS_MEAN'].apply(lambda x: '{:,.4f}'.format(x))\n",
        "clips_for_display ['pd_clips_TOTAL'] = ['{:,}'.format(i) for i in clips_for_display ['pd_clips_TOTAL']]\n",
        "clips_for_display ['gr_clips_TOTAL'] = ['{:,}'.format(i) for i in clips_for_display ['gr_clips_TOTAL']]\n",
        "clips_for_display ['mf_clips_TOTAL'] = ['{:,}'.format(i) for i in clips_for_display ['mf_clips_TOTAL']]\n",
        "clips_for_display ['sc_clips_TOTAL'] = ['{:,}'.format(i) for i in clips_for_display ['sc_clips_TOTAL']]\n",
        "clips_for_display ['spd_clips_TOTAL'] = ['{:,}'.format(i) for i in clips_for_display ['spd_clips_TOTAL']]\n",
        "clips_for_display ['pzn_clips_TOTAL'] = ['{:,}'.format(i) for i in clips_for_display ['pzn_clips_TOTAL']]\n",
        "\n",
        "display(clips_for_display[['VARIANT_ID','UNIQUE_CLIPPING_HH','% CLIPPING','CLIPS_TOTAL','CLIPS_PER_CUSTOMER','pd_clips_TOTAL','gr_clips_TOTAL','mf_clips_TOTAL', 'spd_clips_TOTAL',\n",
        "'pzn_clips_TOTAL', 'sc_clips_TOTAL']])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Redemptions Breakdown\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "redemptions_df = redemptions_sp.select(\"*\").toPandas()\n",
        "redemptions_df = redemptions_df.sort_values('VARIANT_ID', ascending=True)\n",
        "redemptions_for_display = redemptions_df[['VARIANT_ID','VISITORS','REDEEMING_COUNT_COMBINED','REDEMPTIONS_TOTAL','REDEMPTIONS_MEAN','pd_redemptions_TOTAL','gr_redemptions_TOTAL','mf_redemptions_TOTAL', 'spd_redemptions_TOTAL','pzn_redemptions_TOTAL', 'sc_redemptions_TOTAL','MKDN_TOTAL','MKDN_MEAN','pd_MKDN_TOTAL', 'gr_MKDN_TOTAL', 'mf_MKDN_TOTAL', 'spd_MKDN_TOTAL', 'pzn_MKDN_TOTAL', 'sc_MKDN_TOTAL','BP_EARNING_HOUSEHOLDS','BONUS_POINTS_TOTAL','ESTIMATED_MKDN_TOTAL']].copy()\n",
        "\n",
        "redemptions_for_display['% REDEEMING'] = redemptions_for_display ['REDEEMING_COUNT_COMBINED'] / redemptions_for_display ['VISITORS']\n",
        "redemptions_for_display ['UNIQUE_REDEEMERS'] = ['{:,}'.format(i) for i in redemptions_for_display ['REDEEMING_COUNT_COMBINED']]\n",
        "redemptions_for_display ['% REDEEMING'] = redemptions_for_display['% REDEEMING'].apply(lambda x: '{:,.4f}'.format(x))\n",
        "\n",
        "redemptions_for_display['% BP EARNING'] = redemptions_for_display ['BP_EARNING_HOUSEHOLDS'] / redemptions_for_display ['VISITORS']\n",
        "redemptions_for_display ['UNIQUE_BP_EARNING_HH'] = ['{:,}'.format(i) for i in redemptions_for_display ['BP_EARNING_HOUSEHOLDS']]\n",
        "redemptions_for_display ['% BP EARNING'] = redemptions_for_display['% BP EARNING'].apply(lambda x: '{:,.4f}'.format(x))\n",
        "\n",
        "redemptions_for_display ['REDEMPTIONS_TOTAL'] = ['{:,}'.format(i) for i in redemptions_for_display ['REDEMPTIONS_TOTAL']]\n",
        "redemptions_for_display ['REDEMPTIONS_PER_CUSTOMER'] = redemptions_for_display['REDEMPTIONS_MEAN'].apply(lambda x: '{:,.4f}'.format(x))\n",
        "redemptions_for_display ['MKDN_PER_CUSTOMER'] = redemptions_for_display['MKDN_MEAN'].apply(lambda x: '${:,.4f}'.format(x))\n",
        "\n",
        "redemptions_for_display ['pd_redemptions_TOTAL'] = ['{:,}'.format(i) for i in redemptions_for_display ['pd_redemptions_TOTAL']]\n",
        "redemptions_for_display ['gr_redemptions_TOTAL'] = ['{:,}'.format(i) for i in redemptions_for_display ['gr_redemptions_TOTAL']]\n",
        "redemptions_for_display ['mf_redemptions_TOTAL'] = ['{:,}'.format(i) for i in redemptions_for_display ['mf_redemptions_TOTAL']]\n",
        "redemptions_for_display ['sc_redemptions_TOTAL'] = ['{:,}'.format(i) for i in redemptions_for_display ['sc_redemptions_TOTAL']]\n",
        "redemptions_for_display ['spd_redemptions_TOTAL'] = ['{:,}'.format(i) for i in redemptions_for_display ['spd_redemptions_TOTAL']]\n",
        "redemptions_for_display ['pzn_redemptions_TOTAL'] = ['{:,}'.format(i) for i in redemptions_for_display ['pzn_redemptions_TOTAL']]\n",
        "redemptions_for_display ['BONUS_POINTS_TOTAL'] = ['{:,}'.format(i) for i in redemptions_for_display ['BONUS_POINTS_TOTAL']]\n",
        "\n",
        "redemptions_for_display['MKDN_TOTAL'] = redemptions_for_display['MKDN_TOTAL'].apply(lambda x: '${:,.0f}'.format(x))\n",
        "redemptions_for_display['pd_MKDN_TOTAL'] = redemptions_for_display['pd_MKDN_TOTAL'].apply(lambda x: '${:,.0f}'.format(x))\n",
        "redemptions_for_display['gr_MKDN_TOTAL'] = redemptions_for_display['gr_MKDN_TOTAL'].apply(lambda x: '${:,.0f}'.format(x))\n",
        "redemptions_for_display['mf_MKDN_TOTAL'] = redemptions_for_display['mf_MKDN_TOTAL'].apply(lambda x: '${:,.0f}'.format(x))\n",
        "redemptions_for_display['sc_MKDN_TOTAL'] = redemptions_for_display['sc_MKDN_TOTAL'].apply(lambda x: '${:,.0f}'.format(x))\n",
        "redemptions_for_display['spd_MKDN_TOTAL'] = redemptions_for_display['spd_MKDN_TOTAL'].apply(lambda x: '${:,.0f}'.format(x))\n",
        "redemptions_for_display['pzn_MKDN_TOTAL'] = redemptions_for_display['pzn_MKDN_TOTAL'].apply(lambda x: '${:,.0f}'.format(x))\n",
        "redemptions_for_display['ESTIMATED_MKDN_TOTAL'] = redemptions_for_display['ESTIMATED_MKDN_TOTAL'].apply(lambda x: '${:,.0f}'.format(x))\n",
        "\n",
        "display(redemptions_for_display[['VARIANT_ID','UNIQUE_REDEEMERS','% REDEEMING','REDEMPTIONS_TOTAL','pd_redemptions_TOTAL','gr_redemptions_TOTAL','mf_redemptions_TOTAL', 'spd_redemptions_TOTAL',\n",
        "'pzn_redemptions_TOTAL', 'sc_redemptions_TOTAL','% BP EARNING','BONUS_POINTS_TOTAL','ESTIMATED_MKDN_TOTAL']])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Markdown Breakdown\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "markdown_for_display = redemptions_for_display.merge(store_df_for_display[['VARIANT_ID','STORE_MKDN_TOTAL']],on='VARIANT_ID', how='left')\n",
        "markdown_for_display = markdown_for_display.merge(ecomm_df_for_display[['VARIANT_ID','ECOMM_MKDN_TOTAL']],on='VARIANT_ID', how='left')\n",
        "\n",
        "display(markdown_for_display[['VARIANT_ID','MKDN_TOTAL','ECOMM_MKDN_TOTAL','STORE_MKDN_TOTAL','pd_MKDN_TOTAL', 'gr_MKDN_TOTAL', 'mf_MKDN_TOTAL', 'spd_MKDN_TOTAL', 'pzn_MKDN_TOTAL', 'sc_MKDN_TOTAL']])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Basket Health\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "basket_health_df = basket_health_agg_sp.select(\"*\").toPandas()\n",
        "basket_health_df = basket_health_df.sort_values('VARIANT_ID', ascending=True)\n",
        "bh_for_display = basket_health_df[['VARIANT_ID','VISITORS','UNIQUE_BASKET_HEALTH_HH','BASKET_RATE_TOTAL','BASKET_RATE_MEAN','AB_RATE_TOTAL','AB_RATE_MEAN','DE_RATE_TOTAL', 'DE_RATE_MEAN', 'TOTAL_BASKET_HEALTH_TXNS']].copy()\n",
        "bh_for_display['% BASKET HEALTH HH'] = bh_for_display ['UNIQUE_BASKET_HEALTH_HH'] / bh_for_display ['VISITORS']\n",
        "bh_for_display['% BASKET TXNS'] = bh_for_display ['TOTAL_BASKET_HEALTH_TXNS'] / combined_df ['COMBINED_ORDERS_TOTAL']\n",
        "\n",
        "bh_for_display ['UNIQUE_BASKET_HEALTH_HH'] = ['{:,}'.format(i) for i in bh_for_display ['UNIQUE_BASKET_HEALTH_HH']]\n",
        "bh_for_display ['% BASKET HEALTH HH'] = bh_for_display ['% BASKET HEALTH HH'].apply(lambda x: '{:,.4f}'.format(x*100))\n",
        "\n",
        "bh_for_display ['NET_BASKET_RATE_TOTAL'] = bh_for_display['BASKET_RATE_TOTAL'].apply(lambda x: '{:,.4f}'.format(x))\n",
        "bh_for_display ['NET_BASKET_RATE_MEAN'] = bh_for_display['BASKET_RATE_MEAN'].apply(lambda x: '{:,.4f}'.format(x))\n",
        "bh_for_display ['AB_RATE_MEAN'] = bh_for_display ['AB_RATE_MEAN'].apply(lambda x: '{:,.4f}'.format(x))\n",
        "bh_for_display ['DE_RATE_MEAN'] = bh_for_display['DE_RATE_MEAN'].apply(lambda x: '{:,.4f}'.format(x))\n",
        "bh_for_display ['TOTAL_BASKET_HEALTH_TXNS'] = ['{:,}'.format(i) for i in bh_for_display ['TOTAL_BASKET_HEALTH_TXNS']]\n",
        "bh_for_display ['% BASKET TXNS'] = bh_for_display['% BASKET TXNS'].apply(lambda x: '{:,.4f}'.format(x*100))\n",
        "\n",
        "display(bh_for_display[['VARIANT_ID','UNIQUE_BASKET_HEALTH_HH','% BASKET HEALTH HH','NET_BASKET_RATE_TOTAL','NET_BASKET_RATE_MEAN','AB_RATE_MEAN','DE_RATE_MEAN','TOTAL_BASKET_HEALTH_TXNS', '% BASKET TXNS']])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Category Depth And Breadth\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "category_breadth_depth_df = category_agg_sp.select(\"*\").toPandas()\n",
        "category_breadth_depth_df = category_breadth_depth_df.sort_values('VARIANT_ID', ascending=True)\n",
        "\n",
        "category_breadth_depth_df['CATEGORIES_TOTAL'] = category_breadth_depth_df['CATEGORIES_TOTAL'].apply(lambda x: '{:,.0f}'.format(x))\n",
        "category_breadth_depth_df['ITEMS_TOTAL'] = category_breadth_depth_df['ITEMS_TOTAL'].apply(lambda x: '{:,.0f}'.format(x))\n",
        "\n",
        "for col in ['CATEGORIES_MEAN', 'CATEGORIES_SD', 'ITEMS_MEAN', 'ITEMS_SD', 'CATEGORY_DEPTH', 'CD_COV_ITEMS_CATEGORIES']:\n",
        "    category_breadth_depth_df[col] = category_breadth_depth_df[col].astype(str).str.replace(',', '').astype(float)\n",
        "category_breadth_depth_df[col] = category_breadth_depth_df[col].round(4)\n",
        "\n",
        "display(category_breadth_depth_df)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Account Health\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "account_health_agg_df = account_health_agg_sp.select(\"*\").toPandas()\n",
        "account_health_agg_df = account_health_agg_df.sort_values('VARIANT_ID', ascending=True)\n",
        "\n",
        "account_health_display = account_health_agg_df[['VARIANT_ID','UNIQUE_HOUSEHOLDS','email_count', 'phone_count', 'fn_ln_count', 'bday_count', 'address_count']].copy()\n",
        "columns_to_format = ['UNIQUE_HOUSEHOLDS','email_count', 'phone_count', 'fn_ln_count', 'bday_count', 'address_count']\n",
        "account_health_display[columns_to_format] = account_health_display[columns_to_format].applymap(lambda x: f\"{x:,}\")\n",
        "display(account_health_display)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Output Table\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "summary_table1 = base_df_for_display[['VARIANT_ID','VISITORS','UNIQUE_HOUSEHOLDS','AUTHENTICATED_RATE','VISITS_TOTAL','VISITS_PER_CUSTOMER','SEARCHES_TOTAL','SEARCHES_PER_CUSTOMER','SEARCHES_CVR','CART_ADDS_TOTAL','CART_ADDS_PER_CUSTOMER','CART_ADDS_CVR','COUPON_CLIPS_TOTAL','COUPON_CLIPS_PER_CUSTOMER','COUPON_CLIPS_CVR']].merge(\n",
        "combined_for_display[['VARIANT_ID','UNITS_TOTAL','UNITS_PER_CUSTOMER','TXNS_TOTAL','TXNS_PER_CUSTOMER','COMBINED_REVENUE_TOTAL','COMBINED_REVENUE_PER_CUSTOMER','COMBINED_NONZERO_REVENUE_PER_CUSTOMER','CVR','AOV','UPO']],\n",
        "on='VARIANT_ID', how='left'\n",
        ").merge(\n",
        "redemptions_for_display[['VARIANT_ID','REDEMPTIONS_TOTAL','REDEMPTIONS_PER_CUSTOMER','MKDN_TOTAL','MKDN_PER_CUSTOMER']],\n",
        "on='VARIANT_ID', how='left'\n",
        ")\n",
        "\n",
        "\n",
        "summary_table2 = summary_table1.merge(ecomm_df_for_display[['VARIANT_ID','ECOMM_REVENUE_TOTAL','ECOMM_REVENUE_PER_CUSTOMER','ECOMM_NONZERO_REVENUE_PER_CUSTOMER','ECOMM_AOV', 'ECOMM_UPO','ECOMM_REDEMPTIONS_TOTAL','ECOMM_MKDN_TOTAL','ECOMM_REDEMPTIONS_PER_CUSTOMER', 'ECOMM_MKDN_PER_CUSTOMER','ECOMM_CVR','ECOMM_BNC_TOTAL','ECOMM_BNC_CVR']],on='VARIANT_ID', how='left')\n",
        "\n",
        "summary_table3 = summary_table2.merge(margin_df_for_display[['VARIANT_ID','MARGIN_TOTAL','MARGIN_PER_CUSTOMER']], on='VARIANT_ID', how='left')\n",
        "\n",
        "summary_table4 = summary_table3.merge(agp_df_for_display[['VARIANT_ID','AGP_TOTAL','AGP_PER_CUSTOMER','NET_SALES_TOTAL','NET_SALES_PER_CUSTOMER']], on='VARIANT_ID', how='left')\n",
        "\n",
        "summary_table5 = summary_table4.merge(category_breadth_depth_df[['VARIANT_ID','CATEGORIES_MEAN','CATEGORY_DEPTH']], on='VARIANT_ID', how='left')\n",
        "\n",
        "output_table = summary_table5[['VARIANT_ID','VISITORS','COMBINED_REVENUE_PER_CUSTOMER','COMBINED_NONZERO_REVENUE_PER_CUSTOMER','NET_SALES_PER_CUSTOMER','ECOMM_REVENUE_PER_CUSTOMER','MKDN_PER_CUSTOMER','AGP_PER_CUSTOMER','CVR', 'AOV', 'UPO', 'TXNS_PER_CUSTOMER', 'UNITS_PER_CUSTOMER','VISITS_PER_CUSTOMER','SEARCHES_PER_CUSTOMER','CART_ADDS_PER_CUSTOMER','COUPON_CLIPS_PER_CUSTOMER','REDEMPTIONS_PER_CUSTOMER','SEARCHES_CVR','CART_ADDS_CVR','COUPON_CLIPS_CVR','VISITS_TOTAL','SEARCHES_TOTAL','CART_ADDS_TOTAL','COUPON_CLIPS_TOTAL','TXNS_TOTAL','UNITS_TOTAL','COMBINED_REVENUE_TOTAL','NET_SALES_TOTAL','MKDN_TOTAL','REDEMPTIONS_TOTAL','ECOMM_REVENUE_TOTAL', 'ECOMM_MKDN_TOTAL', 'ECOMM_REDEMPTIONS_TOTAL', 'ECOMM_NONZERO_REVENUE_PER_CUSTOMER', 'ECOMM_REDEMPTIONS_PER_CUSTOMER', 'ECOMM_MKDN_PER_CUSTOMER','ECOMM_CVR','ECOMM_BNC_TOTAL','ECOMM_BNC_CVR','CATEGORIES_MEAN','CATEGORY_DEPTH','UNIQUE_HOUSEHOLDS','AUTHENTICATED_RATE']]\n",
        "\n",
        "output_table = output_table.rename(columns={\"CATEGORIES_MEAN\": \"CATEGORY_BREADTH\"})\n",
        "\n",
        "display(output_table)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Summary Table\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## TODO - We may be able to delete this cell\n",
        "summary_table4 = summary_table4[['VARIANT_ID','UNIQUE_HOUSEHOLDS','COMBINED_REVENUE_PER_CUSTOMER','COMBINED_NONZERO_REVENUE_PER_CUSTOMER','NET_SALES_PER_CUSTOMER','CVR','MKDN_PER_CUSTOMER','AGP_PER_CUSTOMER','TXNS_PER_CUSTOMER','UNITS_PER_CUSTOMER','VISITS_PER_CUSTOMER','SEARCHES_PER_CUSTOMER','CART_ADDS_PER_CUSTOMER','COUPON_CLIPS_PER_CUSTOMER','MARGIN_PER_CUSTOMER','REDEMPTIONS_PER_CUSTOMER']]\n",
        "\n",
        "display(summary_table4)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if WINSORIZE != 'OFF':\n",
        " \n",
        "  if round(ecomm_agg_df['WINSORIZATION_THRESHOLD'].iloc[0],2) == 0:\n",
        "    ecomm_thresh = 'NO WINSORIZATION DUE TO $0.00 AT 99th'\n",
        "    ecomm_win_cnt = 0\n",
        "    ecomm_win_pct = 0\n",
        "  else:\n",
        "    ecomm_thresh = f'''${round(ecomm_agg_df['WINSORIZATION_THRESHOLD'].iloc[0],2)}'''\n",
        "    ecomm_win_cnt = ecomm_agg_df['WINSORIZED_CUSTOMERS']\n",
        "    ecomm_win_pct = round(ecomm_agg_df['WINSORIZED_CUSTOMERS']/ecomm_agg_df['VISITORS']*100.00,3)\n",
        "  \n",
        "  if round(store_agg_df['WINSORIZATION_THRESHOLD'].iloc[0],2) == 0:\n",
        "    store_thresh = 'NO WINSORIZATION DUE TO $0.00 AT 99th'\n",
        "    store_win_cnt = 0\n",
        "    store_win_pct = 0\n",
        "  else:\n",
        "    store_thresh = f'''${round(store_agg_df['WINSORIZATION_THRESHOLD'].iloc[0],2)}'''\n",
        "    store_win_cnt = store_agg_df['WINSORIZED_CUSTOMERS']\n",
        "    store_win_pct = round(store_agg_df['WINSORIZED_CUSTOMERS']/store_agg_df['VISITORS']*100.00,3)\n",
        "  \n",
        "  if round(combined_df['WINSORIZATION_THRESHOLD'].iloc[0],2) == 0:\n",
        "    combined_thresh = 'NO WINSORIZATION DUE TO $0.00 AT 99th'\n",
        "    combined_win_cnt = 0\n",
        "    combined_win_pct = 0\n",
        "  else:\n",
        "    combined_thresh = f'''${round(combined_df['WINSORIZATION_THRESHOLD'].iloc[0],2)}'''\n",
        "    combined_win_cnt = combined_df['WINSORIZED_CUSTOMERS']\n",
        "    combined_win_pct = round(combined_df['WINSORIZED_CUSTOMERS']/combined_df['VISITORS']*100.00,3)\n",
        "  \n",
        "  displayHTML(f'''<h3><font color=\"black\"> Winsorized (99th Percentile) Values: </font></h3>\n",
        "                  <p><font color=\"black\"> EComm 99th:  {ecomm_thresh} </font></p>\n",
        "                  <p><font color=\"black\"> In-Store 99th:  {store_thresh} </font></p>\n",
        "                  <p><font color=\"black\"> Combined 99th:  {combined_thresh} </font></p>\n",
        "                  ''')\n",
        "else:\n",
        "    displayHTML(f'''<h3><font color=\"black\"> Winsorization is turned OFF for this experiment analysis. </font></h3>\n",
        "    ''')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "if WINSORIZE != 'OFF':\n",
        "    win_df_ecomm = pd.DataFrame({\n",
        "\"Variant\": ecomm_agg_df['VARIANT_ID'],\n",
        "\"EComm Txns Winzorized Count\": ecomm_win_cnt,\n",
        "\"Ecomm Txns Winzorized Percentage\": ecomm_win_pct\n",
        "})\n",
        "\n",
        "win_df_store = pd.DataFrame({\n",
        "\"Variant\": store_agg_df['VARIANT_ID'],\n",
        "\"In-Store Txns Winzorized Count\": store_win_cnt,\n",
        "\"In-Store Txns Winzorized Percentage\": store_win_pct\n",
        "})\n",
        "\n",
        "win_df_combined = pd.DataFrame({\n",
        "\"Variant\": combined_df['VARIANT_ID'],\n",
        "\"Combined Txns Winzorized Count\": combined_win_cnt,\n",
        "\"Combined Txns Winzorized Percentage\": combined_win_pct\n",
        "})\n",
        "\n",
        "display(win_df_combined.merge(win_df_store, on = 'Variant').merge(win_df_ecomm, on = 'Variant'))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Experiment Details & Health Checks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Experiment Details\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "current_time = str(datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\"))\n",
        " \n",
        "if PAGE_FILTER_INPUT:\n",
        "  page_filter_display = PAGE_FILTER_INPUT.upper()\n",
        "  page_filter_link = PAGE_FILTER_INPUT.replace(\", \",\"_\").replace(\",\",\"_\").replace(\" \",\"_\")\n",
        "else:\n",
        "  page_filter_display = 'No_Filter'\n",
        "  page_filter_link = 'No_Filter'\n",
        " \n",
        "db_id = spark.conf.get(\"spark.databricks.clusterUsageTags.clusterOwnerOrgId\")\n",
        "host_name = spark.conf.get(\"spark.databricks.workspaceUrl\")\n",
        "if 'gcp' in host_name.lower():\n",
        "  output_table_link = f\"https://{db_id}.1.gcp.databricks.com/files/SAFE/{EXPERIMENT_ID}_{page_filter_link}_{OS_PLATFORM}_{current_time}.csv?o={db_id}\"\n",
        "else:\n",
        "  output_table_link = f\"https://adb-{db_id}.12.azuredatabricks.net/files/SAFE/{EXPERIMENT_ID}_{page_filter_link}_{OS_PLATFORM}_{current_time}.csv?o={db_id}\"\n",
        " \n",
        "output_table.to_csv(f'''/dbfs/FileStore/SAFE/{EXPERIMENT_ID}_{page_filter_link}_{OS_PLATFORM}_{current_time}.csv''',index=False)\n",
        " \n",
        "displayHTML(f'''<h3><font color=\"grey\"> Experiment: {EXPERIMENT_ID} conducted from {EXP_START_DATE} to {EXP_END_DATE} </font></h3>\n",
        "          <h3><font color=\"grey\"> Comparing variations: {list(base_df['VARIANT_ID'])} </font></h3>\n",
        "          <h3><font color=\"grey\"> Page filter: {page_filter_display} </font></h3>\n",
        "          <h3><font color=\"grey\"> Exposure filter: {EXPOSURE_FILTER} </font></h3>\n",
        "          <h3><font color=\"grey\"> App filter: {OS_PLATFORM} </font></h3>\n",
        "          <h3><font color=\"grey\"> Detailed Output Table: {output_table_link} </font></h3>\n",
        "           ''')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Summary Metrics (In-Store + Ecomm)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Customers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This test is for final DataFrame\n",
        "visitors_test = hypothesis_test_compare_means(\n",
        "df = base_df,\n",
        "variant_column_name = \"VARIANT_ID\",\n",
        "control_variant_name = control_variant_nm,\n",
        "mean_column_name = \"VISITORS\",\n",
        "std_column_name = \"VISITS_SD\",\n",
        "n_column_name = \"VISITORS\",\n",
        "metric_type = 'mean',\n",
        "pooled = False,\n",
        "one_tailed = False,\n",
        "ci_level = 1-SIGNIFICANCE,\n",
        "positive_good = True,\n",
        "rounding = 3\n",
        ")\n",
        "\n",
        "display(visitors_test[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Average Visits Per Customer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sessions_test = hypothesis_test_compare_means(\n",
        "df = base_df,\n",
        "variant_column_name = \"VARIANT_ID\",\n",
        "control_variant_name = control_variant_nm,\n",
        "mean_column_name = \"VISITS_MEAN\",\n",
        "std_column_name = \"VISITS_SD\",\n",
        "n_column_name = \"VISITORS\",\n",
        "metric_type = 'mean',\n",
        "pooled = False,\n",
        "one_tailed = False,\n",
        "ci_level = 1-SIGNIFICANCE,\n",
        "positive_good = True,\n",
        "rounding = 3\n",
        ")\n",
        "\n",
        "display(sessions_test[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Visits Per Customer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sessions_test[1].show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Average Revenue Per Customer (RPC)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "revenue_test = hypothesis_test_compare_means(\n",
        "df = combined_df,\n",
        "variant_column_name = \"VARIANT_ID\",\n",
        "control_variant_name = control_variant_nm,\n",
        "mean_column_name = \"COMBINED_REVENUE_MEAN\",\n",
        "std_column_name = \"COMBINED_REVENUE_SD\",\n",
        "n_column_name = \"VISITORS\",\n",
        "metric_type = \"mean\",\n",
        "pooled = False,\n",
        "one_tailed = False,\n",
        "ci_level = 1-SIGNIFICANCE,\n",
        "positive_good = True,\n",
        "rounding = 3\n",
        ")\n",
        "\n",
        "display(revenue_test[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RPC\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "revenue_test[1].show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Average Non-Zero Revenue Per Customer (RPC)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "nz_revenue_test = hypothesis_test_compare_means(\n",
        "df = combined_df,\n",
        "variant_column_name = \"VARIANT_ID\",\n",
        "control_variant_name = control_variant_nm,\n",
        "mean_column_name = \"COMBINED_REVENUE_NONZERO_MEAN\",\n",
        "std_column_name = \"COMBINED_REVENUE_NONZERO_SD\",\n",
        "n_column_name = \"VISITORS\",\n",
        "metric_type = \"mean\",\n",
        "pooled = False,\n",
        "one_tailed = False,\n",
        "ci_level = 1-SIGNIFICANCE,\n",
        "positive_good = True,\n",
        "rounding = 3\n",
        ")\n",
        "\n",
        "display(nz_revenue_test[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# NONZERO RPC\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "nz_revenue_test[1].show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Average AGP Per Customer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "agp_test = hypothesis_test_compare_means(\n",
        "df = agp_agg_df,\n",
        "variant_column_name = \"VARIANT_ID\",\n",
        "control_variant_name = control_variant_nm,\n",
        "mean_column_name = \"AGP_MEAN\",\n",
        "std_column_name = \"AGP_SD\",\n",
        "n_column_name = \"VISITORS\",\n",
        "metric_type = \"mean\",\n",
        "pooled = False,\n",
        "one_tailed = False,\n",
        "ci_level = 1-SIGNIFICANCE,\n",
        "positive_good = True,\n",
        "rounding = 3\n",
        ")\n",
        "\n",
        "display(agp_test[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AGP Per Customer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "agp_test[1].show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Average Net Sales Per Customer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "net_sales_test = hypothesis_test_compare_means(\n",
        "df = agp_agg_df,\n",
        "variant_column_name = \"VARIANT_ID\",\n",
        "control_variant_name = control_variant_nm,\n",
        "mean_column_name = \"NET_SALES_MEAN\",\n",
        "std_column_name = \"NET_SALES_SD\",\n",
        "n_column_name = \"VISITORS\",\n",
        "metric_type = \"mean\",\n",
        "pooled = False,\n",
        "one_tailed = False,\n",
        "ci_level = 1-SIGNIFICANCE,\n",
        "positive_good = True,\n",
        "rounding = 3\n",
        ")\n",
        "\n",
        "display(net_sales_test[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Net Sales Per Customer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "net_sales_test[1].show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Average Margin Per Customer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    margin_test = hypothesis_test_compare_means(\n",
        "    df = margin_agg_df,\n",
        "    variant_column_name = \"VARIANT_ID\",\n",
        "    control_variant_name = control_variant_nm,\n",
        "    mean_column_name = \"MARGIN_MEAN\",\n",
        "    std_column_name = \"MARGIN_SD\",\n",
        "    n_column_name = \"VISITORS\",\n",
        "    metric_type = \"mean\",\n",
        "    pooled = False,\n",
        "    one_tailed = False,\n",
        "    ci_level = 1-SIGNIFICANCE,\n",
        "    positive_good = True,\n",
        "    rounding = 3\n",
        "    )\n",
        "\n",
        "    display(margin_test[0])\n",
        "except:\n",
        "    print(\"Margin is not computed for this test. Please check the last time that margin was updated.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Margin Per Customer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    margin_test[1].show()\n",
        "except:\n",
        "    print(\"Margin is not computed for this test. Please check the last time that margin was updated.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Average TXNs Per Customer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "orders_test = hypothesis_test_compare_means(\n",
        "df = combined_df,\n",
        "variant_column_name = \"VARIANT_ID\",\n",
        "control_variant_name = control_variant_nm,\n",
        "mean_column_name = \"COMBINED_ORDERS_MEAN\",\n",
        "std_column_name = \"COMBINED_ORDERS_SD\",\n",
        "n_column_name = \"VISITORS\",\n",
        "metric_type = 'mean',\n",
        "pooled = False,\n",
        "one_tailed = False,\n",
        "ci_level = 1-SIGNIFICANCE,\n",
        "positive_good = True,\n",
        "rounding = 4\n",
        ")\n",
        "\n",
        "display(orders_test[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TXNs Per Customer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "orders_test[1].show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Average Units Sold Per Customer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "units_test = hypothesis_test_compare_means(\n",
        "df = combined_df,\n",
        "variant_column_name = \"VARIANT_ID\",\n",
        "control_variant_name = control_variant_nm,\n",
        "mean_column_name = \"COMBINED_UNITS_MEAN\",\n",
        "std_column_name = \"COMBINED_UNITS_SD\",\n",
        "n_column_name = \"VISITORS\",\n",
        "metric_type = 'mean',\n",
        "pooled = False,\n",
        "one_tailed = False,\n",
        "ci_level = 1-SIGNIFICANCE,\n",
        "positive_good = True,\n",
        "rounding = 4\n",
        ")\n",
        "\n",
        "display(units_test[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Units Per Customer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "units_test[1].show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Total Redemptions Per Customer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "coupon_redemptions_test = hypothesis_test_compare_means(\n",
        "df = redemptions_df,\n",
        "variant_column_name = \"VARIANT_ID\",\n",
        "control_variant_name = control_variant_nm,\n",
        "mean_column_name = \"REDEMPTIONS_MEAN\",\n",
        "std_column_name = \"REDEMPTIONS_SD\",\n",
        "n_column_name = \"VISITORS\",\n",
        "metric_type = \"mean\",\n",
        "pooled = False,\n",
        "one_tailed = False,\n",
        "ci_level = 1-SIGNIFICANCE,\n",
        "positive_good = True,\n",
        "rounding = 3\n",
        ")\n",
        "\n",
        "display(coupon_redemptions_test[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Redemptions Per Customer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "coupon_redemptions_test[1].show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Total Markdown Per Customer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mkdn_test = hypothesis_test_compare_means(\n",
        "df = redemptions_df,\n",
        "variant_column_name = \"VARIANT_ID\",\n",
        "control_variant_name = control_variant_nm,\n",
        "mean_column_name = \"MKDN_MEAN\",\n",
        "std_column_name = \"MKDN_SD\",\n",
        "n_column_name = \"VISITORS\",\n",
        "metric_type = \"mean\",\n",
        "pooled = False,\n",
        "one_tailed = False,\n",
        "ci_level = 1-SIGNIFICANCE,\n",
        "positive_good = True,\n",
        "rounding = 3\n",
        ")\n",
        "\n",
        "display(mkdn_test[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Markdown Per Customer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mkdn_test[1].show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Average Order Value\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "AOV_test = hypothesis_test_compare_means(\n",
        "df = combined_df,\n",
        "variant_column_name = \"VARIANT_ID\",\n",
        "control_variant_name = control_variant_nm,\n",
        "mean_column_name = [\"COMBINED_REVENUE_MEAN\",\"COMBINED_ORDERS_MEAN\"],\n",
        "std_column_name = [\"COMBINED_REVENUE_SD\",\"COMBINED_ORDERS_SD\"],\n",
        "n_column_name = \"VISITORS\",\n",
        "cov_column_name = \"COMBINED_COV_REVENUE_ORDERS\",\n",
        "metric_type = 'ratio',\n",
        "pooled = False,\n",
        "one_tailed = False,\n",
        "ci_level = 1-SIGNIFICANCE,\n",
        "positive_good = True,\n",
        "rounding = 4\n",
        ")\n",
        "\n",
        "display(AOV_test[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AOV\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "AOV_test[1].show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Average Units Per Order (Basket Size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "UPO_test = hypothesis_test_compare_means(\n",
        "df = combined_df,\n",
        "variant_column_name = \"VARIANT_ID\",\n",
        "control_variant_name = control_variant_nm,\n",
        "mean_column_name = [\"COMBINED_UNITS_MEAN\",\"COMBINED_ORDERS_MEAN\"],\n",
        "std_column_name = [\"COMBINED_UNITS_SD\",\"COMBINED_ORDERS_SD\"],\n",
        "n_column_name = \"VISITORS\",\n",
        "cov_column_name = \"COMBINED_COV_UNITS_ORDERS\",\n",
        "metric_type = \"ratio\",\n",
        "pooled = False,\n",
        "one_tailed = False,\n",
        "ci_level = 1-SIGNIFICANCE,\n",
        "positive_good = True,\n",
        "rounding = 4\n",
        ")\n",
        "\n",
        "display(UPO_test[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# UPO\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "UPO_test[1].show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Engagement Metrics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Average Cart Adds Per Customer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cartadds_test = hypothesis_test_compare_means(\n",
        "df = base_df,\n",
        "variant_column_name = \"VARIANT_ID\",\n",
        "control_variant_name = control_variant_nm,\n",
        "mean_column_name = \"CART_ADDS_MEAN\",\n",
        "std_column_name = \"CART_ADDS_SD\",\n",
        "n_column_name = \"VISITORS\",\n",
        "metric_type = 'mean',\n",
        "pooled = False,\n",
        "one_tailed = False,\n",
        "ci_level = 1-SIGNIFICANCE,\n",
        "positive_good = True,\n",
        "rounding = 4\n",
        ")\n",
        "\n",
        "display(cartadds_test[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Cart Adds Per Customer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cartadds_test[1].show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Coupon Clips Per Customer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ACC_test = hypothesis_test_compare_means(\n",
        "df = base_df,\n",
        "variant_column_name = \"VARIANT_ID\",\n",
        "control_variant_name = control_variant_nm,\n",
        "mean_column_name = \"COUPON_CLIPS_MEAN\",\n",
        "std_column_name = \"COUPON_CLIPS_SD\",\n",
        "n_column_name = \"VISITORS\",\n",
        "metric_type = 'mean',\n",
        "pooled = False,\n",
        "one_tailed = False,\n",
        "ci_level = 1-SIGNIFICANCE,\n",
        "positive_good = True,\n",
        "rounding = 4\n",
        ")\n",
        "\n",
        "display(ACC_test[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Coupon Clips Per Customer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ACC_test[1].show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Average Searches Per Customer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "searches_test = hypothesis_test_compare_means(\n",
        "df = base_df,\n",
        "variant_column_name = \"VARIANT_ID\",\n",
        "control_variant_name = control_variant_nm,\n",
        "mean_column_name = \"SEARCHES_MEAN\",\n",
        "std_column_name = \"SEARCHES_SD\",\n",
        "n_column_name = \"VISITORS\",\n",
        "metric_type = 'mean',\n",
        "pooled = False,\n",
        "one_tailed = False,\n",
        "ci_level = 1-SIGNIFICANCE,\n",
        "positive_good = True,\n",
        "rounding = 4\n",
        ")\n",
        "\n",
        "display(searches_test[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Searches Per Customer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "searches_test[1].show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Add to Cart CVR\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cartadds_cvr_test = hypothesis_test_compare_means(\n",
        "df = base_df,\n",
        "variant_column_name = \"VARIANT_ID\",\n",
        "control_variant_name = control_variant_nm,\n",
        "mean_column_name = \"UNIQUE_USERS_THAT_ADDTOCART\",\n",
        "std_column_name = \"\",\n",
        "n_column_name = \"VISITORS\",\n",
        "metric_type = 'rate',\n",
        "pooled = True,\n",
        "one_tailed = False,\n",
        "ci_level = 1-SIGNIFICANCE,\n",
        "positive_good = True,\n",
        "rounding = 4\n",
        ")\n",
        "\n",
        "display(cartadds_cvr_test[0])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ATC CVR\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cartadds_cvr_test[1].show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Transactions CVR\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "orders_cvr_test = hypothesis_test_compare_means(\n",
        "df = combined_df,\n",
        "variant_column_name = \"VARIANT_ID\",\n",
        "control_variant_name = control_variant_nm,\n",
        "mean_column_name = \"PURCHASING_CUSTOMERS\",\n",
        "std_column_name = \"\",\n",
        "n_column_name = \"VISITORS\",\n",
        "metric_type = 'rate',\n",
        "pooled = True,\n",
        "one_tailed = False,\n",
        "ci_level = 1-SIGNIFICANCE,\n",
        "positive_good = True,\n",
        "rounding = 4\n",
        ")\n",
        "\n",
        "display(orders_cvr_test[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TXNs CVR\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "orders_cvr_test[1].show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Coupon Clips CVR\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "couponclips_cvr_test = hypothesis_test_compare_means(\n",
        "df = base_df,\n",
        "variant_column_name = \"VARIANT_ID\",\n",
        "control_variant_name = control_variant_nm,\n",
        "mean_column_name = \"UNIQUE_USERS_THAT_COUPON_CLIP\",\n",
        "std_column_name = \"\",\n",
        "n_column_name = \"VISITORS\",\n",
        "metric_type = 'rate',\n",
        "pooled = True,\n",
        "one_tailed = False,\n",
        "ci_level = 1-SIGNIFICANCE,\n",
        "positive_good = True,\n",
        "rounding = 4\n",
        ")\n",
        "\n",
        "display(couponclips_cvr_test[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CC CVR\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "couponclips_cvr_test[1].show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Search CVR\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "search_cvr_test = hypothesis_test_compare_means(\n",
        "df = base_df,\n",
        "variant_column_name = \"VARIANT_ID\",\n",
        "control_variant_name = control_variant_nm,\n",
        "mean_column_name = \"UNIQUE_USERS_THAT_SEARCH\",\n",
        "std_column_name = \"\",\n",
        "n_column_name = \"VISITORS\",\n",
        "metric_type = 'rate',\n",
        "pooled = True,\n",
        "one_tailed = False,\n",
        "ci_level = 1-SIGNIFICANCE,\n",
        "positive_good = True,\n",
        "rounding = 4\n",
        ")\n",
        "\n",
        "display(search_cvr_test[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SRCH CVR\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "search_cvr_test[1].show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Clips\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Total Clips (LOY_CLIPS)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "clipping_count = hypothesis_test_compare_means(\n",
        "df = clips_df,\n",
        "variant_column_name = \"VARIANT_ID\",\n",
        "control_variant_name = control_variant_nm,\n",
        "mean_column_name = \"CLIPS_MEAN\",\n",
        "std_column_name = \"CLIPS_SD\",\n",
        "n_column_name = \"VISITORS\",\n",
        "metric_type = \"mean\",\n",
        "pooled = False,\n",
        "one_tailed = False,\n",
        "ci_level = 1-SIGNIFICANCE,\n",
        "positive_good = True,\n",
        "rounding = 3\n",
        ")\n",
        "\n",
        "display(clipping_count[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LOY CLIPS\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "clipping_count[1].show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Store Coupon Clips\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sc_clips = hypothesis_test_compare_means(\n",
        "df = clips_df,\n",
        "variant_column_name = \"VARIANT_ID\",\n",
        "control_variant_name = control_variant_nm,\n",
        "mean_column_name = \"sc_clips_MEAN\",\n",
        "std_column_name = \"sc_clips_SD\",\n",
        "n_column_name = \"VISITORS\",\n",
        "metric_type = \"mean\",\n",
        "pooled = False,\n",
        "one_tailed = False,\n",
        "ci_level = 1-SIGNIFICANCE,\n",
        "positive_good = True,\n",
        "rounding = 3\n",
        ")\n",
        "\n",
        "display(sc_clips[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# STORE CLIPS\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sc_clips[1].show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Manufacturer Coupon Clips\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mf_clips = hypothesis_test_compare_means(\n",
        "df = clips_df,\n",
        "variant_column_name = \"VARIANT_ID\",\n",
        "control_variant_name = control_variant_nm,\n",
        "mean_column_name = \"mf_clips_MEAN\",\n",
        "std_column_name = \"mf_clips_SD\",\n",
        "n_column_name = \"VISITORS\",\n",
        "metric_type = \"mean\",\n",
        "pooled = False,\n",
        "one_tailed = False,\n",
        "ci_level = 1-SIGNIFICANCE,\n",
        "positive_good = True,\n",
        "rounding = 3\n",
        ")\n",
        "\n",
        "display(mf_clips[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MANUFACTURE CLIPS\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mf_clips[1].show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Grocery Reward Clips\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gr_clips = hypothesis_test_compare_means(\n",
        "df = clips_df,\n",
        "variant_column_name = \"VARIANT_ID\",\n",
        "control_variant_name = control_variant_nm,\n",
        "mean_column_name = \"gr_clips_MEAN\",\n",
        "std_column_name = \"gr_clips_SD\",\n",
        "n_column_name = \"VISITORS\",\n",
        "metric_type = \"mean\",\n",
        "pooled = False,\n",
        "one_tailed = False,\n",
        "ci_level = 1-SIGNIFICANCE,\n",
        "positive_good = True,\n",
        "rounding = 3\n",
        ")\n",
        "\n",
        "display(gr_clips[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GROCERY REWARD CLIPS\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gr_clips[1].show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Personalized Deals Clips\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pd_clips = hypothesis_test_compare_means(\n",
        "df = clips_df,\n",
        "variant_column_name = \"VARIANT_ID\",\n",
        "control_variant_name = control_variant_nm,\n",
        "mean_column_name = \"pd_clips_MEAN\",\n",
        "std_column_name = \"pd_clips_SD\",\n",
        "n_column_name = \"VISITORS\",\n",
        "metric_type = \"mean\",\n",
        "pooled = False,\n",
        "one_tailed = False,\n",
        "ci_level = 1-SIGNIFICANCE,\n",
        "positive_good = True,\n",
        "rounding = 3\n",
        ")\n",
        "\n",
        "display(pd_clips[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PD CLIPS\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pd_clips[1].show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Special-Personalized Deals Clips\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "spd_clips = hypothesis_test_compare_means(\n",
        "df = clips_df,\n",
        "variant_column_name = \"VARIANT_ID\",\n",
        "control_variant_name = control_variant_nm,\n",
        "mean_column_name = \"spd_clips_MEAN\",\n",
        "std_column_name = \"spd_clips_SD\",\n",
        "n_column_name = \"VISITORS\",\n",
        "metric_type = \"mean\",\n",
        "pooled = False,\n",
        "one_tailed = False,\n",
        "ci_level = 1-SIGNIFICANCE,\n",
        "positive_good = True,\n",
        "rounding = 3\n",
        ")\n",
        "\n",
        "display(spd_clips[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SPD CLIPS\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "spd_clips[1].show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PZN-Personalized Deals Clips\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    pzn_clips = hypothesis_test_compare_means(\n",
        "    df = clips_df,\n",
        "    variant_column_name = \"VARIANT_ID\",\n",
        "    control_variant_name = control_variant_nm,\n",
        "    mean_column_name = \"pzn_clips_MEAN\",\n",
        "    std_column_name = \"pzn_clips_SD\",\n",
        "    n_column_name = \"VISITORS\",\n",
        "    metric_type = \"mean\",\n",
        "    pooled = False,\n",
        "    one_tailed = False,\n",
        "    ci_level = 1-SIGNIFICANCE,\n",
        "    positive_good = True,\n",
        "    rounding = 3\n",
        "    )\n",
        "\n",
        "    display(pzn_clips[0])\n",
        "except:\n",
        "    print('No PZN deals')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PZN PD CLIPS\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    pzn_clips[1].show()\n",
        "except:\n",
        "    print('No PZN deals')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Redemptions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Unique Redeemers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "redeeming_count = hypothesis_test_compare_means(\n",
        "df = redemptions_df,\n",
        "variant_column_name = \"VARIANT_ID\",\n",
        "control_variant_name = control_variant_nm,\n",
        "mean_column_name = \"RPO_COMBINED_MEAN\",\n",
        "std_column_name = \"RPO_ECOMBINED_SD\",\n",
        "n_column_name = \"VISITORS\",\n",
        "metric_type = \"mean\",\n",
        "pooled = False,\n",
        "one_tailed = False,\n",
        "ci_level = 1-SIGNIFICANCE,\n",
        "positive_good = True,\n",
        "rounding = 3\n",
        ")\n",
        "\n",
        "display(redeeming_count[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# UNIQUE REDEEMERS\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "redeeming_count[1].show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Average Total Redemptions Per Customer Ecomm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "coupon_redemptions_ecomm_test = hypothesis_test_compare_means(\n",
        "df = ecomm_agg_df,\n",
        "variant_column_name = \"VARIANT_ID\",\n",
        "control_variant_name = control_variant_nm,\n",
        "mean_column_name = \"ECOMM_REDEMPTIONS_MEAN\",\n",
        "std_column_name = \"ECOMM_REDEMPTIONS_SD\",\n",
        "n_column_name = \"VISITORS\",\n",
        "metric_type = \"mean\",\n",
        "pooled = False,\n",
        "one_tailed = False,\n",
        "ci_level = 1-SIGNIFICANCE,\n",
        "positive_good = True,\n",
        "rounding = 3\n",
        ")\n",
        "\n",
        "display(coupon_redemptions_ecomm_test[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Ecomm Redemptions Per Customer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "coupon_redemptions_ecomm_test[1].show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Average Total Redemptions Per Customer In-Store\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "coupon_redemptions_store_test = hypothesis_test_compare_means(\n",
        "df = store_agg_df,\n",
        "variant_column_name = \"VARIANT_ID\",\n",
        "control_variant_name = control_variant_nm,\n",
        "mean_column_name = \"STORE_REDEMPTIONS_MEAN\",\n",
        "std_column_name = \"STORE_REDEMPTIONS_SD\",\n",
        "n_column_name = \"VISITORS\",\n",
        "metric_type = \"mean\",\n",
        "pooled = False,\n",
        "one_tailed = False,\n",
        "ci_level = 1-SIGNIFICANCE,\n",
        "positive_good = True,\n",
        "rounding = 3\n",
        ")\n",
        "\n",
        "display(coupon_redemptions_store_test[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# In Store Redemptions Per Customer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "coupon_redemptions_store_test[1].show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Store Coupon Redemptions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sc_redemptions = hypothesis_test_compare_means(\n",
        "df = redemptions_df,\n",
        "variant_column_name = \"VARIANT_ID\",\n",
        "control_variant_name = control_variant_nm,\n",
        "mean_column_name = \"sc_redemptions_MEAN\",\n",
        "std_column_name = \"sc_redemptions_SD\",\n",
        "n_column_name = \"VISITORS\",\n",
        "metric_type = \"mean\",\n",
        "pooled = False,\n",
        "one_tailed = False,\n",
        "ci_level = 1-SIGNIFICANCE,\n",
        "positive_good = True,\n",
        "rounding = 3\n",
        ")\n",
        "\n",
        "display(sc_redemptions[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# STORE COUPON REDEMPTIONS\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sc_redemptions[1].show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Average Manufacturer Coupon Redemptions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mf_redemptions = hypothesis_test_compare_means(\n",
        "df = redemptions_df,\n",
        "variant_column_name = \"VARIANT_ID\",\n",
        "control_variant_name = control_variant_nm,\n",
        "mean_column_name = \"mf_redemptions_MEAN\",\n",
        "std_column_name = \"mf_redemptions_SD\",\n",
        "n_column_name = \"VISITORS\",\n",
        "metric_type = \"mean\",\n",
        "pooled = False,\n",
        "one_tailed = False,\n",
        "ci_level = 1-SIGNIFICANCE,\n",
        "positive_good = True,\n",
        "rounding = 3\n",
        ")\n",
        "\n",
        "display(mf_redemptions[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MANUFACTURER COUPON REDEMPTIONS\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mf_redemptions[1].show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Average Grocery Reward Redemptions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gr_redemptions = hypothesis_test_compare_means(\n",
        "df = redemptions_df,\n",
        "variant_column_name = \"VARIANT_ID\",\n",
        "control_variant_name = control_variant_nm,\n",
        "mean_column_name = \"gr_redemptions_MEAN\",\n",
        "std_column_name = \"gr_redemptions_SD\",\n",
        "n_column_name = \"VISITORS\",\n",
        "metric_type = \"mean\",\n",
        "pooled = False,\n",
        "one_tailed = False,\n",
        "ci_level = 1-SIGNIFICANCE,\n",
        "positive_good = True,\n",
        "rounding = 3\n",
        ")\n",
        "\n",
        "display(gr_redemptions[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GROCERY REWARDS REDEMPTIONS\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gr_redemptions[1].show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Average Personalized Deals Redemptions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pd_redemptions = hypothesis_test_compare_means(\n",
        "df = redemptions_df,\n",
        "variant_column_name = \"VARIANT_ID\",\n",
        "control_variant_name = control_variant_nm,\n",
        "mean_column_name = \"pd_redemptions_MEAN\",\n",
        "std_column_name = \"pd_redemptions_SD\",\n",
        "n_column_name = \"VISITORS\",\n",
        "metric_type = \"mean\",\n",
        "pooled = False,\n",
        "one_tailed = False,\n",
        "ci_level = 1-SIGNIFICANCE,\n",
        "positive_good = True,\n",
        "rounding = 3\n",
        ")\n",
        "\n",
        "display(pd_redemptions[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PD REDEMPTIONS\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pd_redemptions[1].show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Special-Personalized Deals Redemptions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "spd_redemptions = hypothesis_test_compare_means(\n",
        "df = redemptions_df,\n",
        "variant_column_name = \"VARIANT_ID\",\n",
        "control_variant_name = control_variant_nm,\n",
        "mean_column_name = \"spd_redemptions_MEAN\",\n",
        "std_column_name = \"spd_redemptions_SD\",\n",
        "n_column_name = \"VISITORS\",\n",
        "metric_type = \"mean\",\n",
        "pooled = False,\n",
        "one_tailed = False,\n",
        "ci_level = 1-SIGNIFICANCE,\n",
        "positive_good = True,\n",
        "rounding = 3\n",
        ")\n",
        "\n",
        "display(spd_redemptions[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SPD Redemptions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "spd_redemptions[1].show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PZN-Personalized Deals Redemptions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    pzn_redemptions = hypothesis_test_compare_means(\n",
        "    df = redemptions_df,\n",
        "    variant_column_name = \"VARIANT_ID\",\n",
        "    control_variant_name = control_variant_nm,\n",
        "    mean_column_name = \"pzn_redemptions_MEAN\",\n",
        "    std_column_name = \"pzn_redemptions_SD\",\n",
        "    n_column_name = \"VISITORS\",\n",
        "    metric_type = \"mean\",\n",
        "    pooled = False,\n",
        "    one_tailed = False,\n",
        "    ci_level = 1-SIGNIFICANCE,\n",
        "    positive_good = True,\n",
        "    rounding = 3\n",
        "    )\n",
        "\n",
        "    display(pzn_redemptions[0])\n",
        "except:\n",
        "    print('No PZN deals')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PZN PD REDEMPTIONS\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    pzn_redemptions[1].show()\n",
        "except:\n",
        "    print('No PZN deals')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Categories\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Categories Breadth Test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "CB_test = hypothesis_test_compare_means(\n",
        "df = category_breadth_depth_df,\n",
        "variant_column_name = \"VARIANT_ID\",\n",
        "control_variant_name = \"VARIANT A\",\n",
        "mean_column_name = \"CATEGORIES_MEAN\",\n",
        "std_column_name = \"CATEGORIES_SD\",\n",
        "n_column_name = \"UNIQUE_HOUSEHOLDS\",\n",
        "metric_type = 'mean',\n",
        "pooled = False,\n",
        "one_tailed = False,\n",
        "ci_level = 1-SIGNIFICANCE,\n",
        "positive_good = True,\n",
        "rounding = 4\n",
        ")\n",
        "\n",
        "display(CB_test[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "CB_test[1].show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Category Depth Test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "CD_test = hypothesis_test_compare_means(\n",
        "df = category_breadth_depth_df,\n",
        "variant_column_name = \"VARIANT_ID\",\n",
        "control_variant_name = \"VARIANT A\",\n",
        "mean_column_name = [\"ITEMS_MEAN\",\"CATEGORIES_MEAN\"],\n",
        "std_column_name = [\"ITEMS_SD\",\"CATEGORIES_SD\"],\n",
        "n_column_name = \"UNIQUE_HOUSEHOLDS\",\n",
        "cov_column_name = \"CD_COV_ITEMS_CATEGORIES\",\n",
        "metric_type = 'ratio',\n",
        "pooled = False,\n",
        "one_tailed = False,\n",
        "ci_level = 1-SIGNIFICANCE,\n",
        "positive_good = True,\n",
        "rounding = 4\n",
        ")\n",
        "\n",
        "display(CD_test[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "CD_test[1].show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Account Health\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# E-mail Test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "acc_email_test = hypothesis_test_compare_means(\n",
        "df = account_health_agg_df,\n",
        "variant_column_name = \"VARIANT_ID\",\n",
        "control_variant_name = control_variant_nm,\n",
        "mean_column_name = \"email_count\",\n",
        "std_column_name = \"\",\n",
        "n_column_name = \"UNIQUE_HOUSEHOLDS\",\n",
        "metric_type = 'rate',\n",
        "pooled = False,\n",
        "one_tailed = False,\n",
        "ci_level = 1-SIGNIFICANCE,\n",
        "positive_good = True,\n",
        "rounding = 4\n",
        ")\n",
        "\n",
        "display(acc_email_test[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "acc_email_test[1].show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Phone Test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "acc_phone_test = hypothesis_test_compare_means(\n",
        "df = account_health_agg_df,\n",
        "variant_column_name = \"VARIANT_ID\",\n",
        "control_variant_name = control_variant_nm,\n",
        "mean_column_name = \"phone_count\",\n",
        "std_column_name = \"\",\n",
        "n_column_name = \"UNIQUE_HOUSEHOLDS\",\n",
        "metric_type = 'rate',\n",
        "pooled = False,\n",
        "one_tailed = False,\n",
        "ci_level = 1-SIGNIFICANCE,\n",
        "positive_good = True,\n",
        "rounding = 4\n",
        ")\n",
        "\n",
        "display(acc_phone_test[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "acc_phone_test[1].show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Reachability Test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "acc_reachablity_test = hypothesis_test_compare_means(\n",
        "df = account_health_agg_df,\n",
        "variant_column_name = \"VARIANT_ID\",\n",
        "control_variant_name = control_variant_nm,\n",
        "mean_column_name = \"reachablity_score\",\n",
        "std_column_name = \"\",\n",
        "n_column_name = \"UNIQUE_HOUSEHOLDS\",\n",
        "metric_type = 'rate',\n",
        "pooled = False,\n",
        "one_tailed = False,\n",
        "ci_level = 1-SIGNIFICANCE,\n",
        "positive_good = True,\n",
        "rounding = 4\n",
        ")\n",
        "\n",
        "display(acc_reachablity_test[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "acc_reachablity_test[1].show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Name Fill Test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "acc_namefill_test = hypothesis_test_compare_means(\n",
        "df = account_health_agg_df,\n",
        "variant_column_name = \"VARIANT_ID\",\n",
        "control_variant_name = control_variant_nm,\n",
        "mean_column_name = \"fn_ln_count\",\n",
        "std_column_name = \"\",\n",
        "n_column_name = \"UNIQUE_HOUSEHOLDS\",\n",
        "metric_type = 'rate',\n",
        "pooled = False,\n",
        "one_tailed = False,\n",
        "ci_level = 1-SIGNIFICANCE,\n",
        "positive_good = True,\n",
        "rounding = 4\n",
        ")\n",
        "\n",
        "display(acc_namefill_test[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "acc_namefill_test[1].show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Birth Date Test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "acc_bd_test = hypothesis_test_compare_means(\n",
        "df = account_health_agg_df,\n",
        "variant_column_name = \"VARIANT_ID\",\n",
        "control_variant_name = control_variant_nm,\n",
        "mean_column_name = \"bday_count\",\n",
        "std_column_name = \"\",\n",
        "n_column_name = \"UNIQUE_HOUSEHOLDS\",\n",
        "metric_type = 'rate',\n",
        "pooled = False,\n",
        "one_tailed = False,\n",
        "ci_level = 1-SIGNIFICANCE,\n",
        "positive_good = True,\n",
        "rounding = 4\n",
        ")\n",
        "\n",
        "display(acc_bd_test[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "acc_bd_test[1].show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Address Fill Test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "acc_address_test = hypothesis_test_compare_means(\n",
        "df = account_health_agg_df,\n",
        "variant_column_name = \"VARIANT_ID\",\n",
        "control_variant_name = control_variant_nm,\n",
        "mean_column_name = \"address_count\",\n",
        "std_column_name = \"\",\n",
        "n_column_name = \"UNIQUE_HOUSEHOLDS\",\n",
        "metric_type = 'rate',\n",
        "pooled = False,\n",
        "one_tailed = False,\n",
        "ci_level = 1-SIGNIFICANCE,\n",
        "positive_good = True,\n",
        "rounding = 4\n",
        ")\n",
        "\n",
        "display(acc_address_test[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "acc_address_test[1].show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Health Score Test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "acc_health_test = hypothesis_test_compare_means(\n",
        "df = account_health_agg_df,\n",
        "variant_column_name = \"VARIANT_ID\",\n",
        "control_variant_name = control_variant_nm,\n",
        "mean_column_name = \"health_score\",\n",
        "std_column_name = \"stdev_health_score\",\n",
        "n_column_name = \"UNIQUE_HOUSEHOLDS\",\n",
        "metric_type = 'mean',\n",
        "pooled = False,\n",
        "one_tailed = False,\n",
        "ci_level = 1-SIGNIFICANCE,\n",
        "positive_good = True,\n",
        "rounding = 4\n",
        ")\n",
        "display(acc_health_test[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "acc_health_test[1].show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Markdown\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Average Store Coupon Markdown Per Customer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sc_mkdn = hypothesis_test_compare_means(\n",
        "df = redemptions_df,\n",
        "variant_column_name = \"VARIANT_ID\",\n",
        "control_variant_name = control_variant_nm,\n",
        "mean_column_name = \"sc_MKDN_MEAN\",\n",
        "std_column_name = \"sc_MKDN_SD\",\n",
        "n_column_name = \"VISITORS\",\n",
        "metric_type = \"mean\",\n",
        "pooled = False,\n",
        "one_tailed = False,\n",
        "ci_level = 1-SIGNIFICANCE,\n",
        "positive_good = True,\n",
        "rounding = 3\n",
        ")\n",
        "\n",
        "display(sc_mkdn[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# STORE COUPON MARKDOWN PER CUSTOMER\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sc_mkdn[1].show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Average Manufacturer Coupon Markdown Per Customer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mf_mkdn = hypothesis_test_compare_means(\n",
        "df = redemptions_df,\n",
        "variant_column_name = \"VARIANT_ID\",\n",
        "control_variant_name = control_variant_nm,\n",
        "mean_column_name = \"mf_MKDN_MEAN\",\n",
        "std_column_name = \"mf_MKDN_SD\",\n",
        "n_column_name = \"VISITORS\",\n",
        "metric_type = \"mean\",\n",
        "pooled = False,\n",
        "one_tailed = False,\n",
        "ci_level = 1-SIGNIFICANCE,\n",
        "positive_good = True,\n",
        "rounding = 3\n",
        ")\n",
        "\n",
        "display(mf_mkdn[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MANUFACTURER MARKDOWN\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mf_mkdn[1].show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Average Grocery Rewards Markdown Per Customer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gr_mkdn = hypothesis_test_compare_means(\n",
        "df = redemptions_df,\n",
        "variant_column_name = \"VARIANT_ID\",\n",
        "control_variant_name = control_variant_nm,\n",
        "mean_column_name = \"gr_MKDN_MEAN\",\n",
        "std_column_name = \"gr_MKDN_SD\",\n",
        "n_column_name = \"VISITORS\",\n",
        "metric_type = \"mean\",\n",
        "pooled = False,\n",
        "one_tailed = False,\n",
        "ci_level = 1-SIGNIFICANCE,\n",
        "positive_good = True,\n",
        "rounding = 3\n",
        ")\n",
        "\n",
        "display(gr_mkdn[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GROCERY REWARDS MARKDOWN\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gr_mkdn[1].show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Average Personalized Deals Markdown Per Customer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pd_mkdn = hypothesis_test_compare_means(\n",
        "df = redemptions_df,\n",
        "variant_column_name = \"VARIANT_ID\",\n",
        "control_variant_name = control_variant_nm,\n",
        "mean_column_name = \"pd_MKDN_MEAN\",\n",
        "std_column_name = \"pd_MKDN_SD\",\n",
        "n_column_name = \"VISITORS\",\n",
        "metric_type = \"mean\",\n",
        "pooled = False,\n",
        "one_tailed = False,\n",
        "ci_level = 1-SIGNIFICANCE,\n",
        "positive_good = True,\n",
        "rounding = 3\n",
        ")\n",
        "\n",
        "display(pd_mkdn[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PD DEALS MARKDOWN\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pd_mkdn[1].show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Average Special-Personalized Deals Markdown Per Customer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "spd_mkdn = hypothesis_test_compare_means(\n",
        "df = redemptions_df,\n",
        "variant_column_name = \"VARIANT_ID\",\n",
        "control_variant_name = control_variant_nm,\n",
        "mean_column_name = \"spd_MKDN_MEAN\",\n",
        "std_column_name = \"spd_MKDN_SD\",\n",
        "n_column_name = \"VISITORS\",\n",
        "metric_type = \"mean\",\n",
        "pooled = False,\n",
        "one_tailed = False,\n",
        "ci_level = 1-SIGNIFICANCE,\n",
        "positive_good = True,\n",
        "rounding = 3\n",
        ")\n",
        "\n",
        "display(spd_mkdn[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SPD DEALS MARKDOWN\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "spd_mkdn[1].show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Average PZN-Personalized Deals Markdown Per Customer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    pzn_mkdn = hypothesis_test_compare_means(\n",
        "    df = combined_df,\n",
        "    variant_column_name = \"VARIANT_ID\",\n",
        "    control_variant_name = control_variant_nm,\n",
        "    mean_column_name = \"pzn_MKDN_MEAN\",\n",
        "    std_column_name = \"pzn_MKDN_SD\",\n",
        "    n_column_name = \"VISITORS\",\n",
        "    metric_type = \"mean\",\n",
        "    pooled = False,\n",
        "    one_tailed = False,\n",
        "    ci_level = 1-SIGNIFICANCE,\n",
        "    positive_good = True,\n",
        "    rounding = 3\n",
        "    )\n",
        "\n",
        "    display(pzn_mkdn[0])\n",
        "except:\n",
        "    print('No PZN deals')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PZN PD DEALS MARKDOWN\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    pzn_mkdn[1].show()\n",
        "except:\n",
        "    print('No PZN deals')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Ecomm Metrics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Ecomm Revenue Per Customer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ecomm_revenue_test = hypothesis_test_compare_means(\n",
        "df = ecomm_agg_df,\n",
        "variant_column_name = \"VARIANT_ID\",\n",
        "control_variant_name = control_variant_nm,\n",
        "mean_column_name = \"ECOMM_REVENUE_MEAN\",\n",
        "std_column_name = \"ECOMM_REVENUE_SD\",\n",
        "n_column_name = \"VISITORS\",\n",
        "metric_type = \"mean\",\n",
        "pooled = False,\n",
        "one_tailed = False,\n",
        "ci_level = 1-SIGNIFICANCE,\n",
        "positive_good = True,\n",
        "rounding = 3\n",
        ")\n",
        "\n",
        "display(ecomm_revenue_test[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ECOMM RPC\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ecomm_revenue_test[1].show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Ecomm Non-Zero Revenue Per Customer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## ECOMM non-zero Revenue\n",
        "ecomm_nonzero_revenue_test = hypothesis_test_compare_means(\n",
        "df = ecomm_agg_df,\n",
        "variant_column_name = \"VARIANT_ID\",\n",
        "control_variant_name = control_variant_nm,\n",
        "mean_column_name = \"ECOMM_REVENUE_NONZERO_MEAN\",\n",
        "std_column_name = \"ECOMM_REVENUE_NONZERO_SD\",\n",
        "n_column_name = \"VISITORS\",\n",
        "metric_type = \"mean\",\n",
        "pooled = False,\n",
        "one_tailed = False,\n",
        "ci_level = 1-SIGNIFICANCE,\n",
        "positive_good = True,\n",
        "rounding = 3\n",
        ")\n",
        "\n",
        "display(ecomm_nonzero_revenue_test[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ECOMM NONZERO RPC\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ecomm_nonzero_revenue_test[1].show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Ecomm Order CVR\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## E-Com Order Conversion Rate\n",
        "ecomm_orders_cvr_test = hypothesis_test_compare_means(\n",
        "df = ecomm_agg_df,\n",
        "variant_column_name = \"VARIANT_ID\",\n",
        "control_variant_name = control_variant_nm,\n",
        "mean_column_name = \"PURCHASING_CUSTOMERS\",\n",
        "std_column_name = \"\",\n",
        "n_column_name = \"VISITORS\",\n",
        "metric_type = 'rate',\n",
        "pooled = True,\n",
        "one_tailed = False,\n",
        "ci_level = 1-SIGNIFICANCE,\n",
        "positive_good = True,\n",
        "rounding = 4\n",
        ")\n",
        "\n",
        "display(ecomm_orders_cvr_test[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ECOMM ORDER CVR\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ecomm_orders_cvr_test[1].show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Ecomm TXNs Per Customer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "ecomm_orders_test = hypothesis_test_compare_means(\n",
        "df = ecomm_agg_df,\n",
        "variant_column_name = \"VARIANT_ID\",\n",
        "control_variant_name = control_variant_nm,\n",
        "mean_column_name = \"ECOMM_ORDERS_MEAN\",\n",
        "std_column_name = \"ECOMM_ORDERS_SD\",\n",
        "n_column_name = \"VISITORS\",\n",
        "metric_type = \"mean\",\n",
        "pooled = False,\n",
        "one_tailed = False,\n",
        "ci_level = 1-SIGNIFICANCE,\n",
        "positive_good = True,\n",
        "rounding = 3\n",
        ")\n",
        "\n",
        "display(ecomm_orders_test[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ECOMM TXNS PER CUSTOMER\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ecomm_orders_test[1].show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Ecomm Units Per Customer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ecomm_units_test = hypothesis_test_compare_means(\n",
        "df = ecomm_agg_df,\n",
        "variant_column_name = \"VARIANT_ID\",\n",
        "control_variant_name = control_variant_nm,\n",
        "mean_column_name = \"ECOMM_UNITS_MEAN\",\n",
        "std_column_name = \"ECOMM_UNITS_SD\",\n",
        "n_column_name = \"VISITORS\",\n",
        "metric_type = \"mean\",\n",
        "pooled = False,\n",
        "one_tailed = False,\n",
        "ci_level = 1-SIGNIFICANCE,\n",
        "positive_good = True,\n",
        "rounding = 3\n",
        ")\n",
        "\n",
        "display(ecomm_units_test[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ECOMM UNITS PER CUSTOMER\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ecomm_units_test[1].show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Ecomm Average Order Value (AOV)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ecomm_AOV_test = hypothesis_test_compare_means(\n",
        "df = ecomm_agg_df,\n",
        "variant_column_name = \"VARIANT_ID\",\n",
        "control_variant_name = control_variant_nm,\n",
        "mean_column_name = [\"ECOMM_REVENUE_MEAN\",\"ECOMM_ORDERS_MEAN\"],\n",
        "std_column_name = [\"ECOMM_REVENUE_SD\",\"ECOMM_ORDERS_SD\"],\n",
        "n_column_name = \"VISITORS\",\n",
        "cov_column_name = \"ECOMM_COV_REVENUE_ORDERS\",\n",
        "metric_type = 'ratio',\n",
        "pooled = False,\n",
        "one_tailed = False,\n",
        "ci_level = 1-SIGNIFICANCE,\n",
        "positive_good = True,\n",
        "rounding = 4\n",
        ")\n",
        "\n",
        "display(ecomm_AOV_test[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ECOMM AOV\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ecomm_AOV_test[1].show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "## ECOMM non-zero aov\n",
        "ecomm_AOV_nonzero_test = hypothesis_test_compare_means(\n",
        "df = ecomm_agg_df,\n",
        "variant_column_name = \"VARIANT_ID\",\n",
        "control_variant_name = control_variant_nm,\n",
        "mean_column_name = [\"ECOMM_REVENUE_NONZERO_MEAN\",\"ECOMM_ORDERS_MEAN\"],\n",
        "std_column_name = [\"ECOMM_REVENUE_NONZERO_SD\",\"ECOMM_ORDERS_SD\"],\n",
        "n_column_name = \"VISITORS\",\n",
        "cov_column_name = \"ECOMM_COV_REVENUE_ORDERS\",\n",
        "metric_type = 'ratio',\n",
        "pooled = False,\n",
        "one_tailed = False,\n",
        "ci_level = 1-SIGNIFICANCE,\n",
        "positive_good = True,\n",
        "rounding = 4\n",
        ")\n",
        "\n",
        "display(ecomm_AOV_nonzero_test[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "ecomm_AOV_nonzero_test[1].show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Ecomm Units Per Order (Basket Size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ecomm_UPO_test = hypothesis_test_compare_means(\n",
        "df = ecomm_agg_df,\n",
        "variant_column_name = \"VARIANT_ID\",\n",
        "control_variant_name = control_variant_nm,\n",
        "mean_column_name = [\"ECOMM_UNITS_MEAN\",\"ECOMM_ORDERS_MEAN\"],\n",
        "std_column_name = [\"ECOMM_UNITS_SD\",\"ECOMM_ORDERS_SD\"],\n",
        "n_column_name = \"VISITORS\",\n",
        "cov_column_name = \"ECOMM_COV_UNITS_ORDERS\",\n",
        "metric_type = \"ratio\",\n",
        "pooled = False,\n",
        "one_tailed = False,\n",
        "ci_level = 1-SIGNIFICANCE,\n",
        "positive_good = True,\n",
        "rounding = 4\n",
        ")\n",
        "\n",
        "display(ecomm_UPO_test[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ECOMM UPO\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ecomm_UPO_test[1].show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# E-Comm Average Markdown Per Customer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ecomm_markdown_pc_test = hypothesis_test_compare_means(\n",
        "df = ecomm_agg_df,\n",
        "variant_column_name = \"VARIANT_ID\",\n",
        "control_variant_name = control_variant_nm,\n",
        "mean_column_name = \"ECOMM_MKDN_MEAN\",\n",
        "std_column_name = \"ECOMM_MKDN_SD\",\n",
        "n_column_name = \"VISITORS\",\n",
        "metric_type = 'mean',\n",
        "pooled = True,\n",
        "one_tailed = False,\n",
        "ci_level = 1-SIGNIFICANCE,\n",
        "positive_good = True,\n",
        "rounding = 4\n",
        ")\n",
        "\n",
        "display(ecomm_markdown_pc_test[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ECOMM MKDN PER CUSTOMER\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ecomm_markdown_pc_test[1].show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# E-Comm BNC Test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ecomm_bnc_test = hypothesis_test_compare_means(\n",
        "df = ecomm_agg_df,\n",
        "variant_column_name = \"VARIANT_ID\",\n",
        "control_variant_name = control_variant_nm,\n",
        "mean_column_name = \"ECOMM_BNC_TOTAL\",\n",
        "std_column_name = \"ECOMM_BNC_TOTAL_SD\",\n",
        "n_column_name = \"VISITORS\",\n",
        "metric_type = 'mean',\n",
        "pooled = True,\n",
        "one_tailed = False,\n",
        "ci_level = 1-SIGNIFICANCE,\n",
        "positive_good = True,\n",
        "rounding = 4\n",
        ")\n",
        "\n",
        "display(ecomm_bnc_test[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "ecomm_bnc_test[1].show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# E-Comm BNC CVR Test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ecomm_bnc_cvr_test = hypothesis_test_compare_means(\n",
        "df = ecomm_agg_df,\n",
        "variant_column_name = \"VARIANT_ID\",\n",
        "control_variant_name = control_variant_nm,\n",
        "mean_column_name = \"ECOMM_BNC_TOTAL\",\n",
        "std_column_name = \"\",\n",
        "n_column_name = \"VISITORS\",\n",
        "metric_type = 'rate',\n",
        "pooled = True,\n",
        "one_tailed = False,\n",
        "ci_level = 1-SIGNIFICANCE,\n",
        "positive_good = True,\n",
        "rounding = 4\n",
        ")\n",
        "\n",
        "display(ecomm_bnc_cvr_test[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# BNC CVR\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ecomm_bnc_cvr_test[1].show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# In-Store Metrics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# In-Store Revenue Per Customer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "store_revenue_test = hypothesis_test_compare_means(\n",
        "df = store_agg_df,\n",
        "variant_column_name = \"VARIANT_ID\",\n",
        "control_variant_name = control_variant_nm,\n",
        "mean_column_name = \"STORE_REVENUE_MEAN\",\n",
        "std_column_name = \"STORE_REVENUE_SD\",\n",
        "n_column_name = \"VISITORS\",\n",
        "metric_type = \"mean\",\n",
        "pooled = False,\n",
        "one_tailed = False,\n",
        "ci_level = 1-SIGNIFICANCE,\n",
        "positive_good = True,\n",
        "rounding = 3\n",
        ")\n",
        "\n",
        "display(store_revenue_test[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# In-Store RPC\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "store_revenue_test[1].show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# In-Store NonZero Revenue Per Customer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Store non-zero revenue\n",
        "store_revenue_nonzero_test = hypothesis_test_compare_means(\n",
        "df = store_agg_df,\n",
        "variant_column_name = \"VARIANT_ID\",\n",
        "control_variant_name = control_variant_nm,\n",
        "mean_column_name = \"STORE_REVENUE_NONZERO_MEAN\",\n",
        "std_column_name = \"STORE_REVENUE_NONZERO_SD\",\n",
        "n_column_name = \"VISITORS\",\n",
        "metric_type = \"mean\",\n",
        "pooled = False,\n",
        "one_tailed = False,\n",
        "ci_level = 1-SIGNIFICANCE,\n",
        "positive_good = True,\n",
        "rounding = 3\n",
        ")\n",
        "\n",
        "display(store_revenue_nonzero_test[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# IN-STORE NONZERO RPC\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "store_revenue_nonzero_test[1].show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# In-Store Order CVR\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## In-Store Order Conversion Rate\n",
        "store_orders_cvr_test = hypothesis_test_compare_means(\n",
        "df = store_agg_df,\n",
        "variant_column_name = \"VARIANT_ID\",\n",
        "control_variant_name = control_variant_nm,\n",
        "mean_column_name = \"PURCHASING_CUSTOMERS\",\n",
        "std_column_name = \"\",\n",
        "n_column_name = \"VISITORS\",\n",
        "metric_type = 'rate',\n",
        "pooled = True,\n",
        "one_tailed = False,\n",
        "ci_level = 1-SIGNIFICANCE,\n",
        "positive_good = True,\n",
        "rounding = 4\n",
        ")\n",
        "\n",
        "display(store_orders_cvr_test[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# IN-STORE ORDER CVR\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "store_orders_cvr_test[1].show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# In-Store TXNs Per Customer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "store_orders_test = hypothesis_test_compare_means(\n",
        "df = store_agg_df,\n",
        "variant_column_name = \"VARIANT_ID\",\n",
        "control_variant_name = control_variant_nm,\n",
        "mean_column_name = \"STORE_ORDERS_MEAN\",\n",
        "std_column_name = \"STORE_ORDERS_SD\",\n",
        "n_column_name = \"VISITORS\",\n",
        "metric_type = \"mean\",\n",
        "pooled = False,\n",
        "one_tailed = False,\n",
        "ci_level = 1-SIGNIFICANCE,\n",
        "positive_good = True,\n",
        "rounding = 3\n",
        ")\n",
        "\n",
        "display(store_orders_test[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# IN-STORE TXNs PER CUSTOMER\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "store_orders_test[1].show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# In-Store Average Units Sold Per Customer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "store_units_test = hypothesis_test_compare_means(\n",
        "df = store_agg_df,\n",
        "variant_column_name = \"VARIANT_ID\",\n",
        "control_variant_name = control_variant_nm,\n",
        "mean_column_name = \"STORE_UNITS_MEAN\",\n",
        "std_column_name = \"STORE_UNITS_SD\",\n",
        "n_column_name = \"VISITORS\",\n",
        "metric_type = \"mean\",\n",
        "pooled = False,\n",
        "one_tailed = False,\n",
        "ci_level = 1-SIGNIFICANCE,\n",
        "positive_good = True,\n",
        "rounding = 3\n",
        ")\n",
        "\n",
        "display(store_units_test[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# IN-STORE UNITS PER CUSTOMER\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "store_units_test[1].show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# In-Store Average Order Value (AOV)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "store_AOV_test = hypothesis_test_compare_means(\n",
        "df = store_agg_df,\n",
        "variant_column_name = \"VARIANT_ID\",\n",
        "control_variant_name = control_variant_nm,\n",
        "mean_column_name = [\"STORE_REVENUE_MEAN\",\"STORE_ORDERS_MEAN\"],\n",
        "std_column_name = [\"STORE_REVENUE_SD\",\"STORE_ORDERS_SD\"],\n",
        "n_column_name = \"VISITORS\",\n",
        "cov_column_name = \"STORE_COV_REVENUE_ORDERS\",\n",
        "metric_type = 'ratio',\n",
        "pooled = False,\n",
        "one_tailed = False,\n",
        "ci_level = 1-SIGNIFICANCE,\n",
        "positive_good = True,\n",
        "rounding = 4\n",
        ")\n",
        "\n",
        "display(store_AOV_test[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# IN-STORE AOV\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "store_AOV_test[1].show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "## Store non-zero aov\n",
        "\n",
        "store_AOV_nonzero_test = hypothesis_test_compare_means(\n",
        "df = store_agg_df,\n",
        "variant_column_name = \"VARIANT_ID\",\n",
        "control_variant_name = control_variant_nm,\n",
        "mean_column_name = [\"STORE_REVENUE_NONZERO_MEAN\",\"STORE_ORDERS_MEAN\"],\n",
        "std_column_name = [\"STORE_REVENUE_NONZERO_SD\",\"STORE_ORDERS_SD\"],\n",
        "n_column_name = \"VISITORS\",\n",
        "cov_column_name = \"STORE_COV_REVENUE_ORDERS\",\n",
        "metric_type = 'ratio',\n",
        "pooled = False,\n",
        "one_tailed = False,\n",
        "ci_level = 1-SIGNIFICANCE,\n",
        "positive_good = True,\n",
        "rounding = 4\n",
        ")\n",
        "\n",
        "display(store_AOV_nonzero_test[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "store_AOV_nonzero_test[1].show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# In-Store Average Units Per Order (Basket Size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "store_UPO_test = hypothesis_test_compare_means(\n",
        "df = store_agg_df,\n",
        "variant_column_name = \"VARIANT_ID\",\n",
        "control_variant_name = control_variant_nm,\n",
        "mean_column_name = [\"STORE_UNITS_MEAN\",\"STORE_ORDERS_MEAN\"],\n",
        "std_column_name = [\"STORE_UNITS_SD\",\"STORE_ORDERS_SD\"],\n",
        "n_column_name = \"VISITORS\",\n",
        "cov_column_name = \"STORE_COV_UNITS_ORDERS\",\n",
        "metric_type = \"ratio\",\n",
        "pooled = False,\n",
        "one_tailed = False,\n",
        "ci_level = 1-SIGNIFICANCE,\n",
        "positive_good = True,\n",
        "rounding = 4\n",
        ")\n",
        "\n",
        "display(store_UPO_test[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# IN-STORE UPO\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "store_UPO_test[1].show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# In-Store Average Markdown Per Customer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "store_markdown_pc_test = hypothesis_test_compare_means(\n",
        "df = store_agg_df,\n",
        "variant_column_name = \"VARIANT_ID\",\n",
        "control_variant_name = control_variant_nm,\n",
        "mean_column_name = \"STORE_MKDN_MEAN\",\n",
        "std_column_name = \"STORE_MKDN_SD\",\n",
        "n_column_name = \"VISITORS\",\n",
        "metric_type = 'mean',\n",
        "pooled = True,\n",
        "one_tailed = False,\n",
        "ci_level = 1-SIGNIFICANCE,\n",
        "positive_good = True,\n",
        "rounding = 4\n",
        ")\n",
        "\n",
        "display(store_markdown_pc_test[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# INSTORE MKDN PER CUSTOMER\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "store_markdown_pc_test[1].show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Basket Health\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Basket Health Mean Rate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bh_test = hypothesis_test_compare_means(\n",
        "df = basket_health_df,\n",
        "variant_column_name = \"VARIANT_ID\",\n",
        "control_variant_name = control_variant_nm,\n",
        "mean_column_name = \"BASKET_RATE_MEAN\",\n",
        "std_column_name = \"BASKET_RATE_SD\",\n",
        "n_column_name = \"UNIQUE_BASKET_HEALTH_HH\",\n",
        "metric_type = \"mean\",\n",
        "pooled = False,\n",
        "one_tailed = False,\n",
        "ci_level = 1-SIGNIFICANCE,\n",
        "positive_good = True,\n",
        "rounding = 3\n",
        ")\n",
        "\n",
        "display(bh_test[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Basket Health Mean Rate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bh_test[1].show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Basket Health (AB) Mean Rate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bh_ab_test = hypothesis_test_compare_means(\n",
        "df = basket_health_df,\n",
        "variant_column_name = \"VARIANT_ID\",\n",
        "control_variant_name = control_variant_nm,\n",
        "mean_column_name = \"AB_RATE_MEAN\",\n",
        "std_column_name = \"AB_RATE_SD\",\n",
        "n_column_name = \"UNIQUE_BASKET_HEALTH_HH\",\n",
        "metric_type = \"mean\",\n",
        "pooled = False,\n",
        "one_tailed = False,\n",
        "ci_level = 1-SIGNIFICANCE,\n",
        "positive_good = True,\n",
        "rounding = 3\n",
        ")\n",
        "\n",
        "display(bh_ab_test[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Basket Health (AB) Mean Rate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bh_ab_test[1].show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Basket Health (DE) Mean Rate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bh_de_test = hypothesis_test_compare_means(\n",
        "df = basket_health_df,\n",
        "variant_column_name = \"VARIANT_ID\",\n",
        "control_variant_name = control_variant_nm,\n",
        "mean_column_name = \"DE_RATE_MEAN\",\n",
        "std_column_name = \"DE_RATE_SD\",\n",
        "n_column_name = \"UNIQUE_BASKET_HEALTH_HH\",\n",
        "metric_type = \"mean\",\n",
        "pooled = False,\n",
        "one_tailed = False,\n",
        "ci_level = 1-SIGNIFICANCE,\n",
        "positive_good = True,\n",
        "rounding = 3\n",
        ")\n",
        "\n",
        "display(bh_de_test[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Basket Health (DE) Mean Rate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bh_de_test[1].show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# SNAP Metrics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Combined SNAP Tender Per Customer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "snap_test = hypothesis_test_compare_means(\n",
        "df = combined_df,\n",
        "variant_column_name = \"VARIANT_ID\",\n",
        "control_variant_name = control_variant_nm,\n",
        "mean_column_name = \"COMBINED_SNAP_MEAN\",\n",
        "std_column_name = \"COMBINED_SNAP_SD\",\n",
        "n_column_name = \"VISITORS\",\n",
        "metric_type = \"mean\",\n",
        "pooled = False,\n",
        "one_tailed = False,\n",
        "ci_level = 1-SIGNIFICANCE,\n",
        "positive_good = True,\n",
        "rounding = 3\n",
        ")\n",
        "\n",
        "display(snap_test[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SNAP PER CUSTOMER\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "snap_test[1].show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Gas Metrics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Gas Revenue Per Customer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gas_revenue_test = hypothesis_test_compare_means(\n",
        "df = gas_agg_df,\n",
        "variant_column_name = \"VARIANT_ID_GAS\",\n",
        "control_variant_name = control_variant_nm,\n",
        "mean_column_name = \"GAS_REVENUE_MEAN\",\n",
        "std_column_name = \"GAS_REVENUE_SD\",\n",
        "n_column_name = \"VISITORS\",\n",
        "metric_type = \"mean\",\n",
        "pooled = False,\n",
        "one_tailed = False,\n",
        "ci_level = 1-SIGNIFICANCE,\n",
        "positive_good = True,\n",
        "rounding = 3\n",
        ")\n",
        "\n",
        "display(gas_revenue_test[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GAS REVENUE PER CUSTOMER\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gas_revenue_test[1].show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Gas Markdown\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gas_mkdn_test = hypothesis_test_compare_means(\n",
        "df = gas_agg_df,\n",
        "variant_column_name = \"VARIANT_ID_GAS\",\n",
        "control_variant_name = control_variant_nm,\n",
        "mean_column_name = \"GAS_MKDN_MEAN\",\n",
        "std_column_name = \"GAS_MKDN_SD\",\n",
        "n_column_name = \"VISITORS\",\n",
        "metric_type = \"mean\",\n",
        "pooled = False,\n",
        "one_tailed = False,\n",
        "ci_level = 1-SIGNIFICANCE,\n",
        "positive_good = True,\n",
        "rounding = 3\n",
        ")\n",
        "\n",
        "display(gas_mkdn_test[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "gas_mkdn_test[1].show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### EMAIL/PUSH/SMS HYPOTHESIS TEST\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# email_optin_test = hypothesis_test_compare_means(\n",
        "# df = combined_df,\n",
        "# variant_column_name = \"VARIANT_ID\",\n",
        "# control_variant_name = control_variant_nm,\n",
        "# mean_column_name = \"EMAIL_OPTIN_COUNT\",\n",
        "# std_column_name = \"\",\n",
        "# n_column_name = \"VISITORS\",\n",
        "# metric_type = 'rate',\n",
        "# pooled = True,\n",
        "# one_tailed = False,\n",
        "# ci_level = 1-SIGNIFICANCE,\n",
        "# positive_good = True,\n",
        "# rounding = 4\n",
        "# )\n",
        "\n",
        "# display(email_optin_test[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# email_optin_test = hypothesis_test_compare_means(\n",
        "# df = combined_df,\n",
        "# variant_column_name = \"VARIANT_ID\",\n",
        "# control_variant_name = control_variant_nm,\n",
        "# mean_column_name = \"SMS_OPTIN_COUNT\",\n",
        "# std_column_name = \"\",\n",
        "# n_column_name = \"VISITORS\",\n",
        "# metric_type = 'rate',\n",
        "# pooled = True,\n",
        "# one_tailed = False,\n",
        "# ci_level = 1-SIGNIFICANCE,\n",
        "# positive_good = True,\n",
        "# rounding = 4\n",
        "# )\n",
        "\n",
        "# display(email_optin_test[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# push_enabled_test = hypothesis_test_compare_means(\n",
        "# df = combined_df,\n",
        "# variant_column_name = \"VARIANT_ID\",\n",
        "# control_variant_name = control_variant_nm,\n",
        "# mean_column_name = \"PUSH_ENABLED_COUNT\",\n",
        "# std_column_name = \"\",\n",
        "# n_column_name = \"VISITORS\",\n",
        "# metric_type = 'rate',\n",
        "# pooled = True,\n",
        "# one_tailed = False,\n",
        "# ci_level = 1-SIGNIFICANCE,\n",
        "# positive_good = True,\n",
        "# rounding = 4\n",
        "# )\n",
        "\n",
        "# display(push_enabled_test[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Results Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to process test results into a structured format\n",
        "def process_ttest_result(df, metric_name):\n",
        "   if df is None or df.empty:\n",
        "       print(f\"Skipping {metric_name} due to missing data\")\n",
        "       return None\n",
        "   df[\"Variant\"] = df[\"Variant\"].str.upper()  # Standardize variant names\n",
        "   control_row = df[df[\"Variant\"] == \"CONTROL\"]\n",
        "   if control_row.empty:\n",
        "       print(f\"Skipping {metric_name} due to missing CONTROL group\")\n",
        "       return None\n",
        "   variant_rows = df[df[\"Variant\"] != \"CONTROL\"]  # Get all other variants\n",
        "   structured_data = {\"Variant\": [\"CONTROL\"], metric_name: [control_row[\"Mean\"].values[0]]}\n",
        "   # Append all variant means first\n",
        "   for _, variant_row in variant_rows.iterrows():\n",
        "       structured_data[\"Variant\"].append(variant_row[\"Variant\"])\n",
        "       structured_data[metric_name].append(variant_row[\"Mean\"])\n",
        "   # Append Absolute Delta, Relative Delta, and P-Values\n",
        "   for metric_type in [\"Absolute Delta\", \"Relative Delta\", \"P-Value\"]:\n",
        "    #    structured_data[\"Variant\"].append(metric_type)  # Single row label for each section\n",
        "    #    structured_data[metric_name].append(\"\")  # Placeholder for format consistency\n",
        "       for _, variant_row in variant_rows.iterrows():\n",
        "           structured_data[\"Variant\"].append(f\"{variant_row['Variant']} {metric_type}\")\n",
        "           structured_data[metric_name].append(variant_row[metric_type])\n",
        "   return pd.DataFrame(structured_data)\n",
        "# List of t-test dataframes to process\n",
        " \n",
        "ttest_results = [\n",
        "    (\"Customers\", visitors_test[0]),\n",
        "   (\"Total Revenue Per Customer\", revenue_test[0]),\n",
        "   (\"Non Zero Revenue RPC\", nz_revenue_test[0]),\n",
        "   (\"Net Sales Per Customer\", net_sales_test[0]),\n",
        "   (\"Ecomm Revenue per Customer\", ecomm_revenue_test[0]),\n",
        "   (\"Ecomm Non Zero RPC\", ecomm_nonzero_revenue_test[0]),\n",
        "   (\"Markdown Per Customer\", mkdn_test[0]),\n",
        "   (\"AGP Per Customer\", agp_test[0]),\n",
        "   (\"CVR\", orders_cvr_test[0]),\n",
        "   (\"AOV\", AOV_test[0]),\n",
        "   (\"UPO\", UPO_test[0]),\n",
        "   (\"Transactions Per Customer\", orders_test[0]),\n",
        "   (\"Units Per Customer\", units_test[0]),\n",
        "   (\"Category Breadth\", CB_test[0]),\n",
        "   (\"Category Depth\", CD_test[0]),\n",
        "   (\"Visits Per Customer\", sessions_test[0]),\n",
        "   (\"Searches Per Customer\", searches_test[0]),\n",
        "   (\"Cart Adds\", cartadds_test[0]),\n",
        "   (\"Coupon Clips Per Customer\", ACC_test[0]),\n",
        "   (\"Redemptions Per Customer\", coupon_redemptions_test[0]),\n",
        "   (\"% of Customers who Search\", search_cvr_test[0]),\n",
        "   (\"% of Customers who Add to Cart\", cartadds_cvr_test[0]),\n",
        "   (\"% of Customers who Clip\", couponclips_cvr_test[0]),\n",
        "   (\"Redemptions Per Customer - Ecomm\", coupon_redemptions_ecomm_test[0]),\n",
        "   (\"Ecomm MKDN per Customer\", ecomm_markdown_pc_test[0]),\n",
        "   (\"Ecomm CVR\", ecomm_orders_cvr_test[0]),\n",
        "   (\"Ecomm TXNs Per Customer\", ecomm_orders_test[0]),\n",
        "   (\"Ecomm BNC\",ecomm_bnc_test[0]),\n",
        "   (\"Ecomm BNC CVR\", ecomm_bnc_cvr_test[0])\n",
        "]\n",
        " \n",
        "# Process each test result and merge them into a summary DataFrame\n",
        "summary_dfs = [process_ttest_result(df, name) for name, df in ttest_results if process_ttest_result(df, name) is not None]\n",
        "# Extract \"Variant\" column from the first DataFrame (avoid duplication)\n",
        "variant_column = summary_dfs[0][[\"Variant\"]].copy()\n",
        "# Drop \"Variant\" from all other DataFrames before merging\n",
        "for i in range(len(summary_dfs)):\n",
        "   summary_dfs[i] = summary_dfs[i].drop(columns=[\"Variant\"], errors=\"ignore\")\n",
        "# Merge all processed DataFrames on their row index\n",
        "summary_df = pd.concat([variant_column] + summary_dfs, axis=1)\n",
        "# Ensure \"Variant\" is in Databricks display\n",
        "summary_df.reset_index(drop=True, inplace=True)\n",
        "# Display the structured DataFrame in Databricks\n",
        "display(summary_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Numeriic Results Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "# Function to process test results into structured format\n",
        "def process_ttest_result(df, metric_name):\n",
        "   if df is None or df.empty:\n",
        "       print(f\"Skipping {metric_name} due to missing data\")\n",
        "       return None\n",
        "   # Standardize variant names\n",
        "   df[\"Variant\"] = df[\"Variant\"].str.upper()\n",
        "   # Define numeric columns (including \"Relative Delta\")\n",
        "   numeric_cols = ['Mean', 'Absolute Delta', 'Relative Delta', 'P-Value']\n",
        "   # Remove '%' sign before converting to float\n",
        "   for col in numeric_cols:\n",
        "       if col in df.columns:\n",
        "           df[col] = df[col].astype(str).str.replace('%', '', regex=True)  # Remove %\n",
        "           df[col] = pd.to_numeric(df[col], errors='coerce')  # Convert to float\n",
        "   # Validate control group presence\n",
        "   control_row = df[df[\"Variant\"] == \"CONTROL\"]\n",
        "   if control_row.empty:\n",
        "       print(f\"Skipping {metric_name} due to missing CONTROL group\")\n",
        "       return None\n",
        "   # Separate control and variants\n",
        "   variant_rows = df[df[\"Variant\"] != \"CONTROL\"]\n",
        "   # Structured data storage\n",
        "   structured_data = {\n",
        "       \"Variant\": [\"CONTROL\"],\n",
        "       metric_name: [control_row[\"Mean\"].values[0]]\n",
        "   }\n",
        "   # Add variant means\n",
        "   for _, variant_row in variant_rows.iterrows():\n",
        "       structured_data[\"Variant\"].append(variant_row[\"Variant\"])\n",
        "       structured_data[metric_name].append(variant_row[\"Mean\"])\n",
        "   # Add metric sections with NaN placeholders\n",
        "   for metric_type in [\"Absolute Delta\", \"Relative Delta\", \"P-Value\"]:\n",
        "    #    structured_data[\"Variant\"].append(metric_type)  # Section header\n",
        "    #    structured_data[metric_name].append(np.nan)  # NaN placeholder\n",
        "       # Add variant-specific values\n",
        "       for _, variant_row in variant_rows.iterrows():\n",
        "           structured_data[\"Variant\"].append(f\"{variant_row['Variant']} {metric_type}\")\n",
        "           structured_data[metric_name].append(variant_row.get(metric_type, np.nan))\n",
        "   return pd.DataFrame(structured_data)\n",
        "# Your original list of t-test results (preserved exactly)\n",
        "ttest_results = [\n",
        "    (\"Customers\", visitors_test[0]),\n",
        "   (\"Total Revenue Per Customer\", revenue_test[0]),\n",
        "   (\"Non Zero Revenue RPC\", nz_revenue_test[0]),\n",
        "   (\"Net Sales Per Customer\", net_sales_test[0]),\n",
        "   (\"Ecomm Revenue per Customer\", ecomm_revenue_test[0]),\n",
        "   (\"Ecomm Non Zero RPC\", ecomm_nonzero_revenue_test[0]),\n",
        "   (\"Markdown Per Customer\", mkdn_test[0]),\n",
        "   (\"AGP Per Customer\", agp_test[0]),\n",
        "   (\"CVR\", orders_cvr_test[0]),\n",
        "   (\"AOV\", AOV_test[0]),\n",
        "   (\"UPO\", UPO_test[0]),\n",
        "   (\"Transactions Per Customer\", orders_test[0]),\n",
        "   (\"Units Per Customer\", units_test[0]),\n",
        "   (\"Category Breadth\", CB_test[0]),\n",
        "   (\"Category Depth\", CD_test[0]),\n",
        "   (\"Visits Per Customer\", sessions_test[0]),\n",
        "   (\"Searches Per Customer\", searches_test[0]),\n",
        "   (\"Cart Adds\", cartadds_test[0]),\n",
        "   (\"Coupon Clips Per Customer\", ACC_test[0]),\n",
        "   (\"Redemptions Per Customer\", coupon_redemptions_test[0]),\n",
        "   (\"% of Customers who Search\", search_cvr_test[0]),\n",
        "   (\"% of Customers who Add to Cart\", cartadds_cvr_test[0]),\n",
        "   (\"% of Customers who Clip\", couponclips_cvr_test[0]),\n",
        "   (\"Redemptions Per Customer - Ecomm\", coupon_redemptions_ecomm_test[0]),\n",
        "   (\"Ecomm MKDN per Customer\", ecomm_markdown_pc_test[0]),\n",
        "   (\"Ecomm CVR\", ecomm_orders_cvr_test[0]),\n",
        "   (\"Ecomm TXNs Per Customer\", ecomm_orders_test[0]),\n",
        "   (\"Ecomm BNC\",ecomm_bnc_test[0]),\n",
        "   (\"Ecomm BNC CVR\", ecomm_bnc_cvr_test[0]),\n",
        "]\n",
        " \n",
        "# Process all results\n",
        "summary_dfs = []\n",
        "for name, df in ttest_results:\n",
        "   processed_df = process_ttest_result(df, name)\n",
        "   if processed_df is not None:\n",
        "       summary_dfs.append(processed_df)\n",
        "# Merge results\n",
        "if summary_dfs:\n",
        "   variant_column = summary_dfs[0][[\"Variant\"]].copy()\n",
        "   # Drop variant column from other DFs before merging\n",
        "   for df in summary_dfs:\n",
        "       df.drop(columns=[\"Variant\"], errors=\"ignore\", inplace=True)\n",
        "   # Concatenate all data\n",
        "   summary_df = pd.concat([variant_column] + summary_dfs, axis=1, join=\"outer\")\n",
        "   # Convert numeric columns\n",
        "   for col in summary_df.columns.difference([\"Variant\"]):\n",
        "       summary_df[col] = pd.to_numeric(summary_df[col], errors=\"coerce\")\n",
        "   # Reset index for clean display\n",
        "   summary_df.reset_index(drop=True, inplace=True)\n",
        "   # Debugging Check: Verify if \"Relative Delta\" and \"CVR\" are correctly processed\n",
        "   print(\"\u2705 Final DataFrame ready for display in Databricks.\")\n",
        "   # Display the final DataFrame (Databricks method)\n",
        "   display(summary_df)\n",
        "else:\n",
        "   print(\"No valid data to display\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Formatting\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_ttest_result(df, metric_name):\n",
        "   if df is None or df.empty:\n",
        "       print(f\"Skipping {metric_name} due to missing data\")\n",
        "       return None\n",
        "   df[\"Variant\"] = df[\"Variant\"].str.upper()\n",
        "   numeric_cols = ['Mean', 'Absolute Delta', 'Relative Delta', 'P-Value']\n",
        "   for col in numeric_cols:\n",
        "       if col in df.columns:\n",
        "           df[col] = df[col].astype(str).str.replace('%', '', regex=True)\n",
        "           df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "   control_row = df[df[\"Variant\"] == \"CONTROL\"]\n",
        "   if control_row.empty:\n",
        "       print(f\"Skipping {metric_name} due to missing CONTROL group\")\n",
        "       return None\n",
        "   variant_rows = df[df[\"Variant\"] != \"CONTROL\"]\n",
        "   structured_data = {\n",
        "       \"Variant\": [\"CONTROL\"],\n",
        "       metric_name: [control_row[\"Mean\"].values[0]]\n",
        "   }\n",
        "   for _, variant_row in variant_rows.iterrows():\n",
        "       structured_data[\"Variant\"].append(variant_row[\"Variant\"])\n",
        "       structured_data[metric_name].append(variant_row[\"Mean\"])\n",
        "   for metric_type in [\"Absolute Delta\", \"Relative Delta\", \"P-Value\"]:\n",
        "    #    structured_data[\"Variant\"].append(metric_type)\n",
        "    #    structured_data[metric_name].append(np.nan)\n",
        "       for _, variant_row in variant_rows.iterrows():\n",
        "           structured_data[\"Variant\"].append(f\"{variant_row['Variant']} {metric_type}\")\n",
        "           structured_data[metric_name].append(variant_row.get(metric_type, np.nan))\n",
        "   return pd.DataFrame(structured_data)\n",
        "ttest_results = [\n",
        "    (\"Customers\", visitors_test[0]),\n",
        "   (\"Total Revenue Per Customer\", revenue_test[0]),\n",
        "   (\"Non Zero Revenue RPC\", nz_revenue_test[0]),\n",
        "   (\"Net Sales Per Customer\", net_sales_test[0]),\n",
        "   (\"Ecomm Revenue per Customer\", ecomm_revenue_test[0]),\n",
        "   (\"Ecomm Non Zero RPC\", ecomm_nonzero_revenue_test[0]),\n",
        "   (\"Markdown Per Customer\", mkdn_test[0]),\n",
        "   (\"AGP Per Customer\", agp_test[0]),\n",
        "   (\"CVR\", orders_cvr_test[0]),\n",
        "   (\"AOV\", AOV_test[0]),\n",
        "   (\"UPO\", UPO_test[0]),\n",
        "   (\"Transactions Per Customer\", orders_test[0]),\n",
        "   (\"Units Per Customer\", units_test[0]),\n",
        "   (\"Category Breadth\", CB_test[0]),\n",
        "   (\"Category Depth\", CD_test[0]),\n",
        "   (\"Visits Per Customer\", sessions_test[0]),\n",
        "   (\"Searches Per Customer\", searches_test[0]),\n",
        "   (\"Cart Adds\", cartadds_test[0]),\n",
        "   (\"Coupon Clips Per Customer\", ACC_test[0]),\n",
        "   (\"Redemptions Per Customer\", coupon_redemptions_test[0]),\n",
        "   (\"% of Customers who Search\", search_cvr_test[0]),\n",
        "   (\"% of Customers who Add to Cart\", cartadds_cvr_test[0]),\n",
        "   (\"% of Customers who Clip\", couponclips_cvr_test[0]),\n",
        "   (\"Redemptions Per Customer - Ecomm\", coupon_redemptions_ecomm_test[0]),\n",
        "   (\"Ecomm MKDN per Customer\", ecomm_markdown_pc_test[0]),\n",
        "   (\"Ecomm CVR\", ecomm_orders_cvr_test[0]),\n",
        "   (\"Ecomm TXNs Per Customer\", ecomm_orders_test[0]),\n",
        "   (\"Ecomm BNC\",ecomm_bnc_test[0]),\n",
        "   (\"Ecomm BNC CVR\", ecomm_bnc_cvr_test[0]),\n",
        "]\n",
        " \n",
        "summary_dfs = []\n",
        "for name, df in ttest_results:\n",
        "   processed_df = process_ttest_result(df, name)\n",
        "   if processed_df is not None:\n",
        "       summary_dfs.append(processed_df)\n",
        "if summary_dfs:\n",
        "   variant_column = summary_dfs[0][[\"Variant\"]].copy()\n",
        "   for df in summary_dfs:\n",
        "       df.drop(columns=[\"Variant\"], errors=\"ignore\", inplace=True)\n",
        "   summary_df = pd.concat([variant_column] + summary_dfs, axis=1, join=\"outer\")\n",
        "   for col in summary_df.columns.difference([\"Variant\"]):\n",
        "       summary_df[col] = pd.to_numeric(summary_df[col], errors=\"coerce\")\n",
        "   summary_df.reset_index(drop=True, inplace=True)\n",
        "   # Define formatting rules using lists\n",
        "   formatting = {\n",
        "       'no_decimals': [\"Customers\", \"Visits\", \"Ecomm BNC\"],\n",
        "       'two_decimals': [\n",
        "           \"Units Per Customer\", \"Searches Per Customer\", \"Cart Adds\",\n",
        "           \"Coupon Clips Per Customer\", \"Redemptions Per Customer\",\n",
        "           \"Redemptions Per Customer - Ecomm\",\"UPO\" ,\"Ecomm TXNs Per Customer\",\"Transactions Per Customer\",\n",
        "           \"Category Breadth\",\"Category Depth\"\n",
        "       ],\n",
        "       'dollars': [\n",
        "           \"Total Revenue Per Customer\", \"Non Zero Revenue RPC\", \"Net Sales Per Customer\",\n",
        "           \"Ecomm Revenue per Customer\", \"Markdown Per Customer\",\"Ecomm Non Zero RPC\",\n",
        "           \"AGP Per Customer\", \"AOV\", \"Ecomm MKDN per Customer\"\n",
        "       ],\n",
        "       'percentages': [\n",
        "           \"CVR\", \"% of Customers who Search\", \"% of Customers who Add to Cart\",\n",
        "           \"% of Customers who Clip\", \"Ecomm CVR\", \"Ecomm BNC CVR\"\n",
        "       ]\n",
        "   }\n",
        " \n",
        "   # Create a formatted copy of the DataFrame\n",
        "   formatted_df = summary_df.copy()\n",
        "   # Identify special rows (needs to be done before any formatting)\n",
        "   p_value_mask = formatted_df['Variant'].str.contains('P-Value', na=False)\n",
        "   relative_delta_mask = formatted_df['Variant'].str.contains('Relative Delta', na=False)\n",
        "   special_mask = p_value_mask | relative_delta_mask\n",
        "   \n",
        "   # Apply column formatting to ALL NON-special rows\n",
        "   for category, cols in formatting.items():\n",
        "       for col in cols:\n",
        "           if col not in formatted_df.columns:\n",
        "               continue\n",
        "           non_special_mask = ~special_mask\n",
        "           if category == 'no_decimals':\n",
        "               formatted_df.loc[non_special_mask, col] = formatted_df.loc[non_special_mask, col].apply(\n",
        "                   lambda x: f\"{float(x):,.0f}\" if pd.notnull(x) else \"\")\n",
        "           elif category == 'two_decimals':\n",
        "               formatted_df.loc[non_special_mask, col] = formatted_df.loc[non_special_mask, col].apply(\n",
        "               lambda x: f\"{float(x):,.2f}\" if pd.notnull(x) else \"\")\n",
        "           elif category == 'dollars':\n",
        "               formatted_df.loc[non_special_mask, col] = formatted_df.loc[non_special_mask, col].apply(\n",
        "                   lambda x: f\"${float(x):,.2f}\" if pd.notnull(x) else \"\")\n",
        "           elif category == 'percentages':\n",
        "               formatted_df.loc[non_special_mask, col] = formatted_df.loc[non_special_mask, col].apply(\n",
        "               lambda x: f\"{float(x):.2f}%\" if pd.notnull(x) else \"\")\n",
        "               \n",
        "   # Format p-value rows with 3 decimal places\n",
        "   for col in formatted_df.columns.difference(['Variant']):\n",
        "       formatted_df.loc[p_value_mask, col] = (\n",
        "           formatted_df.loc[p_value_mask, col]\n",
        "           .apply(pd.to_numeric, errors='coerce')\n",
        "           .apply(lambda x: f\"{x:.3f}\" if pd.notnull(x) else \"\"))\n",
        " \n",
        " \n",
        "   # Format for relative delta\n",
        "   for col in formatted_df.columns.difference(['Variant']):\n",
        "       formatted_df.loc[relative_delta_mask, col] = (\n",
        "           formatted_df.loc[relative_delta_mask, col]\n",
        "           .astype(str)\n",
        "           .str.replace('%', '', regex=False)\n",
        "           .apply(pd.to_numeric, errors='coerce')\n",
        "           .apply(lambda x: f\"{x:.2f}%\" if pd.notnull(x) else \"\"))\n",
        "       \n",
        "   print(\"\u2705 Final DataFrame formatted with proper numeric displays.\")\n",
        "   display(formatted_df)\n",
        "else:\n",
        "   print(\"No valid data to display\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Scroll Table\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def style_summary_table(formatted_df):\n",
        "# \"\"\"Generate 1920px-fixed table with restored header colors\"\"\"\n",
        "# df = formatted_df.copy().fillna('')\n",
        "# # ======================================================================\n",
        "# # COLUMN CATEGORIES (MODIFY THESE TO MATCH YOUR DATA)\n",
        "# # ======================================================================\n",
        "# transaction_columns = [\n",
        "# \"Customers\",\"RPC Test\", \"Non Zero Revenue Test\",\"Net Sales Per Customer\",\n",
        "# \"AGP Test\",\n",
        "# \"CVR\", \"AOV Test\", \"UPO Test\", \"Transactions Per Customer\",\n",
        "# \"Units Per Customer\",\"Ecomm Revenue per Customer Test\",\"Markdown Per Customer\",\n",
        "# \"Revenue Per Customer\", \"Non Zero Revenue RPC\", \"Ecomm Revenue per Customer\",\n",
        "# \"Markdown Per Customer\", \"AGP\", \"Transactions CVR\", \"AOV\", \"UPO\"\n",
        "# ]\n",
        "# engagement_columns = [\n",
        "# \"Visits\", \"Searches Per Customer\", \"Cart Adds\",\n",
        "# \"Coupon Clips Per Customer\", \"Redemptions Per Customer\",\n",
        "# \"Search CVR\", \"Cart Adds CVR\", \"Coupons Clips CVR\",\"Redemptions Per Customer - Ecomm\"\n",
        "# ]\n",
        "# ecomm_columns = [\n",
        "# \"Ecomm Revenue Per Customer Test\", \"Ecomm MKDN per Customer\",\n",
        "# \"Ecomm CVR\", \"Ecomm TXNs Per Customer\", \"Ecomm BNC CVR\"\n",
        "# ]\n",
        "# exclude_from_coloring = [\"Customers\"]\n",
        "# # ======================================================================\n",
        "# # UPDATED CSS WITH HEADER COLOR FIXES\n",
        "# # ======================================================================\n",
        "# styled_html = f\"\"\"\n",
        "# <style>\n",
        "# .dashboard-container {{\n",
        "# max-width: 1920px;\n",
        "# margin: 0 auto;\n",
        "# font-family: Arial, sans-serif;\n",
        "# }}\n",
        "# .compact-table {{\n",
        "# width: 100%;\n",
        "# border-collapse: collapse;\n",
        "# table-layout: fixed;\n",
        "# font-size: 12px;\n",
        "# }}\n",
        "# .compact-table th {{\n",
        "# color: #2c3e50;\n",
        "# padding: 6px 8px;\n",
        "# border: 1px solid #dee2e6;\n",
        "# font-weight: bold;\n",
        "# overflow: hidden;\n",
        "# }}\n",
        "# /* Header Color Fixes */\n",
        "# .transaction-header {{\n",
        "# background-color: #d0e0f5 !important;\n",
        "# min-width: 120px;\n",
        "# }}\n",
        "# .engagement-header {{\n",
        "# background-color: #f7d7af !important;\n",
        "# min-width: 110px;\n",
        "# }}\n",
        "# .ecomm-header {{\n",
        "# background-color: #e2e3e5 !important;\n",
        "# min-width: 130px;\n",
        "# }}\n",
        "# .variant-header {{\n",
        "# background-color: #bfbfbf !important;\n",
        "# width: 150px;\n",
        "# }}\n",
        "# .compact-table td {{\n",
        "# padding: 6px 8px;\n",
        "# border: 1px solid #dee2e6;\n",
        "# text-align: right;\n",
        "# vertical-align: middle;\n",
        "# white-space: nowrap;\n",
        "# }}\n",
        "# /* Conditional Formatting */\n",
        "# .positive {{ background-color: #C6EFCE !important; color: #006100 !important; }}\n",
        "# .negative {{ background-color: #FFC7CE !important; color: #9C0006 !important; }}\n",
        "# .significant {{ border: 2px solid #5B9BD5 !important; }}\n",
        "# /* Legend Styling */\n",
        "# .compact-legend {{\n",
        "# margin: 15px 0;\n",
        "# padding: 12px;\n",
        "# background: #f8f9fa;\n",
        "# border-radius: 6px;\n",
        "# display: flex;\n",
        "# gap: 25px;\n",
        "# justify-content: center;\n",
        "# }}\n",
        "# .legend-item {{\n",
        "# display: flex;\n",
        "# align-items: center;\n",
        "# gap: 6px;\n",
        "# font-size: 12px;\n",
        "# }}\n",
        "# .legend-color {{\n",
        "# width: 16px;\n",
        "# height: 16px;\n",
        "# border-radius: 3px;\n",
        "# }}\n",
        "# </style>\n",
        "# <div class=\"dashboard-container\">\n",
        "# <table class=\"compact-table\">\n",
        "# <colgroup>\n",
        "# {' '.join([f'<col style=\"width: 150px\">' if col == \"Variant\" else\n",
        "# f'<col style=\"width: 120px\">' if col in transaction_columns else\n",
        "# f'<col style=\"width: 110px\">' if col in engagement_columns else\n",
        "# f'<col style=\"width: 130px\">' for col in df.columns])}\n",
        "# </colgroup>\n",
        "# <thead>\n",
        "# <tr>\n",
        "# {' '.join([f'<th class=\"variant-header\">{col}</th>' if col == \"Variant\" else\n",
        "# f'<th class=\"transaction-header\">{col}</th>' if col in transaction_columns else\n",
        "# f'<th class=\"engagement-header\">{col}</th>' if col in engagement_columns else\n",
        "# f'<th class=\"ecomm-header\">{col}</th>' for col in df.columns])}\n",
        "# </tr>\n",
        "# </thead>\n",
        "# <tbody>\"\"\"\n",
        "# # ======================================================================\n",
        "# # CONDITIONAL FORMATTING LOGIC (PRESERVES FORMATTED VALUES)\n",
        "# # ======================================================================\n",
        "# pvalue_lookup = {}\n",
        "# for idx, row in df.iterrows():\n",
        "# if \"P-Value\" in row[\"Variant\"]:\n",
        "# variant = row[\"Variant\"].replace(\" P-Value\", \"\")\n",
        "# for metric in df.columns[1:]:\n",
        "# raw_value = str(row[metric]).replace('$','').replace('%','').replace(',','').strip()\n",
        "# try:\n",
        "# pvalue_lookup[(variant, metric)] = float(raw_value) if raw_value else 1.0\n",
        "# except:\n",
        "# pvalue_lookup[(variant, metric)] = 1.0\n",
        "# for idx, row in df.iterrows():\n",
        "# styled_html += \"<tr>\"\n",
        "# for col in df.columns:\n",
        "# cell_value = row[col]\n",
        "# cell_class = \"\"\n",
        "# if col == \"Variant\":\n",
        "# styled_html += f'<td>{cell_value}</td>'\n",
        "# continue\n",
        "# if \"Relative Delta\" in row[\"Variant\"]:\n",
        "# try:\n",
        "# variant = row[\"Variant\"].replace(\" Relative Delta\", \"\")\n",
        "# pvalue = pvalue_lookup.get((variant, col), 1)\n",
        "# if col not in exclude_from_coloring:\n",
        "# clean_value = str(cell_value).replace('$','').replace('%','').replace(',','').strip()\n",
        "# numeric_value = float(clean_value) if clean_value else 0\n",
        "# if pvalue < 0.05:\n",
        "# cell_class = \"positive\" if numeric_value > 0 else \"negative\"\n",
        "# except: pass\n",
        "# if \"P-Value\" in row[\"Variant\"]:\n",
        "# try:\n",
        "# if col not in exclude_from_coloring:\n",
        "# clean_value = str(cell_value).replace('$','').replace('%','').replace(',','').strip()\n",
        "# if float(clean_value) < 0.05:\n",
        "# cell_class = \"significant\"\n",
        "# except: pass\n",
        "# styled_html += f'<td class=\"{cell_class}\">{cell_value}</td>'\n",
        "# styled_html += \"</tr>\"\n",
        "# # ======================================================================\n",
        "# # COMPACT LEGEND\n",
        "# # ======================================================================\n",
        "# styled_html += f\"\"\"\n",
        "# </tbody>\n",
        "# </table>\n",
        "# <div class=\"compact-legend\">\n",
        "# <div class=\"legend-item\">\n",
        "# <div class=\"legend-color\" style=\"background-color: #d0e0f5\"></div>\n",
        "# <span>Transaction Metrics</span>\n",
        "# </div>\n",
        "# <div class=\"legend-item\">\n",
        "# <div class=\"legend-color\" style=\"background-color: #f7d7af\"></div>\n",
        "# <span>Engagement Metrics</span>\n",
        "# </div>\n",
        "# <div class=\"legend-item\">\n",
        "# <div class=\"legend-color\" style=\"background-color: #e2e3e5\"></div>\n",
        "# <span>Ecomm Metrics</span>\n",
        "# </div>\n",
        "# </div>\n",
        "# </div>\"\"\"\n",
        "# return styled_html\n",
        "# # Usage formatted_df\n",
        "# displayHTML(style_summary_table(formatted_df))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Break Out Table\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def style_summary_table(formatted_df):\n",
        "   \"\"\"Generate three 1920px-fixed tables with improved border visibility\"\"\"\n",
        "   df = formatted_df.copy().fillna('')\n",
        "   # ======================================================================\n",
        "   # COLUMN CATEGORIES\n",
        "   # ======================================================================\n",
        "   transaction_columns = [\n",
        "       \"Customers\",\"Total Revenue Per Customer\", \"Non Zero Revenue RPC\", \"Net Sales Per Customer\",\"Markdown Per Customer\",\n",
        "       \"AGP Per Customer\",\"CVR\", \"AOV\", \"UPO\", \"Transactions Per Customer\", \"Units Per Customer\",\"Category Breadth\",\"Category Depth\"\n",
        "   ]\n",
        "   engagement_columns = [\n",
        "       \"Visits Per Customer\", \"Searches Per Customer\", \"Cart Adds\",\n",
        "       \"Coupon Clips Per Customer\", \"Redemptions Per Customer\",\n",
        "       \"% of Customers who Search\", \"% of Customers who Add to Cart\", \"% of Customers who Clip\"\n",
        "   ]\n",
        "   ecomm_columns = [\n",
        "       \"Ecomm Revenue per Customer\",\"Ecomm Non Zero RPC\",\"Ecomm MKDN per Customer\", \"Redemptions Per Customer - Ecomm\",\n",
        "       \"Ecomm CVR\", \"Ecomm TXNs Per Customer\", \"Ecomm BNC\", \"Ecomm BNC CVR\",\n",
        "   ]\n",
        "   exclude_from_coloring = [\"Customers\",\"Ecomm BNC\"]\n",
        " \n",
        "   reverse_color_columns = [\n",
        "       \"Markdown Per Customer\",\n",
        "       \"Ecomm MKDN per Customer\"\n",
        "   ]\n",
        "   def validate_columns(expected_cols, category_name):\n",
        "       missing = [col for col in expected_cols if col not in df.columns]\n",
        "       if missing:\n",
        "           print(f\"\u26a0\ufe0f Warning - {category_name} columns missing: {missing}\")\n",
        "   validate_columns([\"Variant\"] + transaction_columns, \"Transaction\")\n",
        "   validate_columns([\"Variant\"] + engagement_columns, \"Engagement\")\n",
        "   validate_columns([\"Variant\"] + ecomm_columns, \"Ecomm\")\n",
        "   # ======================================================================\n",
        "   # P-VALUE LOOKUP\n",
        "   # ======================================================================\n",
        "   pvalue_lookup = {}\n",
        "   for idx, row in df.iterrows():\n",
        "       if \"P-Value\" in row[\"Variant\"]:\n",
        "           variant = row[\"Variant\"].replace(\" P-Value\", \"\")\n",
        "           for metric in df.columns[1:]:\n",
        "               raw_value = str(row[metric]).replace('$','').replace('%','').replace(',','').strip()\n",
        "               try:\n",
        "                   pvalue_lookup[(variant, metric)] = float(raw_value) if raw_value else 1.0\n",
        "               except:\n",
        "                   pvalue_lookup[(variant, metric)] = 1.0\n",
        "   # ======================================================================\n",
        "   # HTML GENERATION WITH PADDING\n",
        "   # ======================================================================\n",
        "   category_configs = [\n",
        "       {\n",
        "           \"name\": \"Transaction\",\n",
        "           \"columns\": transaction_columns,\n",
        "           \"header_class\": \"transaction-header\",\n",
        "           \"legend_color\": \"#d0e0f5\",\n",
        "           \"col_width\": 120\n",
        "       },\n",
        "       {\n",
        "           \"name\": \"Engagement\",\n",
        "           \"columns\": engagement_columns,\n",
        "           \"header_class\": \"engagement-header\",\n",
        "           \"legend_color\": \"#f7d7af\",\n",
        "           \"col_width\": 110\n",
        "       },\n",
        "       {\n",
        "           \"name\": \"Ecomm\",\n",
        "           \"columns\": ecomm_columns,\n",
        "           \"header_class\": \"ecomm-header\",\n",
        "           \"legend_color\": \"#e2e3e5\",\n",
        "           \"col_width\": 130\n",
        "       }\n",
        "   ]\n",
        "   html_outputs = []\n",
        "   for config in category_configs:\n",
        "       subset_cols = [\"Variant\"] + [col for col in config[\"columns\"] if col in df.columns]\n",
        "       if len(subset_cols) == 1: continue\n",
        "       df_subset = df[subset_cols]\n",
        "       styled_html = f\"\"\"\n",
        "<style>\n",
        "   .dashboard-container {{\n",
        "       max-width: 1920px;\n",
        "       margin: 0 auto;\n",
        "       padding: 0 20px;\n",
        "       font-family: Arial, sans-serif;\n",
        "       box-sizing: border-box;\n",
        "   }}\n",
        "   .compact-table {{\n",
        "       width: 100%;\n",
        "       border-collapse: collapse;\n",
        "       table-layout: fixed;\n",
        "       font-size: 12px;\n",
        "       margin: 15px 0;\n",
        "       border: 1px solid #dee2e6;\n",
        "   }}\n",
        "   .compact-table th {{\n",
        "       color: #2c3e50;\n",
        "       padding: 8px 12px;\n",
        "       border: 1px solid #dee2e6;\n",
        "       font-weight: bold;\n",
        "       background-clip: padding-box;\n",
        "   }}\n",
        "   .transaction-header {{ background-color: #d0e0f5 !important; }}\n",
        "   .engagement-header {{ background-color: #f7d7af !important; }}\n",
        "   .ecomm-header {{ background-color: #e2e3e5 !important; }}\n",
        "   .variant-header {{ background-color: #bfbfbf !important; }}\n",
        "   .compact-table td {{\n",
        "       padding: 8px 12px;\n",
        "       border: 1px solid #dee2e6;\n",
        "       text-align: right;\n",
        "       vertical-align: middle;\n",
        "       white-space: nowrap;\n",
        "       position: relative;\n",
        "   }}\n",
        "   .positive {{ background-color: #C6EFCE !important; color: #006100 !important; }}\n",
        "   .negative {{ background-color: #FFC7CE !important; color: #9C0006 !important; }}\n",
        "   .absdelta {{ background-color: #f7f7f7 !important; }}\n",
        "   .significant {{ border: 2px solid #5B9BD5 !important; }}\n",
        "   .legend-container {{\n",
        "       margin: 20px 0;\n",
        "       padding: 15px;\n",
        "       background: #f8f9fa;\n",
        "       border-radius: 8px;\n",
        "       display: flex;\n",
        "       justify-content: center;\n",
        "       gap: 25px;\n",
        "   }}\n",
        "   .legend-item {{\n",
        "       display: flex;\n",
        "       align-items: center;\n",
        "       gap: 8px;\n",
        "       font-size: 13px;\n",
        "   }}\n",
        "   .legend-color {{\n",
        "       width: 18px;\n",
        "       height: 18px;\n",
        "       border-radius: 4px;\n",
        "       border: 1px solid #dee2e6;\n",
        "   }}\n",
        "</style>\n",
        "<div class=\"dashboard-container\">\n",
        "<h3 style=\"margin-bottom: 10px;\">{config['name']} Metrics</h3>\n",
        "<table class=\"compact-table\">\n",
        "<colgroup>\n",
        "       {' '.join([f'<col style=\"width: 150px\">' if col == \"Variant\"\n",
        "                else f'<col style=\"width: {config[\"col_width\"]}px\">'\n",
        "                for col in df_subset.columns])}\n",
        "</colgroup>\n",
        "<thead>\n",
        "<tr>\n",
        "           {' '.join([f'<th class=\"variant-header\">{col}</th>' if col == \"Variant\"\n",
        "                    else f'<th class=\"{config[\"header_class\"]}\">{col}</th>'\n",
        "                    for col in df_subset.columns])}\n",
        "</tr>\n",
        "</thead>\n",
        "<tbody>\"\"\"\n",
        "       for idx, row in df_subset.iterrows():\n",
        "           styled_html += \"<tr>\"\n",
        "           for col in df_subset.columns:\n",
        "               cell_value = row[col]\n",
        "               cell_class = \"\"\n",
        "               if col == \"Variant\":\n",
        "                   styled_html += f'<td>{cell_value}</td>'\n",
        "                   continue\n",
        "               if str(cell_value).strip() == col:\n",
        "                   cell_value = ''\n",
        "               if \"Absolute Delta\" in row[\"Variant\"]:\n",
        "                   cell_class = \"absdelta\"\n",
        "               if \"Relative Delta\" in row[\"Variant\"]:\n",
        "                   try:\n",
        "                       variant = row[\"Variant\"].replace(\" Relative Delta\", \"\")\n",
        "                       pvalue = pvalue_lookup.get((variant, col), 1)\n",
        "                       if col not in exclude_from_coloring:\n",
        "                           clean_value = str(cell_value).replace('$','').replace('%','').replace(',','').strip()\n",
        "                           numeric_value = float(clean_value) if clean_value else 0\n",
        "                           if pvalue < SIGNIFICANCE:\n",
        "                               # Reverse color logic for specific columns\n",
        "                               if col in reverse_color_columns:\n",
        "                                   cell_class = \"positive\" if numeric_value < 0 else \"negative\"\n",
        "                               else:\n",
        "                                   cell_class = \"positive\" if numeric_value > 0 else \"negative\"\n",
        "                   except: pass\n",
        "               if \"P-Value\" in row[\"Variant\"]:\n",
        "                   try:\n",
        "                       if col not in exclude_from_coloring:\n",
        "                           clean_value = str(cell_value).replace('$','').replace('%','').replace(',','').strip()\n",
        "                           if float(clean_value) < SIGNIFICANCE:\n",
        "                               cell_class = \"significant\"\n",
        "                   except: pass\n",
        "               styled_html += f'<td class=\"{cell_class}\">{cell_value}</td>'\n",
        "           styled_html += \"</tr>\"\n",
        "       styled_html += f\"\"\"\n",
        "</tbody>\n",
        "</table>\n",
        "</div>\n",
        "</div>\n",
        "</div>\"\"\"\n",
        "       html_outputs.append(styled_html)\n",
        "   return html_outputs\n",
        "# Usage remains the same:\n",
        "# displayHTML(style_summary_table(formatted_df)[0])  # Transaction\n",
        "# displayHTML(style_summary_table(formatted_df)[1])  # Engagement\n",
        "# displayHTML(style_summary_table(formatted_df)[2])  # Ecomm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Transaction Metrics Display\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Transaction Metrics\n",
        "displayHTML(style_summary_table(formatted_df)[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Engagement Metrics Display\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Engagement Metrics\n",
        "displayHTML(style_summary_table(formatted_df)[1])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Ecomm Metrics Display\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ecomm Metrics\n",
        "displayHTML(style_summary_table(formatted_df)[2])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Power Display\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "power_result_df = spark.sql(\"select * from db_work.POWER_OUTPUT_RESULTS\")\n",
        "experiment_power_df = power_result_df.filter(power_result_df['EXPERIMENT_ID'] == EXPERIMENT_ID).toPandas()\n",
        "if not experiment_power_df.empty:\n",
        "   row = experiment_power_df.iloc[0]\n",
        "   summary_html = f'''\n",
        "<style>\n",
        "   .power-table {{\n",
        "       font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n",
        "       border: 1px solid #e0e0e0;\n",
        "       border-radius: 8px;\n",
        "       margin: 20px 0;\n",
        "       box-shadow: 0 1px 3px rgba(0,0,0,0.12);\n",
        "   }}\n",
        "   .power-table h2 {{\n",
        "       color: #2c3e50;\n",
        "       padding: 15px 20px;\n",
        "       margin: 0;\n",
        "       font-size: 18px;\n",
        "       border-bottom: 1px solid #e0e0e0;\n",
        "   }}\n",
        "   .power-table table {{\n",
        "       width: 100%;\n",
        "       border-collapse: collapse;\n",
        "       background: white;\n",
        "   }}\n",
        "   .power-table th {{\n",
        "       background-color: #0056b3;\n",
        "       color: white;\n",
        "       padding: 12px 15px;\n",
        "       text-align: left;\n",
        "       font-weight: 600;\n",
        "   }}\n",
        "   .power-table td {{\n",
        "       padding: 12px 15px;\n",
        "       border-bottom: 1px solid #e0e0e0;\n",
        "   }}\n",
        "   .power-table tr:nth-child(even) {{\n",
        "       background-color: #f8f9fa;\n",
        "   }}\n",
        "   .power-table .footer {{\n",
        "       text-align: center;\n",
        "       color: #666;\n",
        "       font-size: 12px;\n",
        "       padding: 10px;\n",
        "       border-top: 1px solid #e0e0e0;\n",
        "   }}\n",
        "</style>\n",
        "<div class=\"power-table\">\n",
        "<h2>\ud83d\udcca Sample Size Overview</h2>\n",
        "<table>\n",
        "<thead>\n",
        "<tr>\n",
        "<th>Metric</th>\n",
        "<th>Powered on Revenue & AGP</th>\n",
        "<th>Powered on Deconstructed</th>\n",
        "</tr>\n",
        "</thead>\n",
        "<tbody>\n",
        "<tr>\n",
        "<td>Expected Duration</td>\n",
        "<td>{int(row['POWERED_ON_RPC_DURATION_IN_WEEKS'])} Weeks</td>\n",
        "<td>{int(row['POWERED_ON_DECONSTRUCTED_DURATION_IN_WEEKS'])} Weeks</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>Customers</td>\n",
        "<td>{int(row['POWERED_ON_RPC_HOUSEHOLDS']):,}</td>\n",
        "<td>{int(row['POWERED_ON_DECONSTRUCTED_HOUSEHOLDS']):,}</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>RPC MDE (%)</td>\n",
        "<td>{float(row['POWERED_ON_RPC_RPC_MDE_PCT']):.2f}</td>\n",
        "<td>{float(row['POWERED_ON_DECONSTRUCTED_RPC_MDE_PCT']):.2f}</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>AGP MDE (%)</td>\n",
        "<td>{\"N/A\" if pd.isna(row['POWERED_ON_AGP_AGP_MDE_PCT']) else f\"{float(row['POWERED_ON_AGP_AGP_MDE_PCT']):.2f}\"}</td>\n",
        "<td>{\"N/A\" if pd.isna(row['POWERED_ON_DECONSTRUCTED_AGP_MDE_PCT']) else f\"{float(row['POWERED_ON_DECONSTRUCTED_AGP_MDE_PCT']):.2f}\"}</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>NZ-RPC MDE (%)</td>\n",
        "<td>N/A</td>\n",
        "<td>{float(row['POWERED_ON_DECONSTRUCTED_NZ_RPC_MDE_PCT']):.2f}</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>CVR MDE (%)</td>\n",
        "<td>N/A</td>\n",
        "<td>{float(row['POWERED_ON_DECONSTRUCTED_CVR_MDE_PCT']):.2f}</td>\n",
        "</tr>\n",
        "</tbody>\n",
        "</table>\n",
        "<div class=\"footer\">\n",
        "       Generated by Databricks Dashboard\n",
        "</div>\n",
        "</div>\n",
        "   '''\n",
        "   displayHTML(summary_html)\n",
        "else:\n",
        "   error_html = '''\n",
        "<div style=\"padding: 20px;\n",
        "               margin: 20px 0;\n",
        "               border: 1px solid #ff4444;\n",
        "               border-radius: 8px;\n",
        "               background-color: #ffebee;\n",
        "               color: #cc0000;\n",
        "               font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\">\n",
        "       \u26a0\ufe0f No corresponding experiment ID found in the power calculation table\n",
        "</div>\n",
        "   '''\n",
        "   displayHTML(error_html)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_dashboard_element(summary_df):\n",
        "   \"\"\"Create key metrics summary with guaranteed data population and styling\"\"\"\n",
        "   # ================================\n",
        "   # Experiment ID Check\n",
        "   # ================================\n",
        "   if summary_df.empty or 'Variant' not in summary_df.columns:\n",
        "       return \"\"\"<div style='padding: 15px; margin: 20px;\n",
        "                   border-radius: 5px; background-color: #FFC7CE;\n",
        "                   color: #9C0006; border: 1px solid #9C0006;'>\n",
        "                   \u26a0\ufe0f No corresponding experiment ID found in the power calculation table\n",
        "</div>\"\"\"\n",
        "   # ================================\n",
        "   # Define Metrics\n",
        "   # ================================\n",
        "   metrics = ['AGP Per Customer', 'Total Revenue Per Customer', 'CVR','Net Sales Per Customer']\n",
        "   # ================================\n",
        "   # Data Processing\n",
        "   # ================================\n",
        "   df = summary_df.copy().fillna('')\n",
        "   metrics_data = {}\n",
        "   pvalue_lookup = {}\n",
        "   for idx, row in df.iterrows():\n",
        "       variant = row['Variant']\n",
        "       if 'Variant A' in variant:\n",
        "           continue  # Skip control\n",
        "       # Relative Delta row\n",
        "       if 'Relative Delta' in variant:\n",
        "           base_variant = variant.replace(' Relative Delta', '')\n",
        "           metrics_data[base_variant] = metrics_data.get(base_variant, {})\n",
        "           for metric in metrics:\n",
        "               value = str(row[metric]).replace('%', '')\n",
        "               try:\n",
        "                   metrics_data[base_variant][metric] = float(value) if value.strip() else ''\n",
        "               except:\n",
        "                   metrics_data[base_variant][metric] = ''\n",
        "       # P-Value row\n",
        "       if 'P-Value' in variant:\n",
        "           base_variant = variant.replace(' P-Value', '')\n",
        "           pvalue_lookup[base_variant] = pvalue_lookup.get(base_variant, {})\n",
        "           for metric in metrics:\n",
        "               try:\n",
        "                   pvalue_lookup[base_variant][metric] = float(row[metric])\n",
        "               except:\n",
        "                   pvalue_lookup[base_variant][metric] = 1.0\n",
        "   # ================================\n",
        "   # HTML Template & Rendering\n",
        "   # ================================\n",
        "   styled_html = \"\"\"\n",
        "<style>\n",
        "   .key-metrics {\n",
        "       font-family: Arial, sans-serif;\n",
        "       margin: 20px;\n",
        "       padding: 20px;\n",
        "       border-radius: 8px;\n",
        "       box-shadow: 0 2px 4px rgba(0,0,0,0.1);\n",
        "       background: white;\n",
        "   }\n",
        "   h2 {\n",
        "       color: #2c3e50;\n",
        "       font-size: 22px;\n",
        "       margin-bottom: 20px;\n",
        "   }\n",
        "   table {\n",
        "       border-collapse: collapse;\n",
        "       width: 100%;\n",
        "       font-size: 14px;\n",
        "   }\n",
        "   th, td {\n",
        "       border: 1px solid #d9d9d9;\n",
        "       padding: 12px;\n",
        "       text-align: center;\n",
        "       min-width: 120px;\n",
        "   }\n",
        "   th {\n",
        "       background-color: #0056b3 !important;\n",
        "       color: white !important;\n",
        "       font-weight: bold;\n",
        "   }\n",
        "   .variant-header {\n",
        "       background-color: #f8f9fa;\n",
        "       font-weight: bold;\n",
        "   }\n",
        "   .positive {\n",
        "       background-color: #C6EFCE !important;\n",
        "       color: #006100 !important;\n",
        "   }\n",
        "   .negative {\n",
        "       background-color: #FFC7CE !important;\n",
        "       color: #9C0006 !important;\n",
        "   }\n",
        "</style>\n",
        "<div class=\"key-metrics\">\n",
        "<h2>\ud83d\udcc8 Key Metrics Summary</h2>\n",
        "<table>\n",
        "<tr>\n",
        "<th class=\"variant-header\">Variant</th>\"\"\"\n",
        "   for metric in metrics:\n",
        "       styled_html += f\"<th>{metric} \u0394</th>\"\n",
        "   styled_html += \"</tr>\"\n",
        "   # ================================\n",
        "   # Add Variant Rows\n",
        "   # ================================\n",
        "   for variant in sorted(metrics_data.keys()):\n",
        "       styled_html += f\"<tr><td class='variant-header'><strong>{variant}</strong></td>\"\n",
        "       for metric in metrics:\n",
        "           delta = metrics_data[variant].get(metric, '')\n",
        "           pvalue = pvalue_lookup.get(variant, {}).get(metric, 1.0)\n",
        "           cell_class = \"\"\n",
        "           formatted_value = \"\"\n",
        "           if isinstance(delta, (int, float)):\n",
        "               formatted_value = f\"{delta:.1f}%\"\n",
        "               if pvalue < 0.05:\n",
        "                   cell_class = \"positive\" if delta > 0 else \"negative\"\n",
        "           elif delta != '':\n",
        "               formatted_value = str(delta)\n",
        "           styled_html += f\"<td class='{cell_class}'>{formatted_value}</td>\"\n",
        "       styled_html += \"</tr>\"\n",
        "   styled_html += \"</table></div>\"\n",
        "   return styled_html\n",
        "# Display in Databricks\n",
        "displayHTML(create_dashboard_element(summary_df))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# PHASE 1 BASELINE: Collect All Metrics & Calculate Runtime\n",
        "# This cell saves all aggregated metrics for Phase 2 comparison\n",
        "# NOTE: Using 'import datetime' (not 'from datetime import datetime') to avoid conflicts with SAFE code\n",
        " \n",
        "import pickle\n",
        "import datetime\n",
        "import os\n",
        " \n",
        "# \u23f1\ufe0f STOP TIMER\n",
        "BASELINE_END_TIME = time.time()\n",
        "BASELINE_END_DATETIME = datetime.datetime.now()\n",
        "TOTAL_RUNTIME_SECONDS = BASELINE_END_TIME - BASELINE_START_TIME\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"\ud83d\udcca PHASE 1 BASELINE COLLECTION\")\n",
        "print(\"=\"*70)\n",
        " \n",
        "# Collect all metrics\n",
        "baseline_metrics = {\n",
        "    'experiment_id': EXPERIMENT_ID,\n",
        "    'collection_date': datetime.datetime.now(),\n",
        "    'exp_start_date': EXP_START_DATE,\n",
        "    'exp_end_date': EXP_END_DATE,\n",
        "    'runtime_seconds': TOTAL_RUNTIME_SECONDS,\n",
        "    'start_time': BASELINE_START_DATETIME.strftime('%Y-%m-%d %H:%M:%S'),\n",
        "    'end_time': BASELINE_END_DATETIME.strftime('%Y-%m-%d %H:%M:%S'),\n",
        "}\n",
        " \n",
        "# Collect each aggregation (use try/except to handle missing variables)\n",
        "aggregations_to_collect = [\n",
        "    ('engagement_visitor', 'agg_daily_sp'),\n",
        "    ('margin', 'margin_agg_sp'),\n",
        "    ('agp', 'agp_agg_sp'),\n",
        "    ('ecomm', 'ecomm_agg_sp'),\n",
        "    ('store', 'store_agg_sp'),\n",
        "    ('txns', 'txns_sp'),\n",
        "    ('redemptions', 'redemptions_sp'),\n",
        "    ('bonus_points', 'bp_agg_sp'),\n",
        "    ('clips', 'clips_agg_sp'),\n",
        "    ('basket_health', 'basket_health_agg_sp'),\n",
        "    ('gas', 'gas_agg_sp'),\n",
        "    ('account_health', 'account_health_agg_sp'),\n",
        "]\n",
        " \n",
        "for metric_name, var_name in aggregations_to_collect:\n",
        "    try:\n",
        "        if var_name in dir():\n",
        "            df = eval(f\"{var_name}.toPandas()\")\n",
        "            baseline_metrics[metric_name] = df\n",
        "            print(f\"\u2705 {metric_name:25s} {len(df):5d} rows\")\n",
        "        else:\n",
        "            print(f\"\u26a0\ufe0f  {metric_name:25s} not found\")\n",
        "    except Exception as e:\n",
        "        print(f\"\u274c {metric_name:25s} error: {str(e)[:50]}\")\n",
        " \n",
        "# Determine notebook path for saving pickle file\n",
        "try:\n",
        "    notebook_path = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\n",
        "    notebook_dir = os.path.dirname(notebook_path)\n",
        "    output_path = f'safe_baseline_{EXPERIMENT_ID}.pkl'\n",
        "except Exception as e:\n",
        "    print(f\"\u26a0\ufe0f  Could not determine notebook path: {e}. Falling back to /dbfs/FileStore/.\")\n",
        "    output_path = f'safe_baseline_{EXPERIMENT_ID}.pkl'\n",
        " \n",
        "with open(output_path, 'wb') as f:\n",
        "    pickle.dump(baseline_metrics, f)\n",
        " \n",
        "print(f\"\\n\u2705 Baseline saved to: {output_path}\")\n",
        "print(f\"   Metrics collected: {len([k for k in baseline_metrics.keys() if k not in ['experiment_id', 'collection_date', 'exp_start_date', 'exp_end_date', 'runtime_seconds', 'start_time', 'end_time']])}\")\n",
        " \n",
        "# \u23f1\ufe0f DISPLAY RUNTIME\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"\u23f1\ufe0f  TOTAL RUNTIME\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Start:  {BASELINE_START_DATETIME.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "print(f\"End:    {BASELINE_END_DATETIME.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "print(f\"Total:  {TOTAL_RUNTIME_SECONDS:.2f} seconds ({TOTAL_RUNTIME_SECONDS/60:.2f} minutes)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"\u2705 PHASE 1 BASELINE COLLECTION COMPLETE\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\n\ud83d\udccb COPY THIS FOR YOUR TRACKING:\")\n",
        "print(f\"Experiment: {EXPERIMENT_ID} | Runtime: {TOTAL_RUNTIME_SECONDS:.2f}s ({TOTAL_RUNTIME_SECONDS/60:.2f}min)\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}